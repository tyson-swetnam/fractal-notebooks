{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# CHM Fractal Analysis: Complete Hypothesis Testing\n",
    "\n",
    "This notebook performs comprehensive fractal dimension analysis on Canopy Height Models (CHM) to test **all nine research hypotheses** from the Fractal Self-Affinity in Nature framework.\n",
    "\n",
    "## Five Primary Testable Hypotheses\n",
    "\n",
    "1. **Optimal Filling** - Old-growth forests maximize light interception, producing higher fractal dimensions\n",
    "2. **Scale Invariance** - Steady-state forests show scale-invariant gap distributions (power-law decay)\n",
    "3. **Zeta Distribution** - Canopy gaps follow power-law with exponent \u03b1 \u2248 2.0, related to \u03b6(2)\n",
    "4. **Universal Repulsion** - Dominant tree spacing follows Wigner-Dyson distribution (GUE statistics)\n",
    "5. **Biotic Decoupling** - Established ecosystems show weak correlation with topographic variables\n",
    "\n",
    "## Four Additional Spatial Distribution Hypotheses\n",
    "\n",
    "6. **Fractal String Gap** - Gap sizes follow fractal string spectrum predictions\n",
    "7. **Prime Number Repulsion (GUE)** - Large tree spacing shows prime-like repulsion patterns\n",
    "8. **Complex Dimension Oscillation** - Log-periodic oscillations in canopy structure\n",
    "9. **Riemann Gas Density** - Tree density follows Riemann gas statistical mechanics\n",
    "\n",
    "## Methods\n",
    "\n",
    "- **Differential Box Counting (DBC)** - Self-affine fractal dimension of height surfaces\n",
    "- **Standard Box Counting** - Self-similar fractal dimension comparison  \n",
    "- **Lacunarity Analysis** - Scale invariance and gap texture\n",
    "- **Gap Size Distribution** - Power-law vs exponential fitting\n",
    "- **Nearest Neighbor Spacing** - Wigner-Dyson vs Poisson statistics\n",
    "- **Topographic Correlation** - DEM-CHM regression analysis\n",
    "\n",
    "## Requirements\n",
    "\n",
    "```bash\n",
    "# Create the environment (first time only)\n",
    "mamba env create -f environments/3dep-environment.yml\n",
    "\n",
    "# Install the Jupyter kernel\n",
    "/opt/conda/envs/3dep/bin/python -m ipykernel install --user --name 3dep --display-name \"Python (3DEP)\"\n",
    "\n",
    "# Activate the environment\n",
    "conda activate 3dep\n",
    "```\n",
    "\n",
    "Then select the **\"Python (3DEP)\"** kernel in Jupyter before running this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment ready at 2025-12-22 19:25:14\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import rioxarray as rio\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import json\n",
    "from scipy import ndimage\n",
    "from scipy.stats import ks_2samp, chisquare, pearsonr, spearmanr\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.spatial import cKDTree\n",
    "from scipy.special import zeta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Interactive mapping\n",
    "import ipyleaflet\n",
    "from ipyleaflet import Map, DrawControl, TileLayer, GeoJSON, basemaps\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Geospatial\n",
    "from shapely.geometry import shape, box, Polygon\n",
    "from shapely.ops import transform\n",
    "import pyproj\n",
    "from rasterio.windows import from_bounds\n",
    "from rasterio.enums import Resampling\n",
    "\n",
    "print(f\"Environment ready at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-header",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "config",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auto-detected DEM: /home/jovyan/data-store/data/output/3dep/chm/superior_old_growth_dem.tif\n",
      "CHM Path: /home/jovyan/data-store/data/output/3dep/chm/superior_old_growth_chm.tif\n",
      "DEM Path: /home/jovyan/data-store/data/output/3dep/chm/superior_old_growth_dem.tif\n",
      "Output Directory: /home/jovyan/data-store/data/output/fractal_analysis\n"
     ]
    }
   ],
   "source": [
    "# Path to CHM raster (update this to your CHM file)\n",
    "CHM_PATH = Path(\"/home/jovyan/data-store/data/output/3dep/chm/superior_old_growth_chm.tif\")\n",
    "\n",
    "# Alternative: use a relative path\n",
    "# CHM_PATH = Path(\"./outputs/chm/monument_canyon_rna_chm.tif\")\n",
    "\n",
    "# Path to DEM for Hypothesis 5 (Biotic Decoupling)\n",
    "# Auto-detect: look for _dem.tif in same directory as CHM\n",
    "_auto_dem_path = CHM_PATH.parent / CHM_PATH.name.replace(\"_chm.tif\", \"_dem.tif\")\n",
    "if _auto_dem_path.exists():\n",
    "    DEM_PATH = _auto_dem_path\n",
    "    print(f\"Auto-detected DEM: {DEM_PATH}\")\n",
    "else:\n",
    "    # Manual override - set path here if DEM is elsewhere\n",
    "    DEM_PATH = None  # Path(\"/path/to/dem.tif\")\n",
    "\n",
    "# Output directory for results\n",
    "OUTPUT_DIR = Path(\"/home/jovyan/data-store/data/output/fractal_analysis\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# NoData value (will be read from raster if available)\n",
    "NODATA_VALUE = -9999\n",
    "\n",
    "# Gap threshold (meters) - pixels below this are considered \"gaps\"\n",
    "GAP_THRESHOLD = 2.0\n",
    "\n",
    "# Minimum height for Zeta distribution analysis (meters)\n",
    "# Filters out short pixel heights that skew power-law fitting\n",
    "# Note: Physical tree height limits cause tapered Pareto curves at upper end\n",
    "MIN_HEIGHT_ZETA = 2.0\n",
    "\n",
    "# Dominant tree detection threshold (meters) - for Hypothesis 4\n",
    "DOMINANT_TREE_THRESHOLD = 15.0\n",
    "\n",
    "# Local maxima detection window size (pixels)\n",
    "LOCAL_MAX_WINDOW = 11\n",
    "\n",
    "print(f\"CHM Path: {CHM_PATH}\")\n",
    "print(f\"DEM Path: {DEM_PATH}\")\n",
    "print(f\"Output Directory: {OUTPUT_DIR.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-header",
   "metadata": {},
   "source": [
    "## 3. Load CHM Raster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "load-chm",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CHM from: /home/jovyan/data-store/data/output/3dep/chm/superior_old_growth_chm.tif\n",
      "CHM Shape: (1, 23285, 17812)\n",
      "CHM CRS: EPSG:3857\n",
      "CHM Bounds: (-10027660.03, 6098540.34, -10018754.03, 6110182.84)\n",
      "NoData Value: nan\n",
      "Resolution: (0.5, -0.5)\n",
      "\n",
      "Valid pixels: 358,871,295 / 414,752,420\n",
      "Height range: -5.31 - 34.01 m\n",
      "Mean height: 7.47 m\n"
     ]
    }
   ],
   "source": [
    "# Load CHM\n",
    "print(f\"Loading CHM from: {CHM_PATH}\")\n",
    "\n",
    "if not CHM_PATH.exists():\n",
    "    raise FileNotFoundError(f\"CHM file not found: {CHM_PATH}\")\n",
    "\n",
    "chm = rio.open_rasterio(CHM_PATH, masked=True)\n",
    "\n",
    "# Get NoData value from raster\n",
    "if chm.rio.nodata is not None:\n",
    "    NODATA_VALUE = chm.rio.nodata\n",
    "\n",
    "# Get CRS and bounds\n",
    "chm_crs = chm.rio.crs\n",
    "chm_bounds = chm.rio.bounds()\n",
    "\n",
    "print(f\"CHM Shape: {chm.shape}\")\n",
    "print(f\"CHM CRS: {chm_crs}\")\n",
    "print(f\"CHM Bounds: {chm_bounds}\")\n",
    "print(f\"NoData Value: {NODATA_VALUE}\")\n",
    "print(f\"Resolution: {chm.rio.resolution()}\")\n",
    "\n",
    "# Basic statistics\n",
    "chm_data = chm.values.squeeze()\n",
    "valid_mask = ~np.isnan(chm_data) & (chm_data != NODATA_VALUE)\n",
    "valid_data = chm_data[valid_mask]\n",
    "\n",
    "print(f\"\\nValid pixels: {valid_mask.sum():,} / {chm_data.size:,}\")\n",
    "print(f\"Height range: {valid_data.min():.2f} - {valid_data.max():.2f} m\")\n",
    "print(f\"Mean height: {valid_data.mean():.2f} m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "map-header",
   "metadata": {},
   "source": [
    "## 4. Interactive AOI Selection\n",
    "\n",
    "Draw a rectangle on the map to select an Area of Interest (AOI) for analysis.\n",
    "The AOI will be used to subset the CHM for fractal analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "leaflet-map",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHM bounds (WGS84): (-90.08000268903417, 47.9499981722528, -89.9999987298305, 48.02000043594405)\n",
      "Map center: (47.9850, -90.0400)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e7625c1a1a74e8b97acaf1514419b5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<b>Instructions:</b> Draw a rectangle on the map to select your AOI. The yellow boundary shows the\u2026"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63fc34940db44119aba3bd8f31f5317d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map(center=[47.98499930409842, -90.04000070943233], controls=(ZoomControl(options=['position', 'zoom_in_text',\u2026"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Transform bounds to WGS84 for Leaflet\n",
    "def transform_bounds_to_wgs84(bounds, src_crs):\n",
    "    \"\"\"Transform bounds from source CRS to WGS84.\"\"\"\n",
    "    transformer = pyproj.Transformer.from_crs(src_crs, \"EPSG:4326\", always_xy=True)\n",
    "    west, south, east, north = bounds\n",
    "    west_wgs, south_wgs = transformer.transform(west, south)\n",
    "    east_wgs, north_wgs = transformer.transform(east, north)\n",
    "    return (west_wgs, south_wgs, east_wgs, north_wgs)\n",
    "\n",
    "# Get WGS84 bounds\n",
    "bounds_wgs84 = transform_bounds_to_wgs84(chm_bounds, chm_crs)\n",
    "center_lat = (bounds_wgs84[1] + bounds_wgs84[3]) / 2\n",
    "center_lon = (bounds_wgs84[0] + bounds_wgs84[2]) / 2\n",
    "\n",
    "print(f\"CHM bounds (WGS84): {bounds_wgs84}\")\n",
    "print(f\"Map center: ({center_lat:.4f}, {center_lon:.4f})\")\n",
    "\n",
    "# Store selected AOI\n",
    "selected_aoi = {'geometry': None, 'bounds_native': None}\n",
    "\n",
    "def handle_draw(target, action, geo_json):\n",
    "    \"\"\"Handle drawing events from the map.\"\"\"\n",
    "    if action == 'created':\n",
    "        geom = shape(geo_json['geometry'])\n",
    "        selected_aoi['geometry'] = geom\n",
    "        \n",
    "        # Transform to native CRS\n",
    "        transformer = pyproj.Transformer.from_crs(\"EPSG:4326\", chm_crs, always_xy=True)\n",
    "        geom_native = transform(transformer.transform, geom)\n",
    "        selected_aoi['bounds_native'] = geom_native.bounds\n",
    "        \n",
    "        print(f\"AOI selected!\")\n",
    "        print(f\"  WGS84 bounds: {geom.bounds}\")\n",
    "        print(f\"  Native CRS bounds: {geom_native.bounds}\")\n",
    "\n",
    "# Create map\n",
    "m = Map(\n",
    "    basemap=basemaps.Esri.WorldImagery,\n",
    "    center=(center_lat, center_lon),\n",
    "    zoom=14\n",
    ")\n",
    "\n",
    "# Add CHM boundary\n",
    "chm_boundary = {\n",
    "    \"type\": \"Feature\",\n",
    "    \"geometry\": {\n",
    "        \"type\": \"Polygon\",\n",
    "        \"coordinates\": [[\n",
    "            [bounds_wgs84[0], bounds_wgs84[1]],\n",
    "            [bounds_wgs84[2], bounds_wgs84[1]],\n",
    "            [bounds_wgs84[2], bounds_wgs84[3]],\n",
    "            [bounds_wgs84[0], bounds_wgs84[3]],\n",
    "            [bounds_wgs84[0], bounds_wgs84[1]]\n",
    "        ]]\n",
    "    },\n",
    "    \"properties\": {\"name\": \"CHM Extent\"}\n",
    "}\n",
    "\n",
    "boundary_layer = GeoJSON(\n",
    "    data=chm_boundary,\n",
    "    style={'color': 'yellow', 'weight': 2, 'fillOpacity': 0.0}\n",
    ")\n",
    "m.add_layer(boundary_layer)\n",
    "\n",
    "# Add draw control\n",
    "draw_control = DrawControl(\n",
    "    polygon={},\n",
    "    rectangle={'shapeOptions': {'color': '#00ff00', 'weight': 2}},\n",
    "    polyline={},\n",
    "    circle={},\n",
    "    circlemarker={}\n",
    ")\n",
    "draw_control.on_draw(handle_draw)\n",
    "m.add_control(draw_control)\n",
    "\n",
    "# Instructions widget\n",
    "instructions = widgets.HTML(\n",
    "    value=\"<b>Instructions:</b> Draw a rectangle on the map to select your AOI. \"\n",
    "          \"The yellow boundary shows the CHM extent.\"\n",
    ")\n",
    "\n",
    "display(instructions)\n",
    "display(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "extract-aoi",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using selected AOI\n",
      "\n",
      "Subset shape: (623, 785)\n",
      "Valid pixels: 450,544 (92.1%)\n",
      "NoData pixels: 38,511 (7.9%)\n",
      "Pixel resolution: 0.500m\n"
     ]
    }
   ],
   "source": [
    "# Extract CHM subset for selected AOI\n",
    "# If no AOI selected, use the entire CHM\n",
    "\n",
    "if selected_aoi['bounds_native'] is not None:\n",
    "    # Clip to AOI\n",
    "    west, south, east, north = selected_aoi['bounds_native']\n",
    "    chm_subset = chm.rio.clip_box(minx=west, miny=south, maxx=east, maxy=north)\n",
    "    print(f\"Using selected AOI\")\n",
    "else:\n",
    "    # Use entire CHM\n",
    "    chm_subset = chm\n",
    "    print(f\"No AOI selected - using entire CHM\")\n",
    "\n",
    "# Get the data array\n",
    "chm_array = chm_subset.values.squeeze().astype(np.float64)\n",
    "\n",
    "# Create masks\n",
    "nodata_mask = np.isnan(chm_array) | (chm_array == NODATA_VALUE)\n",
    "valid_mask = ~nodata_mask\n",
    "\n",
    "# Get pixel resolution\n",
    "res_x, res_y = abs(chm_subset.rio.resolution()[0]), abs(chm_subset.rio.resolution()[1])\n",
    "pixel_resolution = (res_x + res_y) / 2\n",
    "\n",
    "print(f\"\\nSubset shape: {chm_array.shape}\")\n",
    "print(f\"Valid pixels: {valid_mask.sum():,} ({100*valid_mask.sum()/chm_array.size:.1f}%)\")\n",
    "print(f\"NoData pixels: {nodata_mask.sum():,} ({100*nodata_mask.sum()/chm_array.size:.1f}%)\")\n",
    "print(f\"Pixel resolution: {pixel_resolution:.3f}m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h1-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# HYPOTHESIS 1: Optimal Filling\n",
    "\n",
    "**Prediction:** Old-growth forests \"maximize light interception while minimizing self-shading,\" producing fractal dimensions substantially greater than disturbed or managed stands.\n",
    "\n",
    "**Method:** Differential Box Counting (DBC) for self-affine surfaces\n",
    "\n",
    "**Expected Results:**\n",
    "- D \u2208 [2.0, 2.3]: Plantation or young forest (simple structure)\n",
    "- D \u2208 [2.3, 2.5]: Disturbed forest (moderate complexity)\n",
    "- D \u2208 [2.5, 2.8]: Old-growth forest (high complexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dbc-functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def differential_box_counting(img, valid_mask, scales=None, min_valid_frac=0.8):\n",
    "    \"\"\"\n",
    "    Compute fractal dimension using Differential Box Counting (Sarkar & Chaudhuri 1994).\n",
    "    \n",
    "    Valid range: D in [2, 3]\n",
    "    - D = 2: Perfectly smooth planar surface\n",
    "    - D = 3: Maximally rough space-filling surface\n",
    "    \"\"\"\n",
    "    rows, cols = img.shape\n",
    "    M = min(rows, cols)\n",
    "\n",
    "    valid_values = img[valid_mask]\n",
    "    z_min_global, z_max_global = valid_values.min(), valid_values.max()\n",
    "    z_range_global = z_max_global - z_min_global\n",
    "\n",
    "    if z_range_global <= 0:\n",
    "        return np.nan, 0.0, np.array([]), np.array([])\n",
    "\n",
    "    if scales is None:\n",
    "        max_scale = M // 4\n",
    "        scales = [2**i for i in range(1, 10) if 2**i <= max_scale]\n",
    "\n",
    "    Ns_list = []\n",
    "    valid_scales = []\n",
    "\n",
    "    for s in scales:\n",
    "        nx = cols // s\n",
    "        ny = rows // s\n",
    "\n",
    "        if nx < 1 or ny < 1:\n",
    "            continue\n",
    "\n",
    "        N_total = 0\n",
    "        valid_boxes = 0\n",
    "\n",
    "        for i in range(ny):\n",
    "            for j in range(nx):\n",
    "                y_start, y_end = i * s, (i + 1) * s\n",
    "                x_start, x_end = j * s, (j + 1) * s\n",
    "\n",
    "                box_data = img[y_start:y_end, x_start:x_end]\n",
    "                box_valid = valid_mask[y_start:y_end, x_start:x_end]\n",
    "\n",
    "                valid_frac = box_valid.sum() / (s * s)\n",
    "                if valid_frac < min_valid_frac:\n",
    "                    continue\n",
    "\n",
    "                valid_boxes += 1\n",
    "                valid_heights = box_data[box_valid]\n",
    "                z_min_box = valid_heights.min()\n",
    "                z_max_box = valid_heights.max()\n",
    "\n",
    "                z_min_scaled = (z_min_box - z_min_global) / z_range_global * M\n",
    "                z_max_scaled = (z_max_box - z_min_global) / z_range_global * M\n",
    "\n",
    "                k = int(np.floor(z_min_scaled / s))\n",
    "                l = int(np.floor(z_max_scaled / s))\n",
    "                n = l - k + 1\n",
    "\n",
    "                N_total += n\n",
    "\n",
    "        if valid_boxes > 0:\n",
    "            Ns_list.append(N_total)\n",
    "            valid_scales.append(s)\n",
    "\n",
    "    if len(valid_scales) < 3:\n",
    "        return np.nan, 0.0, np.array([]), np.array([])\n",
    "\n",
    "    log_inv_s = np.log(1.0 / np.array(valid_scales))\n",
    "    log_Ns = np.log(np.array(Ns_list))\n",
    "\n",
    "    coeffs = np.polyfit(log_inv_s, log_Ns, 1)\n",
    "    D = coeffs[0]\n",
    "\n",
    "    predicted = np.polyval(coeffs, log_inv_s)\n",
    "    ss_res = np.sum((log_Ns - predicted) ** 2)\n",
    "    ss_tot = np.sum((log_Ns - np.mean(log_Ns)) ** 2)\n",
    "    r2 = 1 - (ss_res / ss_tot) if ss_tot > 0 else 0\n",
    "\n",
    "    return D, r2, np.array(valid_scales), np.array(Ns_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "run-h1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "HYPOTHESIS 1: OPTIMAL FILLING\n",
      "======================================================================\n",
      "\n",
      "Running Differential Box Counting (DBC)...\n",
      "\n",
      "\n",
      "DBC Fractal Dimension: D = 2.4805\n",
      "R-squared: 0.9905\n",
      "\n",
      "D = 2.48 suggests DISTURBED forest (moderate complexity)\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"HYPOTHESIS 1: OPTIMAL FILLING\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nRunning Differential Box Counting (DBC)...\\n\")\n",
    "\n",
    "D_dbc, r2_dbc, scales_dbc, Ns_dbc = differential_box_counting(\n",
    "    chm_array, valid_mask, min_valid_frac=0.8\n",
    ")\n",
    "\n",
    "print(f\"\\nDBC Fractal Dimension: D = {D_dbc:.4f}\")\n",
    "print(f\"R-squared: {r2_dbc:.4f}\")\n",
    "\n",
    "# Classification\n",
    "if np.isnan(D_dbc):\n",
    "    h1_classification = \"INSUFFICIENT_DATA\"\n",
    "    h1_interpretation = \"Insufficient data for analysis\"\n",
    "elif D_dbc >= 2.5:\n",
    "    h1_classification = \"OLD_GROWTH\"\n",
    "    h1_interpretation = f\"D = {D_dbc:.2f} suggests OLD GROWTH (high structural complexity)\"\n",
    "elif D_dbc >= 2.3:\n",
    "    h1_classification = \"DISTURBED\"\n",
    "    h1_interpretation = f\"D = {D_dbc:.2f} suggests DISTURBED forest (moderate complexity)\"\n",
    "else:\n",
    "    h1_classification = \"PLANTATION\"\n",
    "    h1_interpretation = f\"D = {D_dbc:.2f} suggests PLANTATION/Managed (simpler structure)\"\n",
    "\n",
    "print(f\"\\n{h1_interpretation}\")\n",
    "\n",
    "# Store results\n",
    "h1_results = {\n",
    "    'hypothesis': 'H1: Optimal Filling',\n",
    "    'method': 'Differential Box Counting (DBC)',\n",
    "    'D_dbc': float(D_dbc) if not np.isnan(D_dbc) else None,\n",
    "    'r_squared': float(r2_dbc),\n",
    "    'classification': h1_classification,\n",
    "    'interpretation': h1_interpretation,\n",
    "    'supports_hypothesis': h1_classification == 'OLD_GROWTH'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h2-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# HYPOTHESIS 2: Scale Invariance\n",
    "\n",
    "**Prediction:** Disruption introduces identifiable gap sizes that interrupt scaling patterns, while steady-state forests show \"scale invariance (gaps of all sizes following strict power-law decay).\"\n",
    "\n",
    "**Method:** Lacunarity analysis using Gliding Box Algorithm\n",
    "\n",
    "**Expected Results:**\n",
    "- High R\u00b2 (>0.95) in log-log lacunarity plot: Scale invariance (old-growth)\n",
    "- Moderate R\u00b2 (0.7-0.95): Partial scale invariance (disturbance recovery)\n",
    "- Low R\u00b2 (<0.7): Characteristic scales present (plantation/recent disturbance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "lacunarity-functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gliding_box_lacunarity(img, valid_mask, box_sizes=None, min_valid_fraction=0.5):\n",
    "    \"\"\"\n",
    "    Calculate lacunarity using the Gliding Box Algorithm.\n",
    "    \n",
    "    Lacunarity: L(r) = (sigma^2 / mu^2) + 1\n",
    "    Scale invariance: L(r) follows power law in log-log space\n",
    "    \"\"\"\n",
    "    rows, cols = img.shape\n",
    "    \n",
    "    img_clean = img.copy()\n",
    "    img_clean[~valid_mask] = np.nan\n",
    "    \n",
    "    if box_sizes is None:\n",
    "        max_size = min(rows, cols) // 4\n",
    "        box_sizes = [2**i for i in range(1, 10) if 2**i <= max_size]\n",
    "    \n",
    "    lacunarity_values = []\n",
    "    valid_sizes = []\n",
    "    \n",
    "    for r in box_sizes:\n",
    "        box_sums = []\n",
    "        \n",
    "        for i in range(0, rows - r + 1, max(1, r // 2)):\n",
    "            for j in range(0, cols - r + 1, max(1, r // 2)):\n",
    "                box = img_clean[i:i+r, j:j+r]\n",
    "                box_valid = valid_mask[i:i+r, j:j+r]\n",
    "                \n",
    "                valid_fraction = box_valid.sum() / (r * r)\n",
    "                if valid_fraction < min_valid_fraction:\n",
    "                    continue\n",
    "                \n",
    "                box_sum = np.nansum(box)\n",
    "                box_sums.append(box_sum)\n",
    "        \n",
    "        if len(box_sums) < 10:\n",
    "            continue\n",
    "        \n",
    "        box_sums = np.array(box_sums)\n",
    "        mu = np.mean(box_sums)\n",
    "        sigma2 = np.var(box_sums)\n",
    "        \n",
    "        L = (sigma2 / (mu ** 2)) + 1 if mu > 0 else 1.0\n",
    "        \n",
    "        lacunarity_values.append(L)\n",
    "        valid_sizes.append(r)\n",
    "    \n",
    "    if len(valid_sizes) < 2:\n",
    "        return np.array([]), np.array([]), 0.0, 0.0\n",
    "    \n",
    "    log_r = np.log(np.array(valid_sizes))\n",
    "    log_L = np.log(np.array(lacunarity_values))\n",
    "    \n",
    "    coeffs = np.polyfit(log_r, log_L, 1)\n",
    "    slope = coeffs[0]\n",
    "    predicted = np.polyval(coeffs, log_r)\n",
    "    ss_res = np.sum((log_L - predicted) ** 2)\n",
    "    ss_tot = np.sum((log_L - np.mean(log_L)) ** 2)\n",
    "    r2 = 1 - (ss_res / ss_tot) if ss_tot > 0 else 0\n",
    "    \n",
    "    return np.array(lacunarity_values), np.array(valid_sizes), r2, slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "run-h2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "HYPOTHESIS 2: SCALE INVARIANCE\n",
      "======================================================================\n",
      "\n",
      "Running Lacunarity Analysis (Gliding Box Algorithm)...\n",
      "\n",
      "Lacunarity R\u00b2 (log-log linearity): 0.9899\n",
      "Lacunarity slope: -0.0669\n",
      "\n",
      "R\u00b2 = 0.99 -> Strong scale invariance (OLD GROWTH signature)\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"HYPOTHESIS 2: SCALE INVARIANCE\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nRunning Lacunarity Analysis (Gliding Box Algorithm)...\\n\")\n",
    "\n",
    "lacunarity, lac_sizes, r2_lac, lac_slope = gliding_box_lacunarity(chm_array, valid_mask)\n",
    "\n",
    "print(f\"Lacunarity R\u00b2 (log-log linearity): {r2_lac:.4f}\")\n",
    "print(f\"Lacunarity slope: {lac_slope:.4f}\")\n",
    "\n",
    "# Classification\n",
    "if r2_lac > 0.95:\n",
    "    h2_classification = \"SCALE_INVARIANT\"\n",
    "    h2_interpretation = f\"R\u00b2 = {r2_lac:.2f} -> Strong scale invariance (OLD GROWTH signature)\"\n",
    "elif r2_lac > 0.70:\n",
    "    h2_classification = \"PARTIAL_INVARIANCE\"\n",
    "    h2_interpretation = f\"R\u00b2 = {r2_lac:.2f} -> Moderate scale invariance (DISTURBANCE recovery)\"\n",
    "else:\n",
    "    h2_classification = \"CHARACTERISTIC_SCALES\"\n",
    "    h2_interpretation = f\"R\u00b2 = {r2_lac:.2f} -> Weak scale invariance (PLANTATION or recent disturbance)\"\n",
    "\n",
    "print(f\"\\n{h2_interpretation}\")\n",
    "\n",
    "h2_results = {\n",
    "    'hypothesis': 'H2: Scale Invariance',\n",
    "    'method': 'Lacunarity (Gliding Box)',\n",
    "    'r_squared': float(r2_lac),\n",
    "    'slope': float(lac_slope),\n",
    "    'classification': h2_classification,\n",
    "    'interpretation': h2_interpretation,\n",
    "    'supports_hypothesis': h2_classification == 'SCALE_INVARIANT'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h3-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# HYPOTHESIS 3: Zeta Distribution\n",
    "\n",
    "**Prediction:** Canopy gaps in mature forests follow a power-law distribution where the exponent approximately equals 2.0, \"related to \u03b6(2),\" connecting to mathematical packing theory.\n",
    "\n",
    "**Method:** Gap size distribution fitting (power-law vs exponential)\n",
    "\n",
    "**Expected Results:**\n",
    "- Power-law with \u03b1 \u2248 2.0: Old-growth (optimal packing, related to \u03b6(2) = \u03c0\u00b2/6)\n",
    "- Power-law with \u03b1 \u2260 2.0: Disturbance-modified gap dynamics\n",
    "- Exponential distribution: Plantation or uniform management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "gap-analysis-functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_gaps(chm, valid_mask, gap_threshold=2.0, min_gap_size=1):\n",
    "    \"\"\"Identify contiguous gap regions in the CHM.\"\"\"\n",
    "    gap_mask = (chm < gap_threshold) & valid_mask\n",
    "    labeled, num_features = ndimage.label(gap_mask)\n",
    "    gap_areas = ndimage.sum(gap_mask, labeled, range(1, num_features + 1))\n",
    "    gap_areas = gap_areas[gap_areas >= min_gap_size]\n",
    "    return gap_areas, labeled\n",
    "\n",
    "\n",
    "def fit_gap_distribution(gap_areas, resolution=1.0):\n",
    "    \"\"\"Fit power law and exponential distributions to gap sizes.\"\"\"\n",
    "    areas = gap_areas * (resolution ** 2)\n",
    "    areas = areas[areas > 0]\n",
    "    \n",
    "    if len(areas) < 10:\n",
    "        return {'error': 'Insufficient gaps for analysis'}\n",
    "    \n",
    "    sorted_areas = np.sort(areas)\n",
    "    ecdf = np.arange(1, len(sorted_areas) + 1) / len(sorted_areas)\n",
    "    ccdf = 1 - ecdf\n",
    "    \n",
    "    # Power law fit\n",
    "    try:\n",
    "        mask = ccdf > 0.01\n",
    "        log_x = np.log(sorted_areas[mask])\n",
    "        log_ccdf = np.log(ccdf[mask])\n",
    "        \n",
    "        pl_coeffs = np.polyfit(log_x, log_ccdf, 1)\n",
    "        alpha_pl = -pl_coeffs[0] + 1\n",
    "        \n",
    "        predicted_pl = np.polyval(pl_coeffs, log_x)\n",
    "        ss_res_pl = np.sum((log_ccdf - predicted_pl) ** 2)\n",
    "        ss_tot = np.sum((log_ccdf - np.mean(log_ccdf)) ** 2)\n",
    "        r2_pl = 1 - (ss_res_pl / ss_tot) if ss_tot > 0 else 0\n",
    "    except:\n",
    "        alpha_pl = np.nan\n",
    "        r2_pl = 0\n",
    "    \n",
    "    # Exponential fit\n",
    "    try:\n",
    "        x_vals = sorted_areas[mask]\n",
    "        log_ccdf_exp = np.log(ccdf[mask])\n",
    "        \n",
    "        exp_coeffs = np.polyfit(x_vals, log_ccdf_exp, 1)\n",
    "        beta_exp = -exp_coeffs[0]\n",
    "        \n",
    "        predicted_exp = np.polyval(exp_coeffs, x_vals)\n",
    "        ss_res_exp = np.sum((log_ccdf_exp - predicted_exp) ** 2)\n",
    "        r2_exp = 1 - (ss_res_exp / ss_tot) if ss_tot > 0 else 0\n",
    "    except:\n",
    "        beta_exp = np.nan\n",
    "        r2_exp = 0\n",
    "    \n",
    "    return {\n",
    "        'n_gaps': len(areas),\n",
    "        'min_area': areas.min(),\n",
    "        'max_area': areas.max(),\n",
    "        'median_area': np.median(areas),\n",
    "        'power_law': {'alpha': alpha_pl, 'r2': r2_pl},\n",
    "        'exponential': {'beta': beta_exp, 'r2': r2_exp},\n",
    "        'zeta_2': np.pi**2 / 6,\n",
    "        'sorted_areas': sorted_areas,\n",
    "        'ccdf': ccdf\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "run-h3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "HYPOTHESIS 3: ZETA DISTRIBUTION\n",
      "======================================================================\n",
      "\n",
      "Running Gap Size Distribution Analysis...\n",
      "\n",
      "Number of gaps: 4753\n",
      "Gap area range: 0.25 - 86.25 m\u00b2\n",
      "\n",
      "Power Law Fit:\n",
      "  \u03b1 (exponent): 1.8404\n",
      "  R\u00b2: 0.9607\n",
      "\n",
      "Exponential Fit:\n",
      "  R\u00b2: 0.7864\n",
      "\n",
      "Zeta Connection:\n",
      "  \u03b6(2) = \u03c0\u00b2/6 \u2248 1.6449\n",
      "  |\u03b1 - 2| = 0.1596\n",
      "\n",
      "Power law \u03b1 \u2248 1.84 close to 2.0 -> OLD GROWTH signature\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"HYPOTHESIS 3: ZETA DISTRIBUTION\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nRunning Gap Size Distribution Analysis...\\n\")\n",
    "\n",
    "# Filter to only analyze canopy pixels above MIN_HEIGHT_ZETA\n",
    "# This excludes short pixel heights that skew the Zeta distribution\n",
    "zeta_valid_mask = valid_mask & (chm_array >= MIN_HEIGHT_ZETA)\n",
    "gap_areas, labeled_gaps = identify_gaps(chm_array, zeta_valid_mask, GAP_THRESHOLD)\n",
    "gap_results = fit_gap_distribution(gap_areas, pixel_resolution)\n",
    "\n",
    "if 'error' in gap_results:\n",
    "    print(f\"Error: {gap_results['error']}\")\n",
    "    h3_classification = \"INSUFFICIENT_DATA\"\n",
    "    h3_interpretation = gap_results['error']\n",
    "else:\n",
    "    print(f\"Number of gaps: {gap_results['n_gaps']}\")\n",
    "    print(f\"Gap area range: {gap_results['min_area']:.2f} - {gap_results['max_area']:.2f} m\u00b2\")\n",
    "    print(f\"\\nPower Law Fit:\")\n",
    "    print(f\"  \u03b1 (exponent): {gap_results['power_law']['alpha']:.4f}\")\n",
    "    print(f\"  R\u00b2: {gap_results['power_law']['r2']:.4f}\")\n",
    "    print(f\"\\nExponential Fit:\")\n",
    "    print(f\"  R\u00b2: {gap_results['exponential']['r2']:.4f}\")\n",
    "    print(f\"\\nZeta Connection:\")\n",
    "    print(f\"  \u03b6(2) = \u03c0\u00b2/6 \u2248 {gap_results['zeta_2']:.4f}\")\n",
    "    print(f\"  |\u03b1 - 2| = {abs(gap_results['power_law']['alpha'] - 2.0):.4f}\")\n",
    "    \n",
    "    pl_r2 = gap_results['power_law']['r2']\n",
    "    exp_r2 = gap_results['exponential']['r2']\n",
    "    alpha = gap_results['power_law']['alpha']\n",
    "    \n",
    "    if pl_r2 > exp_r2 and pl_r2 > 0.8:\n",
    "        if 1.8 <= alpha <= 2.2:\n",
    "            h3_classification = \"ZETA_DISTRIBUTED\"\n",
    "            h3_interpretation = f\"Power law \u03b1 \u2248 {alpha:.2f} close to 2.0 -> OLD GROWTH signature\"\n",
    "        else:\n",
    "            h3_classification = \"POWER_LAW_MODIFIED\"\n",
    "            h3_interpretation = f\"Power law \u03b1 = {alpha:.2f} deviates from 2.0 -> DISTURBANCE modified\"\n",
    "    else:\n",
    "        h3_classification = \"EXPONENTIAL\"\n",
    "        h3_interpretation = f\"Exponential fits better (R\u00b2={exp_r2:.2f}) -> PLANTATION signature\"\n",
    "    \n",
    "    print(f\"\\n{h3_interpretation}\")\n",
    "\n",
    "h3_results = {\n",
    "    'hypothesis': 'H3: Zeta Distribution',\n",
    "    'method': 'Gap Size Distribution',\n",
    "    'n_gaps': gap_results.get('n_gaps', 0),\n",
    "    'power_law_alpha': float(gap_results.get('power_law', {}).get('alpha', np.nan)),\n",
    "    'power_law_r2': float(gap_results.get('power_law', {}).get('r2', 0)),\n",
    "    'exponential_r2': float(gap_results.get('exponential', {}).get('r2', 0)),\n",
    "    'zeta_2': float(np.pi**2 / 6),\n",
    "    'alpha_deviation_from_2': float(abs(gap_results.get('power_law', {}).get('alpha', 0) - 2.0)),\n",
    "    'classification': h3_classification,\n",
    "    'interpretation': h3_interpretation,\n",
    "    'supports_hypothesis': h3_classification == 'ZETA_DISTRIBUTED'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h4-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# HYPOTHESIS 4: Universal Repulsion\n",
    "\n",
    "**Prediction:** Dominant tree apex spacing matches the Wigner-Dyson distribution, indicating \"rigid repulsion rather than random Poisson placement\" consistent with random matrix statistics.\n",
    "\n",
    "**Method:** Nearest neighbor spacing distribution of tree tops\n",
    "\n",
    "**Expected Results:**\n",
    "- Wigner-Dyson (GUE): P(s) ~ s\u00b2 exp(-4s\u00b2/\u03c0) -> Repulsion (competition-structured, old-growth)\n",
    "- Poisson: P(s) ~ exp(-s) -> Random placement (young or disturbed forest)\n",
    "- Clustering: Peak at small s -> Regeneration patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "tree-detection-functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_local_maxima(img, valid_mask, window_size=11, height_threshold=15.0):\n",
    "    \"\"\"\n",
    "    Detect local maxima (tree tops) in the CHM.\n",
    "    \n",
    "    Returns coordinates of dominant tree apices.\n",
    "    \"\"\"\n",
    "    # Apply height threshold\n",
    "    img_masked = np.where(valid_mask & (img >= height_threshold), img, -np.inf)\n",
    "    \n",
    "    # Find local maxima using maximum filter\n",
    "    local_max = ndimage.maximum_filter(img_masked, size=window_size)\n",
    "    is_peak = (img_masked == local_max) & (img_masked > -np.inf)\n",
    "    \n",
    "    # Get coordinates\n",
    "    peak_coords = np.array(np.where(is_peak)).T  # (N, 2) array of (row, col)\n",
    "    peak_heights = img[is_peak]\n",
    "    \n",
    "    return peak_coords, peak_heights\n",
    "\n",
    "\n",
    "def wigner_dyson_gue(s):\n",
    "    \"\"\"Wigner-Dyson distribution for GUE (Gaussian Unitary Ensemble).\"\"\"\n",
    "    return (32 / np.pi**2) * s**2 * np.exp(-4 * s**2 / np.pi)\n",
    "\n",
    "\n",
    "def poisson_spacing(s):\n",
    "    \"\"\"Poisson (random) spacing distribution.\"\"\"\n",
    "    return np.exp(-s)\n",
    "\n",
    "\n",
    "def compute_nearest_neighbor_spacing(coords, resolution=1.0):\n",
    "    \"\"\"\n",
    "    Compute nearest neighbor spacing distribution.\n",
    "    \n",
    "    Returns normalized spacings (s = spacing / mean_spacing).\n",
    "    \"\"\"\n",
    "    if len(coords) < 10:\n",
    "        return np.array([]), 0.0\n",
    "    \n",
    "    # Convert to physical coordinates\n",
    "    coords_physical = coords * resolution\n",
    "    \n",
    "    # Build KD-tree\n",
    "    tree = cKDTree(coords_physical)\n",
    "    \n",
    "    # Find nearest neighbor distances (k=2 because first is self)\n",
    "    distances, _ = tree.query(coords_physical, k=2)\n",
    "    nn_distances = distances[:, 1]  # Second column is nearest neighbor\n",
    "    \n",
    "    # Normalize by mean spacing\n",
    "    mean_spacing = np.mean(nn_distances)\n",
    "    normalized_spacings = nn_distances / mean_spacing\n",
    "    \n",
    "    return normalized_spacings, mean_spacing\n",
    "\n",
    "\n",
    "def fit_spacing_distribution(spacings):\n",
    "    \"\"\"\n",
    "    Compare spacing distribution to Wigner-Dyson and Poisson.\n",
    "    \n",
    "    Uses Kolmogorov-Smirnov test.\n",
    "    \"\"\"\n",
    "    if len(spacings) < 10:\n",
    "        return {'error': 'Insufficient trees for analysis'}\n",
    "    \n",
    "    # Create empirical histogram\n",
    "    s_range = np.linspace(0.01, 3.0, 100)\n",
    "    \n",
    "    # Generate theoretical samples\n",
    "    n_samples = len(spacings) * 10\n",
    "    \n",
    "    # Wigner-Dyson samples (inverse transform sampling approximation)\n",
    "    # Using rejection sampling\n",
    "    wd_samples = []\n",
    "    while len(wd_samples) < n_samples:\n",
    "        s = np.random.uniform(0, 4, 1000)\n",
    "        p = wigner_dyson_gue(s)\n",
    "        accept = np.random.uniform(0, 1, 1000) < p / 0.5  # 0.5 is approx max\n",
    "        wd_samples.extend(s[accept].tolist())\n",
    "    wd_samples = np.array(wd_samples[:n_samples])\n",
    "    \n",
    "    # Poisson samples\n",
    "    poisson_samples = np.random.exponential(1.0, n_samples)\n",
    "    \n",
    "    # KS tests\n",
    "    ks_wd, p_wd = ks_2samp(spacings, wd_samples)\n",
    "    ks_poisson, p_poisson = ks_2samp(spacings, poisson_samples)\n",
    "    \n",
    "    # Compute histogram for visualization\n",
    "    hist, bin_edges = np.histogram(spacings, bins=30, density=True, range=(0, 3))\n",
    "    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "    \n",
    "    return {\n",
    "        'n_trees': len(spacings),\n",
    "        'ks_wigner_dyson': ks_wd,\n",
    "        'p_wigner_dyson': p_wd,\n",
    "        'ks_poisson': ks_poisson,\n",
    "        'p_poisson': p_poisson,\n",
    "        'histogram': hist,\n",
    "        'bin_centers': bin_centers,\n",
    "        'spacings': spacings\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "run-h4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "HYPOTHESIS 4: UNIVERSAL REPULSION\n",
      "======================================================================\n",
      "\n",
      "Detecting dominant tree apices...\n",
      "\n",
      "Dominant trees detected: 1715\n",
      "Height threshold: >= 15.0m\n",
      "Detection window: 11 pixels\n",
      "Tree height range: 15.0 - 28.6m\n",
      "Mean nearest neighbor distance: 2.49m\n",
      "\n",
      "Wigner-Dyson (GUE) fit:\n",
      "  KS statistic: 0.6167\n",
      "  p-value: 0.0000\n",
      "\n",
      "Poisson (random) fit:\n",
      "  KS statistic: 0.4334\n",
      "  p-value: 0.0000\n",
      "\n",
      "Neither distribution fits well -> Complex spatial structure\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"HYPOTHESIS 4: UNIVERSAL REPULSION\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nDetecting dominant tree apices...\\n\")\n",
    "\n",
    "# Detect tree tops\n",
    "tree_coords, tree_heights = detect_local_maxima(\n",
    "    chm_array, valid_mask, \n",
    "    window_size=LOCAL_MAX_WINDOW, \n",
    "    height_threshold=DOMINANT_TREE_THRESHOLD\n",
    ")\n",
    "\n",
    "print(f\"Dominant trees detected: {len(tree_coords)}\")\n",
    "print(f\"Height threshold: >= {DOMINANT_TREE_THRESHOLD}m\")\n",
    "print(f\"Detection window: {LOCAL_MAX_WINDOW} pixels\")\n",
    "\n",
    "if len(tree_coords) >= 10:\n",
    "    print(f\"Tree height range: {tree_heights.min():.1f} - {tree_heights.max():.1f}m\")\n",
    "    \n",
    "    # Compute spacing distribution\n",
    "    spacings, mean_spacing = compute_nearest_neighbor_spacing(tree_coords, pixel_resolution)\n",
    "    print(f\"Mean nearest neighbor distance: {mean_spacing:.2f}m\")\n",
    "    \n",
    "    # Fit distributions\n",
    "    spacing_results = fit_spacing_distribution(spacings)\n",
    "    \n",
    "    print(f\"\\nWigner-Dyson (GUE) fit:\")\n",
    "    print(f\"  KS statistic: {spacing_results['ks_wigner_dyson']:.4f}\")\n",
    "    print(f\"  p-value: {spacing_results['p_wigner_dyson']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nPoisson (random) fit:\")\n",
    "    print(f\"  KS statistic: {spacing_results['ks_poisson']:.4f}\")\n",
    "    print(f\"  p-value: {spacing_results['p_poisson']:.4f}\")\n",
    "    \n",
    "    # Classification\n",
    "    ks_wd = spacing_results['ks_wigner_dyson']\n",
    "    ks_po = spacing_results['ks_poisson']\n",
    "    \n",
    "    if ks_wd < ks_po:\n",
    "        if spacing_results['p_wigner_dyson'] > 0.05:\n",
    "            h4_classification = \"WIGNER_DYSON\"\n",
    "            h4_interpretation = f\"Spacing follows Wigner-Dyson (p={spacing_results['p_wigner_dyson']:.3f}) -> REPULSION (OLD GROWTH)\"\n",
    "        else:\n",
    "            h4_classification = \"REPULSION_PARTIAL\"\n",
    "            h4_interpretation = f\"Closer to Wigner-Dyson but not significant -> PARTIAL repulsion\"\n",
    "    else:\n",
    "        if spacing_results['p_poisson'] > 0.05:\n",
    "            h4_classification = \"POISSON\"\n",
    "            h4_interpretation = f\"Spacing follows Poisson (p={spacing_results['p_poisson']:.3f}) -> RANDOM placement\"\n",
    "        else:\n",
    "            h4_classification = \"NEITHER\"\n",
    "            h4_interpretation = f\"Neither distribution fits well -> Complex spatial structure\"\n",
    "    \n",
    "    print(f\"\\n{h4_interpretation}\")\n",
    "else:\n",
    "    spacing_results = {'error': 'Insufficient trees'}\n",
    "    h4_classification = \"INSUFFICIENT_DATA\"\n",
    "    h4_interpretation = f\"Only {len(tree_coords)} trees detected - need at least 10\"\n",
    "    mean_spacing = np.nan\n",
    "    spacings = np.array([])\n",
    "    print(f\"\\n{h4_interpretation}\")\n",
    "\n",
    "h4_results = {\n",
    "    'hypothesis': 'H4: Universal Repulsion',\n",
    "    'method': 'Nearest Neighbor Spacing Distribution',\n",
    "    'n_trees': len(tree_coords),\n",
    "    'height_threshold': DOMINANT_TREE_THRESHOLD,\n",
    "    'mean_spacing_m': float(mean_spacing) if not np.isnan(mean_spacing) else None,\n",
    "    'ks_wigner_dyson': float(spacing_results.get('ks_wigner_dyson', np.nan)),\n",
    "    'ks_poisson': float(spacing_results.get('ks_poisson', np.nan)),\n",
    "    'p_wigner_dyson': float(spacing_results.get('p_wigner_dyson', np.nan)),\n",
    "    'p_poisson': float(spacing_results.get('p_poisson', np.nan)),\n",
    "    'classification': h4_classification,\n",
    "    'interpretation': h4_interpretation,\n",
    "    'supports_hypothesis': h4_classification == 'WIGNER_DYSON'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h5-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# HYPOTHESIS 5: Biotic Decoupling\n",
    "\n",
    "**Prediction:** Established ecosystems \"buffer environmental constraints through niche construction,\" showing weaker correlations between canopy structure and topographic variables compared to younger or compromised stands.\n",
    "\n",
    "**Method:** Correlation analysis between CHM roughness and topography\n",
    "\n",
    "**With DEM (preferred):** Correlates CHM roughness with:\n",
    "- Elevation\n",
    "- Slope (degrees)\n",
    "- Topographic Position Index (TPI)\n",
    "- Terrain roughness\n",
    "\n",
    "**Without DEM (fallback):** Correlates CHM roughness with position (weaker test)\n",
    "\n",
    "**Expected Results:**\n",
    "- Low correlation (|r| < 0.3): Biotic decoupling (old-growth, self-organized)\n",
    "- Moderate correlation (0.3 < |r| < 0.6): Partial coupling (recovery stage)\n",
    "- High correlation (|r| > 0.6): Strong topographic control (young/disturbed)\n",
    "\n",
    "**Note:** For full H5 analysis, ensure DEM is generated alongside CHM in `chm_user_aoi.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "topographic-functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_topographic_variables(dem_array, valid_mask, resolution=1.0):\n",
    "    \"\"\"\n",
    "    Compute topographic variables from DEM: slope, aspect, TPI, roughness.\n",
    "    \"\"\"\n",
    "    # Compute gradient (slope components)\n",
    "    gy, gx = np.gradient(np.where(valid_mask, dem_array, np.nan), resolution)\n",
    "    \n",
    "    # Slope (degrees)\n",
    "    slope = np.degrees(np.arctan(np.sqrt(gx**2 + gy**2)))\n",
    "    slope[~valid_mask] = np.nan\n",
    "    \n",
    "    # Aspect (degrees from north, clockwise)\n",
    "    aspect = np.degrees(np.arctan2(-gx, gy))\n",
    "    aspect = np.where(aspect < 0, aspect + 360, aspect)\n",
    "    aspect[~valid_mask] = np.nan\n",
    "    \n",
    "    # Topographic Position Index (TPI) - difference from local mean\n",
    "    window_size = 15\n",
    "    local_mean = ndimage.uniform_filter(\n",
    "        np.where(valid_mask, dem_array, np.nan), \n",
    "        size=window_size, \n",
    "        mode='constant', \n",
    "        cval=np.nan\n",
    "    )\n",
    "    tpi = dem_array - local_mean\n",
    "    tpi[~valid_mask] = np.nan\n",
    "    \n",
    "    # Terrain roughness (local std dev)\n",
    "    roughness = ndimage.generic_filter(\n",
    "        np.where(valid_mask, dem_array, np.nan),\n",
    "        lambda x: np.nanstd(x) if np.sum(~np.isnan(x)) > 5 else np.nan,\n",
    "        size=window_size,\n",
    "        mode='constant',\n",
    "        cval=np.nan\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'elevation': dem_array,\n",
    "        'slope': slope,\n",
    "        'aspect': aspect,\n",
    "        'tpi': tpi,\n",
    "        'roughness': roughness\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_chm_structure_metrics(chm_array, valid_mask, resolution=1.0):\n",
    "    \"\"\"\n",
    "    Compute CHM structure metrics: local roughness, gradient, height.\n",
    "    \"\"\"\n",
    "    window_size = 15\n",
    "    \n",
    "    # CHM roughness (local std dev of heights)\n",
    "    chm_roughness = ndimage.generic_filter(\n",
    "        np.where(valid_mask, chm_array, np.nan),\n",
    "        lambda x: np.nanstd(x) if np.sum(~np.isnan(x)) > 5 else np.nan,\n",
    "        size=window_size,\n",
    "        mode='constant',\n",
    "        cval=np.nan\n",
    "    )\n",
    "    \n",
    "    # CHM gradient magnitude\n",
    "    gy, gx = np.gradient(np.where(valid_mask, chm_array, 0), resolution)\n",
    "    chm_gradient = np.sqrt(gx**2 + gy**2)\n",
    "    chm_gradient[~valid_mask] = np.nan\n",
    "    \n",
    "    return {\n",
    "        'height': chm_array,\n",
    "        'roughness': chm_roughness,\n",
    "        'gradient': chm_gradient\n",
    "    }\n",
    "\n",
    "\n",
    "def analyze_biotic_decoupling(chm_array, valid_mask, dem_array=None, resolution=1.0):\n",
    "    \"\"\"\n",
    "    Analyze correlation between CHM structure and topography.\n",
    "    \n",
    "    If DEM is available: correlate CHM roughness with slope, aspect, elevation, TPI\n",
    "    If DEM not available: correlate CHM roughness with position (weaker test)\n",
    "    \n",
    "    Low correlation = biotic decoupling (self-organized canopy)\n",
    "    High correlation = environmental control\n",
    "    \"\"\"\n",
    "    rows, cols = chm_array.shape\n",
    "    \n",
    "    # Compute CHM structure metrics\n",
    "    chm_metrics = compute_chm_structure_metrics(chm_array, valid_mask, resolution)\n",
    "    \n",
    "    # Compute topographic variables if DEM available\n",
    "    if dem_array is not None:\n",
    "        topo_vars = compute_topographic_variables(dem_array, valid_mask, resolution)\n",
    "        has_dem = True\n",
    "    else:\n",
    "        # Fall back to position-based analysis\n",
    "        y_coords, x_coords = np.meshgrid(np.arange(rows), np.arange(cols), indexing='ij')\n",
    "        topo_vars = {\n",
    "            'x_position': x_coords / cols,\n",
    "            'y_position': y_coords / rows,\n",
    "            'height': chm_array  # Use CHM height as proxy\n",
    "        }\n",
    "        has_dem = False\n",
    "    \n",
    "    # Sample valid pixels for correlation\n",
    "    roughness = chm_metrics['roughness']\n",
    "    valid_indices = np.where(valid_mask & ~np.isnan(roughness))\n",
    "    n_valid = len(valid_indices[0])\n",
    "    \n",
    "    if n_valid < 100:\n",
    "        return {'error': 'Insufficient valid pixels for correlation analysis'}\n",
    "    \n",
    "    # Subsample if too many pixels\n",
    "    max_samples = 50000\n",
    "    if n_valid > max_samples:\n",
    "        idx = np.random.choice(n_valid, max_samples, replace=False)\n",
    "        sample_rows = valid_indices[0][idx]\n",
    "        sample_cols = valid_indices[1][idx]\n",
    "    else:\n",
    "        sample_rows = valid_indices[0]\n",
    "        sample_cols = valid_indices[1]\n",
    "    \n",
    "    # Extract samples\n",
    "    roughness_samples = roughness[sample_rows, sample_cols]\n",
    "    \n",
    "    correlations = {}\n",
    "    \n",
    "    if has_dem:\n",
    "        # Correlate with actual topographic variables\n",
    "        for var_name in ['elevation', 'slope', 'tpi', 'roughness']:\n",
    "            var_data = topo_vars[var_name][sample_rows, sample_cols]\n",
    "            valid_both = ~(np.isnan(roughness_samples) | np.isnan(var_data))\n",
    "            if np.sum(valid_both) > 50:\n",
    "                r, p = pearsonr(roughness_samples[valid_both], var_data[valid_both])\n",
    "                rho, _ = spearmanr(roughness_samples[valid_both], var_data[valid_both])\n",
    "                correlations[f'chm_roughness_vs_{var_name}'] = {'r': r, 'p': p, 'rho': rho}\n",
    "    else:\n",
    "        # Correlate with position (weaker test)\n",
    "        for var_name in ['x_position', 'y_position', 'height']:\n",
    "            var_data = topo_vars[var_name][sample_rows, sample_cols]\n",
    "            valid_both = ~(np.isnan(roughness_samples) | np.isnan(var_data))\n",
    "            if np.sum(valid_both) > 50:\n",
    "                r, p = pearsonr(roughness_samples[valid_both], var_data[valid_both])\n",
    "                correlations[f'chm_roughness_vs_{var_name}'] = {'r': r, 'p': p}\n",
    "    \n",
    "    # Mean absolute correlation\n",
    "    r_values = [abs(v['r']) for v in correlations.values() if not np.isnan(v['r'])]\n",
    "    mean_abs_r = np.mean(r_values) if r_values else np.nan\n",
    "    \n",
    "    return {\n",
    "        'n_samples': len(roughness_samples),\n",
    "        'dem_available': has_dem,\n",
    "        'correlations': correlations,\n",
    "        'mean_abs_correlation': mean_abs_r\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "run-h5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "HYPOTHESIS 5: BIOTIC DECOUPLING\n",
      "======================================================================\n",
      "\n",
      "Analyzing topographic correlations...\n",
      "\n",
      "Loading DEM from: /home/jovyan/data-store/data/output/3dep/chm/superior_old_growth_dem.tif\n",
      "DEM shape: (623, 785)\n",
      "DEM elevation range: 446.8 - 494.6 m\n",
      "Using actual DEM for topographic analysis (slope, TPI, elevation)\n",
      "\n",
      "Samples analyzed: 50,000\n",
      "DEM used: True\n",
      "\n",
      "Correlations (CHM Roughness vs. Environmental Variables):\n",
      "  chm_roughness_vs_elevation: r = +0.0196 (p = 1.18e-05) ***\n",
      "  chm_roughness_vs_slope: r = +0.0546 (p = 2.09e-29) ***\n",
      "  chm_roughness_vs_roughness: r = +0.1052 (p = 4.94e-123) ***\n",
      "\n",
      "Mean |correlation|: 0.0598\n",
      "\n",
      "Mean |r| = 0.06 -> BIOTIC DECOUPLING (self-organized OLD GROWTH)\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"HYPOTHESIS 5: BIOTIC DECOUPLING\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nAnalyzing topographic correlations...\\n\")\n",
    "\n",
    "# Load DEM if available\n",
    "dem_array = None\n",
    "dem_valid_mask = None\n",
    "\n",
    "if DEM_PATH is not None and Path(DEM_PATH).exists():\n",
    "    print(f\"Loading DEM from: {DEM_PATH}\")\n",
    "    dem = rio.open_rasterio(DEM_PATH, masked=True)\n",
    "    \n",
    "    # Clip to same extent as CHM if AOI was selected\n",
    "    if selected_aoi['bounds_native'] is not None:\n",
    "        west, south, east, north = selected_aoi['bounds_native']\n",
    "        dem_subset = dem.rio.clip_box(minx=west, miny=south, maxx=east, maxy=north)\n",
    "    else:\n",
    "        dem_subset = dem\n",
    "    \n",
    "    dem_array = dem_subset.values.squeeze().astype(np.float64)\n",
    "    \n",
    "    # Align DEM to CHM if needed\n",
    "    if dem_array.shape != chm_array.shape:\n",
    "        print(f\"Resampling DEM from {dem_array.shape} to match CHM {chm_array.shape}\")\n",
    "        dem_subset = dem_subset.rio.reproject_match(chm_subset)\n",
    "        dem_array = dem_subset.values.squeeze().astype(np.float64)\n",
    "    \n",
    "    # Create valid mask for DEM\n",
    "    dem_nodata = dem_subset.rio.nodata if dem_subset.rio.nodata is not None else NODATA_VALUE\n",
    "    dem_valid_mask = ~np.isnan(dem_array) & (dem_array != dem_nodata)\n",
    "    \n",
    "    # Use combined valid mask\n",
    "    combined_mask = valid_mask & dem_valid_mask\n",
    "    \n",
    "    print(f\"DEM shape: {dem_array.shape}\")\n",
    "    print(f\"DEM elevation range: {dem_array[dem_valid_mask].min():.1f} - {dem_array[dem_valid_mask].max():.1f} m\")\n",
    "    print(\"Using actual DEM for topographic analysis (slope, TPI, elevation)\")\n",
    "else:\n",
    "    print(\"No DEM available - using CHM-derived position metrics\")\n",
    "    print(\"(For full H5 analysis, run chm_user_aoi.ipynb first to generate DEM)\")\n",
    "    combined_mask = valid_mask\n",
    "\n",
    "# Run biotic decoupling analysis\n",
    "topo_results = analyze_biotic_decoupling(\n",
    "    chm_array, combined_mask, \n",
    "    dem_array=dem_array, \n",
    "    resolution=pixel_resolution\n",
    ")\n",
    "\n",
    "if 'error' in topo_results:\n",
    "    print(f\"\\nError: {topo_results['error']}\")\n",
    "    h5_classification = \"INSUFFICIENT_DATA\"\n",
    "    h5_interpretation = topo_results['error']\n",
    "else:\n",
    "    print(f\"\\nSamples analyzed: {topo_results['n_samples']:,}\")\n",
    "    print(f\"DEM used: {topo_results['dem_available']}\")\n",
    "    print(f\"\\nCorrelations (CHM Roughness vs. Environmental Variables):\")\n",
    "    \n",
    "    for var, stats in topo_results['correlations'].items():\n",
    "        r = stats['r']\n",
    "        p = stats['p']\n",
    "        sig = \"***\" if p < 0.001 else (\"**\" if p < 0.01 else (\"*\" if p < 0.05 else \"\"))\n",
    "        print(f\"  {var}: r = {r:+.4f} (p = {p:.2e}) {sig}\")\n",
    "    \n",
    "    mean_r = topo_results['mean_abs_correlation']\n",
    "    print(f\"\\nMean |correlation|: {mean_r:.4f}\")\n",
    "    \n",
    "    # Classification based on mean absolute correlation\n",
    "    if mean_r < 0.3:\n",
    "        h5_classification = \"DECOUPLED\"\n",
    "        h5_interpretation = f\"Mean |r| = {mean_r:.2f} -> BIOTIC DECOUPLING (self-organized OLD GROWTH)\"\n",
    "    elif mean_r < 0.6:\n",
    "        h5_classification = \"PARTIAL_COUPLING\"\n",
    "        h5_interpretation = f\"Mean |r| = {mean_r:.2f} -> Partial environmental coupling (RECOVERY stage)\"\n",
    "    else:\n",
    "        h5_classification = \"COUPLED\"\n",
    "        h5_interpretation = f\"Mean |r| = {mean_r:.2f} -> Strong topographic control (YOUNG/DISTURBED)\"\n",
    "    \n",
    "    print(f\"\\n{h5_interpretation}\")\n",
    "\n",
    "h5_results = {\n",
    "    'hypothesis': 'H5: Biotic Decoupling',\n",
    "    'method': 'CHM-Topography Correlation Analysis',\n",
    "    'dem_available': topo_results.get('dem_available', False),\n",
    "    'n_samples': topo_results.get('n_samples', 0),\n",
    "    'correlations': {k: {kk: float(vv) if isinstance(vv, (int, float, np.floating)) else vv \n",
    "                         for kk, vv in v.items()} \n",
    "                    for k, v in topo_results.get('correlations', {}).items()},\n",
    "    'mean_abs_correlation': float(topo_results.get('mean_abs_correlation', np.nan)),\n",
    "    'classification': h5_classification,\n",
    "    'interpretation': h5_interpretation,\n",
    "    'supports_hypothesis': h5_classification == 'DECOUPLED'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spatial-hypotheses-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# HYPOTHESES 6-9: Spatial Distribution (Fractal String Theory)\n",
    "\n",
    "These four hypotheses derive from fractal string theory and predict specific patterns in large-tree spatial distributions:\n",
    "\n",
    "6. **Fractal String Gap** - Gap sizes follow fractal string spectrum predictions\n",
    "7. **Prime Number Repulsion (GUE)** - Large tree spacing shows prime-like repulsion\n",
    "8. **Complex Dimension Oscillation** - Log-periodic oscillations in canopy structure\n",
    "9. **Riemann Gas Density** - Tree density follows Riemann gas statistical mechanics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "spatial-functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_log_periodic_oscillations(lacunarity_values, box_sizes):\n",
    "    \"\"\"\n",
    "    Test for log-periodic oscillations in lacunarity curve.\n",
    "    \n",
    "    Complex dimensions in fractal string theory predict oscillations\n",
    "    in log-log space with period related to the complex part.\n",
    "    \"\"\"\n",
    "    if len(lacunarity_values) < 5:\n",
    "        return {'error': 'Insufficient scales for oscillation analysis'}\n",
    "    \n",
    "    log_r = np.log(box_sizes)\n",
    "    log_L = np.log(lacunarity_values)\n",
    "    \n",
    "    # Fit linear trend\n",
    "    coeffs = np.polyfit(log_r, log_L, 1)\n",
    "    trend = np.polyval(coeffs, log_r)\n",
    "    \n",
    "    # Residuals (detrended)\n",
    "    residuals = log_L - trend\n",
    "    \n",
    "    # Check for periodic pattern using autocorrelation\n",
    "    n = len(residuals)\n",
    "    if n >= 5:\n",
    "        autocorr = np.correlate(residuals, residuals, mode='full')\n",
    "        autocorr = autocorr[n-1:] / autocorr[n-1]  # Normalize\n",
    "        \n",
    "        # Find first peak after lag 0\n",
    "        peaks = []\n",
    "        for i in range(1, len(autocorr) - 1):\n",
    "            if autocorr[i] > autocorr[i-1] and autocorr[i] > autocorr[i+1]:\n",
    "                peaks.append((i, autocorr[i]))\n",
    "        \n",
    "        has_oscillation = len(peaks) > 0 and any(p[1] > 0.3 for p in peaks)\n",
    "    else:\n",
    "        has_oscillation = False\n",
    "        autocorr = np.array([])\n",
    "    \n",
    "    # RMS of residuals\n",
    "    rms_residual = np.sqrt(np.mean(residuals**2))\n",
    "    \n",
    "    return {\n",
    "        'residuals': residuals,\n",
    "        'rms_residual': rms_residual,\n",
    "        'has_oscillation': has_oscillation,\n",
    "        'trend_slope': coeffs[0],\n",
    "        'autocorrelation': autocorr[:min(10, len(autocorr))] if len(autocorr) > 0 else []\n",
    "    }\n",
    "\n",
    "\n",
    "def analyze_riemann_gas_density(tree_coords, valid_mask, resolution=1.0, n_bins=20):\n",
    "    \"\"\"\n",
    "    Test if tree density distribution follows Riemann gas statistics.\n",
    "    \n",
    "    In Riemann gas model, the density of 'particles' (trees) should\n",
    "    follow specific distributions related to zeta function zeros.\n",
    "    \"\"\"\n",
    "    if len(tree_coords) < 20:\n",
    "        return {'error': 'Insufficient trees for density analysis'}\n",
    "    \n",
    "    rows, cols = valid_mask.shape\n",
    "    \n",
    "    # Compute local tree density in grid cells\n",
    "    cell_size = max(rows, cols) // n_bins\n",
    "    \n",
    "    densities = []\n",
    "    for i in range(n_bins):\n",
    "        for j in range(n_bins):\n",
    "            y_start, y_end = i * cell_size, (i + 1) * cell_size\n",
    "            x_start, x_end = j * cell_size, (j + 1) * cell_size\n",
    "            \n",
    "            # Count trees in cell\n",
    "            in_cell = ((tree_coords[:, 0] >= y_start) & (tree_coords[:, 0] < y_end) &\n",
    "                       (tree_coords[:, 1] >= x_start) & (tree_coords[:, 1] < x_end))\n",
    "            n_trees = np.sum(in_cell)\n",
    "            \n",
    "            # Count valid pixels in cell\n",
    "            cell_valid = valid_mask[y_start:y_end, x_start:x_end]\n",
    "            valid_area = cell_valid.sum() * resolution**2\n",
    "            \n",
    "            if valid_area > 0:\n",
    "                density = n_trees / valid_area * 10000  # Trees per hectare\n",
    "                densities.append(density)\n",
    "    \n",
    "    densities = np.array(densities)\n",
    "    densities = densities[densities > 0]  # Remove empty cells\n",
    "    \n",
    "    if len(densities) < 10:\n",
    "        return {'error': 'Insufficient non-empty cells'}\n",
    "    \n",
    "    # Normalize densities\n",
    "    mean_density = np.mean(densities)\n",
    "    normalized_densities = densities / mean_density\n",
    "    \n",
    "    # Compute variance-to-mean ratio (Fano factor)\n",
    "    # Poisson: F = 1, Repulsion: F < 1, Clustering: F > 1\n",
    "    fano_factor = np.var(densities) / np.mean(densities) if np.mean(densities) > 0 else np.nan\n",
    "    \n",
    "    # Test against Poisson expectation\n",
    "    expected_var = mean_density  # For Poisson, var = mean\n",
    "    observed_var = np.var(densities)\n",
    "    \n",
    "    return {\n",
    "        'n_cells': len(densities),\n",
    "        'mean_density_per_ha': mean_density,\n",
    "        'density_variance': observed_var,\n",
    "        'fano_factor': fano_factor,\n",
    "        'normalized_densities': normalized_densities,\n",
    "        'interpretation': 'repulsion' if fano_factor < 0.8 else ('clustering' if fano_factor > 1.2 else 'random')\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "run-h6-h9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "HYPOTHESES 6-9: SPATIAL DISTRIBUTION (FRACTAL STRING THEORY)\n",
      "======================================================================\n",
      "\n",
      "--- H6: Fractal String Gap ---\n",
      "Testing if gap size spectrum follows fractal string predictions...\n",
      "Power law exponent \u03b1 = 1.84 consistent with fractal string predictions\n",
      "\n",
      "--- H7: Prime Number Repulsion (GUE) ---\n",
      "Testing if tree spacing shows prime-like repulsion patterns...\n",
      "Tree spacing does not show prime-like repulsion (random or clustered)\n",
      "\n",
      "--- H8: Complex Dimension Oscillation ---\n",
      "Testing for log-periodic oscillations in lacunarity curve...\n",
      "No significant log-periodic oscillations (RMS = 0.0094)\n",
      "\n",
      "--- H9: Riemann Gas Density ---\n",
      "Testing if tree density follows Riemann gas statistics...\n",
      "Fano factor = 65.926 > 1 indicates clustering (not Riemann gas)\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"HYPOTHESES 6-9: SPATIAL DISTRIBUTION (FRACTAL STRING THEORY)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# H6: Fractal String Gap\n",
    "print(\"\\n--- H6: Fractal String Gap ---\")\n",
    "print(\"Testing if gap size spectrum follows fractal string predictions...\")\n",
    "\n",
    "if 'power_law' in gap_results and gap_results['power_law']['r2'] > 0.8:\n",
    "    alpha = gap_results['power_law']['alpha']\n",
    "    # Fractal string theory predicts specific exponents related to complex dimensions\n",
    "    h6_classification = \"FRACTAL_STRING\" if 1.5 <= alpha <= 2.5 else \"NON_FRACTAL\"\n",
    "    h6_interpretation = f\"Power law exponent \u03b1 = {alpha:.2f} {'consistent with' if h6_classification == 'FRACTAL_STRING' else 'outside'} fractal string predictions\"\n",
    "else:\n",
    "    h6_classification = \"NO_POWER_LAW\"\n",
    "    h6_interpretation = \"Gap distribution does not follow power law - fractal string model not applicable\"\n",
    "\n",
    "print(h6_interpretation)\n",
    "\n",
    "h6_results = {\n",
    "    'hypothesis': 'H6: Fractal String Gap',\n",
    "    'classification': h6_classification,\n",
    "    'interpretation': h6_interpretation,\n",
    "    'supports_hypothesis': h6_classification == 'FRACTAL_STRING'\n",
    "}\n",
    "\n",
    "# H7: Prime Number Repulsion (GUE)\n",
    "print(\"\\n--- H7: Prime Number Repulsion (GUE) ---\")\n",
    "print(\"Testing if tree spacing shows prime-like repulsion patterns...\")\n",
    "\n",
    "if h4_classification == 'WIGNER_DYSON':\n",
    "    h7_classification = \"GUE_REPULSION\"\n",
    "    h7_interpretation = \"Tree spacing consistent with GUE statistics (prime-like repulsion)\"\n",
    "elif h4_classification == 'REPULSION_PARTIAL':\n",
    "    h7_classification = \"PARTIAL_REPULSION\"\n",
    "    h7_interpretation = \"Partial evidence for prime-like repulsion in tree spacing\"\n",
    "else:\n",
    "    h7_classification = \"NO_REPULSION\"\n",
    "    h7_interpretation = \"Tree spacing does not show prime-like repulsion (random or clustered)\"\n",
    "\n",
    "print(h7_interpretation)\n",
    "\n",
    "h7_results = {\n",
    "    'hypothesis': 'H7: Prime Number Repulsion (GUE)',\n",
    "    'classification': h7_classification,\n",
    "    'interpretation': h7_interpretation,\n",
    "    'supports_hypothesis': h7_classification == 'GUE_REPULSION'\n",
    "}\n",
    "\n",
    "# H8: Complex Dimension Oscillation\n",
    "print(\"\\n--- H8: Complex Dimension Oscillation ---\")\n",
    "print(\"Testing for log-periodic oscillations in lacunarity curve...\")\n",
    "\n",
    "if len(lacunarity) >= 5:\n",
    "    oscillation_results = analyze_log_periodic_oscillations(lacunarity, lac_sizes)\n",
    "    \n",
    "    if 'error' not in oscillation_results:\n",
    "        rms = oscillation_results['rms_residual']\n",
    "        has_osc = oscillation_results['has_oscillation']\n",
    "        \n",
    "        if has_osc:\n",
    "            h8_classification = \"OSCILLATION_PRESENT\"\n",
    "            h8_interpretation = f\"Log-periodic oscillations detected (RMS = {rms:.4f}) - complex dimension signature\"\n",
    "        else:\n",
    "            h8_classification = \"NO_OSCILLATION\"\n",
    "            h8_interpretation = f\"No significant log-periodic oscillations (RMS = {rms:.4f})\"\n",
    "    else:\n",
    "        h8_classification = \"INSUFFICIENT_DATA\"\n",
    "        h8_interpretation = oscillation_results['error']\n",
    "else:\n",
    "    oscillation_results = {'error': 'Insufficient lacunarity data'}\n",
    "    h8_classification = \"INSUFFICIENT_DATA\"\n",
    "    h8_interpretation = \"Insufficient lacunarity data for oscillation analysis\"\n",
    "\n",
    "print(h8_interpretation)\n",
    "\n",
    "h8_results = {\n",
    "    'hypothesis': 'H8: Complex Dimension Oscillation',\n",
    "    'classification': h8_classification,\n",
    "    'interpretation': h8_interpretation,\n",
    "    'rms_residual': float(oscillation_results.get('rms_residual', np.nan)),\n",
    "    'supports_hypothesis': h8_classification == 'OSCILLATION_PRESENT'\n",
    "}\n",
    "\n",
    "# H9: Riemann Gas Density\n",
    "print(\"\\n--- H9: Riemann Gas Density ---\")\n",
    "print(\"Testing if tree density follows Riemann gas statistics...\")\n",
    "\n",
    "if len(tree_coords) >= 20:\n",
    "    riemann_results = analyze_riemann_gas_density(tree_coords, valid_mask, pixel_resolution)\n",
    "    \n",
    "    if 'error' not in riemann_results:\n",
    "        fano = riemann_results['fano_factor']\n",
    "        interp = riemann_results['interpretation']\n",
    "        \n",
    "        if interp == 'repulsion':\n",
    "            h9_classification = \"RIEMANN_GAS\"\n",
    "            h9_interpretation = f\"Fano factor = {fano:.3f} < 1 indicates repulsion consistent with Riemann gas\"\n",
    "        elif interp == 'clustering':\n",
    "            h9_classification = \"CLUSTERING\"\n",
    "            h9_interpretation = f\"Fano factor = {fano:.3f} > 1 indicates clustering (not Riemann gas)\"\n",
    "        else:\n",
    "            h9_classification = \"POISSON\"\n",
    "            h9_interpretation = f\"Fano factor = {fano:.3f} \u2248 1 indicates random/Poisson distribution\"\n",
    "    else:\n",
    "        h9_classification = \"INSUFFICIENT_DATA\"\n",
    "        h9_interpretation = riemann_results['error']\n",
    "        fano = np.nan\n",
    "else:\n",
    "    riemann_results = {'error': 'Insufficient trees'}\n",
    "    h9_classification = \"INSUFFICIENT_DATA\"\n",
    "    h9_interpretation = f\"Only {len(tree_coords)} trees detected - need at least 20\"\n",
    "    fano = np.nan\n",
    "\n",
    "print(h9_interpretation)\n",
    "\n",
    "h9_results = {\n",
    "    'hypothesis': 'H9: Riemann Gas Density',\n",
    "    'classification': h9_classification,\n",
    "    'interpretation': h9_interpretation,\n",
    "    'fano_factor': float(fano) if not np.isnan(fano) else None,\n",
    "    'supports_hypothesis': h9_classification == 'RIEMANN_GAS'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# COMPREHENSIVE HYPOTHESIS SUMMARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "summary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "                     COMPREHENSIVE HYPOTHESIS TESTING SUMMARY\n",
      "================================================================================\n",
      "\n",
      "CHM: superior_old_growth_chm.tif\n",
      "Analysis Date: 2025-12-22 19:42:02\n",
      "AOI: User-selected\n",
      "Valid pixels: 450,544\n",
      "Pixel resolution: 0.500m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "PRIMARY HYPOTHESES (1-5)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "H1: Optimal Filling\n",
      "    Classification: DISTURBED\n",
      "    D = 2.48 suggests DISTURBED forest (moderate complexity)\n",
      "    Status: NOT SUPPORTED\n",
      "\n",
      "H2: Scale Invariance\n",
      "    Classification: SCALE_INVARIANT\n",
      "    R\u00b2 = 0.99 -> Strong scale invariance (OLD GROWTH signature)\n",
      "    Status: SUPPORTED\n",
      "\n",
      "H3: Zeta Distribution\n",
      "    Classification: ZETA_DISTRIBUTED\n",
      "    Power law \u03b1 \u2248 1.84 close to 2.0 -> OLD GROWTH signature\n",
      "    Status: SUPPORTED\n",
      "\n",
      "H4: Universal Repulsion\n",
      "    Classification: NEITHER\n",
      "    Neither distribution fits well -> Complex spatial structure\n",
      "    Status: NOT SUPPORTED\n",
      "\n",
      "H5: Biotic Decoupling\n",
      "    Classification: DECOUPLED\n",
      "    Mean |r| = 0.06 -> BIOTIC DECOUPLING (self-organized OLD GROWTH)\n",
      "    Status: SUPPORTED\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "SPATIAL DISTRIBUTION HYPOTHESES (6-9)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "H6: Fractal String Gap\n",
      "    Classification: FRACTAL_STRING\n",
      "    Power law exponent \u03b1 = 1.84 consistent with fractal string predictions\n",
      "    Status: SUPPORTED\n",
      "\n",
      "H7: Prime Number Repulsion (GUE)\n",
      "    Classification: NO_REPULSION\n",
      "    Tree spacing does not show prime-like repulsion (random or clustered)\n",
      "    Status: NOT SUPPORTED\n",
      "\n",
      "H8: Complex Dimension Oscillation\n",
      "    Classification: NO_OSCILLATION\n",
      "    No significant log-periodic oscillations (RMS = 0.0094)\n",
      "    Status: NOT SUPPORTED\n",
      "\n",
      "H9: Riemann Gas Density\n",
      "    Classification: CLUSTERING\n",
      "    Fano factor = 65.926 > 1 indicates clustering (not Riemann gas)\n",
      "    Status: NOT SUPPORTED\n",
      "\n",
      "================================================================================\n",
      "OVERALL ASSESSMENT\n",
      "================================================================================\n",
      "\n",
      "Primary hypotheses supported: 3/5\n",
      "All hypotheses supported: 4/9\n",
      "\n",
      "FOREST CLASSIFICATION: RECOVERING / TRANSITIONAL FOREST\n",
      "\n",
      "Mixed evidence suggesting forest in recovery or transition toward old-growth characteristics.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "KEY METRICS\n",
      "--------------------------------------------------------------------------------\n",
      "  DBC Fractal Dimension (D):     2.4805\n",
      "  Lacunarity R\u00b2 (scale inv.):    0.9899 [SCALE INVARIANT]\n",
      "  Gap power-law exponent (\u03b1):    1.8404 [NEAR \u03b6(2)]\n",
      "  Tree spacing (Wigner-Dyson):   NEITHER\n",
      "  Topographic correlation:       0.0598 [DECOUPLED]\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"                     COMPREHENSIVE HYPOTHESIS TESTING SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nCHM: {CHM_PATH.name}\")\n",
    "print(f\"Analysis Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"AOI: {'User-selected' if selected_aoi['bounds_native'] else 'Full extent'}\")\n",
    "print(f\"Valid pixels: {valid_mask.sum():,}\")\n",
    "print(f\"Pixel resolution: {pixel_resolution:.3f}m\")\n",
    "\n",
    "# Collect all hypothesis results\n",
    "all_results = [\n",
    "    h1_results, h2_results, h3_results, h4_results, h5_results,\n",
    "    h6_results, h7_results, h8_results, h9_results\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"PRIMARY HYPOTHESES (1-5)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for i, result in enumerate(all_results[:5], 1):\n",
    "    status = \"SUPPORTED\" if result['supports_hypothesis'] else \"NOT SUPPORTED\"\n",
    "    print(f\"\\nH{i}: {result['hypothesis'].split(': ')[1]}\")\n",
    "    print(f\"    Classification: {result['classification']}\")\n",
    "    print(f\"    {result['interpretation']}\")\n",
    "    print(f\"    Status: {status}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"SPATIAL DISTRIBUTION HYPOTHESES (6-9)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for i, result in enumerate(all_results[5:], 6):\n",
    "    status = \"SUPPORTED\" if result['supports_hypothesis'] else \"NOT SUPPORTED\"\n",
    "    print(f\"\\nH{i}: {result['hypothesis'].split(': ')[1]}\")\n",
    "    print(f\"    Classification: {result['classification']}\")\n",
    "    print(f\"    {result['interpretation']}\")\n",
    "    print(f\"    Status: {status}\")\n",
    "\n",
    "# Overall assessment\n",
    "supported_count = sum(1 for r in all_results if r['supports_hypothesis'])\n",
    "primary_supported = sum(1 for r in all_results[:5] if r['supports_hypothesis'])\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"OVERALL ASSESSMENT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nPrimary hypotheses supported: {primary_supported}/5\")\n",
    "print(f\"All hypotheses supported: {supported_count}/9\")\n",
    "\n",
    "# Forest classification based on hypothesis support\n",
    "if primary_supported >= 4:\n",
    "    forest_class = \"OLD-GROWTH / MATURE FOREST\"\n",
    "    description = \"Strong evidence for self-organized, mature forest structure with optimal packing and scale invariance.\"\n",
    "elif primary_supported >= 2:\n",
    "    forest_class = \"RECOVERING / TRANSITIONAL FOREST\"\n",
    "    description = \"Mixed evidence suggesting forest in recovery or transition toward old-growth characteristics.\"\n",
    "else:\n",
    "    forest_class = \"YOUNG / MANAGED / DISTURBED FOREST\"\n",
    "    description = \"Limited support for old-growth hypotheses; structure consistent with young, managed, or recently disturbed forest.\"\n",
    "\n",
    "print(f\"\\nFOREST CLASSIFICATION: {forest_class}\")\n",
    "print(f\"\\n{description}\")\n",
    "\n",
    "# Key metrics summary\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"KEY METRICS\")\n",
    "print(\"-\"*80)\n",
    "print(f\"  DBC Fractal Dimension (D):     {D_dbc:.4f}\" + (\" [OLD-GROWTH]\" if D_dbc >= 2.5 else \"\"))\n",
    "print(f\"  Lacunarity R\u00b2 (scale inv.):    {r2_lac:.4f}\" + (\" [SCALE INVARIANT]\" if r2_lac > 0.95 else \"\"))\n",
    "print(f\"  Gap power-law exponent (\u03b1):    {gap_results.get('power_law', {}).get('alpha', np.nan):.4f}\" + \n",
    "      (\" [NEAR \u03b6(2)]\" if 1.8 <= gap_results.get('power_law', {}).get('alpha', 0) <= 2.2 else \"\"))\n",
    "print(f\"  Tree spacing (Wigner-Dyson):   {h4_classification}\")\n",
    "print(f\"  Topographic correlation:       {topo_results.get('mean_abs_correlation', np.nan):.4f}\" +\n",
    "      (\" [DECOUPLED]\" if topo_results.get('mean_abs_correlation', 1) < 0.3 else \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viz-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# VISUALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualization",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure saved to: /home/jovyan/data-store/data/output/fractal_analysis/comprehensive_hypothesis_analysis.png\n"
     ]
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(20, 16))\n",
    "\n",
    "# 1. CHM with tree detections\n",
    "ax1 = fig.add_subplot(3, 3, 1)\n",
    "chm_display = np.ma.masked_where(~valid_mask, chm_array)\n",
    "im1 = ax1.imshow(chm_display, cmap='Greens', vmin=0)\n",
    "if len(tree_coords) > 0:\n",
    "    ax1.scatter(tree_coords[:, 1], tree_coords[:, 0], c='red', s=5, alpha=0.5, label=f'{len(tree_coords)} trees')\n",
    "ax1.set_title(f'CHM with Detected Trees (>= {DOMINANT_TREE_THRESHOLD}m)')\n",
    "ax1.legend(loc='upper right')\n",
    "plt.colorbar(im1, ax=ax1, label='Height (m)', shrink=0.8)\n",
    "\n",
    "# 2. DBC log-log plot (H1)\n",
    "ax2 = fig.add_subplot(3, 3, 2)\n",
    "if len(scales_dbc) > 0:\n",
    "    ax2.plot(np.log(1/scales_dbc), np.log(Ns_dbc), 'bo-', markersize=8, label='DBC Data')\n",
    "    coeffs = np.polyfit(np.log(1/scales_dbc), np.log(Ns_dbc), 1)\n",
    "    x_fit = np.log(1/scales_dbc)\n",
    "    ax2.plot(x_fit, np.polyval(coeffs, x_fit), 'r--', linewidth=2, label=f'D = {D_dbc:.3f}')\n",
    "ax2.set_xlabel('log(1/s)')\n",
    "ax2.set_ylabel('log(N(s))')\n",
    "ax2.set_title(f'H1: Optimal Filling (DBC)\\nD = {D_dbc:.3f}, R\u00b2 = {r2_dbc:.3f}')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Lacunarity plot (H2)\n",
    "ax3 = fig.add_subplot(3, 3, 3)\n",
    "if len(lac_sizes) > 0:\n",
    "    ax3.loglog(lac_sizes, lacunarity, 'go-', markersize=8, label='Lacunarity')\n",
    "    ax3.set_xlabel('Box size r (pixels)')\n",
    "    ax3.set_ylabel('Lacunarity \u039b(r)')\n",
    "ax3.set_title(f'H2: Scale Invariance\\nR\u00b2 = {r2_lac:.3f}')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Gap size distribution (H3)\n",
    "ax4 = fig.add_subplot(3, 3, 4)\n",
    "if 'sorted_areas' in gap_results:\n",
    "    ax4.loglog(gap_results['sorted_areas'], gap_results['ccdf'], 'k.', alpha=0.3, markersize=2)\n",
    "    alpha = gap_results['power_law']['alpha']\n",
    "    x_range = np.logspace(np.log10(gap_results['min_area']), np.log10(gap_results['max_area']), 100)\n",
    "    y_pl = (x_range / gap_results['min_area']) ** (-(alpha - 1))\n",
    "    ax4.loglog(x_range, y_pl, 'r-', linewidth=2, label=f'\u03b1 = {alpha:.2f}')\n",
    "    ax4.axvline(gap_results['median_area'], color='blue', linestyle='--', alpha=0.5, label=f'Median = {gap_results[\"median_area\"]:.1f} m\u00b2')\n",
    "ax4.set_xlabel('Gap Area (m\u00b2)')\n",
    "ax4.set_ylabel('P(X > x)')\n",
    "ax4.set_title(f'H3: Zeta Distribution\\n\u03b1 = {gap_results.get(\"power_law\", {}).get(\"alpha\", np.nan):.3f}')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Nearest neighbor spacing (H4)\n",
    "ax5 = fig.add_subplot(3, 3, 5)\n",
    "if 'spacings' in spacing_results and len(spacing_results.get('spacings', [])) > 0:\n",
    "    spacings = spacing_results['spacings']\n",
    "    ax5.hist(spacings, bins=30, density=True, alpha=0.7, color='blue', label='Observed')\n",
    "    s_range = np.linspace(0.01, 3, 100)\n",
    "    ax5.plot(s_range, wigner_dyson_gue(s_range), 'r-', linewidth=2, label='Wigner-Dyson (GUE)')\n",
    "    ax5.plot(s_range, poisson_spacing(s_range), 'g--', linewidth=2, label='Poisson')\n",
    "    ax5.set_xlim(0, 3)\n",
    "ax5.set_xlabel('Normalized spacing s')\n",
    "ax5.set_ylabel('Probability density')\n",
    "ax5.set_title(f'H4: Universal Repulsion\\n{h4_classification}')\n",
    "ax5.legend()\n",
    "ax5.grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Topographic correlation (H5)\n",
    "ax6 = fig.add_subplot(3, 3, 6)\n",
    "if 'correlations' in topo_results and len(topo_results['correlations']) > 0:\n",
    "    # Build display from available correlations\n",
    "    corr_names = []\n",
    "    corr_values = []\n",
    "    name_map = {\n",
    "        'chm_roughness_vs_elevation': 'Elevation',\n",
    "        'chm_roughness_vs_slope': 'Slope',\n",
    "        'chm_roughness_vs_roughness': 'Roughness',\n",
    "        'chm_roughness_vs_height': 'Height',\n",
    "        'chm_roughness_vs_x_position': 'X-pos',\n",
    "        'chm_roughness_vs_y_position': 'Y-pos'\n",
    "    }\n",
    "    for key, label in name_map.items():\n",
    "        if key in topo_results['correlations']:\n",
    "            corr_names.append(label)\n",
    "            corr_values.append(abs(topo_results['correlations'][key]['r']))\n",
    "    if corr_values:\n",
    "        colors = ['green' if v < 0.3 else ('orange' if v < 0.6 else 'red') for v in corr_values]\n",
    "        bars = ax6.bar(corr_names, corr_values, color=colors)\n",
    "        ax6.axhline(0.3, color='green', linestyle='--', alpha=0.5, label='Decoupling threshold')\n",
    "        ax6.axhline(0.6, color='red', linestyle='--', alpha=0.5, label='Strong coupling')\n",
    "ax6.set_ylabel('|Correlation|')\n",
    "ax6.set_title(f'H5: Biotic Decoupling\\nMean |r| = {topo_results.get(\"mean_abs_correlation\", np.nan):.3f}')\n",
    "ax6.set_ylim(0, 1)\n",
    "ax6.legend()\n",
    "\n",
    "# 7. Gap labeled image\n",
    "ax7 = fig.add_subplot(3, 3, 7)\n",
    "gap_display = np.ma.masked_where(labeled_gaps == 0, labeled_gaps)\n",
    "ax7.imshow(chm_display, cmap='Greens', vmin=0, alpha=0.5)\n",
    "ax7.imshow(gap_display > 0, cmap='Reds', alpha=0.5)\n",
    "ax7.set_title(f'Canopy Gaps (< {GAP_THRESHOLD}m)\\n{gap_results.get(\"n_gaps\", 0)} gaps detected')\n",
    "\n",
    "# 8. Hypothesis support summary\n",
    "ax8 = fig.add_subplot(3, 3, 8)\n",
    "ax8.axis('off')\n",
    "\n",
    "summary_text = f\"\"\"\n",
    "HYPOTHESIS TESTING RESULTS\n",
    "{\"=\"*40}\n",
    "\n",
    "H1: Optimal Filling      [{\"SUPPORTED\" if h1_results['supports_hypothesis'] else \"NOT SUPPORTED\":^13}]\n",
    "    D = {D_dbc:.3f}\n",
    "\n",
    "H2: Scale Invariance     [{\"SUPPORTED\" if h2_results['supports_hypothesis'] else \"NOT SUPPORTED\":^13}]\n",
    "    R\u00b2 = {r2_lac:.3f}\n",
    "\n",
    "H3: Zeta Distribution    [{\"SUPPORTED\" if h3_results['supports_hypothesis'] else \"NOT SUPPORTED\":^13}]\n",
    "    \u03b1 = {gap_results.get('power_law', {}).get('alpha', np.nan):.3f}\n",
    "\n",
    "H4: Universal Repulsion  [{\"SUPPORTED\" if h4_results['supports_hypothesis'] else \"NOT SUPPORTED\":^13}]\n",
    "    {h4_classification}\n",
    "\n",
    "H5: Biotic Decoupling    [{\"SUPPORTED\" if h5_results['supports_hypothesis'] else \"NOT SUPPORTED\":^13}]\n",
    "    Mean |r| = {topo_results.get('mean_abs_correlation', np.nan):.3f}\n",
    "\n",
    "{\"=\"*40}\n",
    "Primary Hypotheses: {primary_supported}/5 supported\n",
    "Classification: {forest_class}\n",
    "\"\"\"\n",
    "ax8.text(0.05, 0.95, summary_text, transform=ax8.transAxes,\n",
    "         fontsize=9, verticalalignment='top', fontfamily='monospace',\n",
    "         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "\n",
    "# 9. Spatial hypotheses summary\n",
    "ax9 = fig.add_subplot(3, 3, 9)\n",
    "ax9.axis('off')\n",
    "\n",
    "spatial_text = f\"\"\"\n",
    "SPATIAL DISTRIBUTION HYPOTHESES\n",
    "{\"=\"*40}\n",
    "\n",
    "H6: Fractal String Gap   [{\"SUPPORTED\" if h6_results['supports_hypothesis'] else \"NOT SUPPORTED\":^13}]\n",
    "    {h6_results['classification']}\n",
    "\n",
    "H7: Prime Repulsion      [{\"SUPPORTED\" if h7_results['supports_hypothesis'] else \"NOT SUPPORTED\":^13}]\n",
    "    {h7_results['classification']}\n",
    "\n",
    "H8: Complex Oscillation  [{\"SUPPORTED\" if h8_results['supports_hypothesis'] else \"NOT SUPPORTED\":^13}]\n",
    "    {h8_results['classification']}\n",
    "\n",
    "H9: Riemann Gas          [{\"SUPPORTED\" if h9_results['supports_hypothesis'] else \"NOT SUPPORTED\":^13}]\n",
    "    {h9_results['classification']}\n",
    "\n",
    "{\"=\"*40}\n",
    "All Hypotheses: {supported_count}/9 supported\n",
    "\"\"\"\n",
    "ax9.text(0.05, 0.95, spatial_text, transform=ax9.transAxes,\n",
    "         fontsize=9, verticalalignment='top', fontfamily='monospace',\n",
    "         bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save figure\n",
    "fig_path = OUTPUT_DIR / 'comprehensive_hypothesis_analysis.png'\n",
    "plt.savefig(fig_path, dpi=150, bbox_inches='tight')\n",
    "print(f\"Figure saved to: {fig_path}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "export-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# EXPORT RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "export-results",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to: /home/jovyan/data-store/data/output/fractal_analysis/comprehensive_hypothesis_results.json\n",
      "\n",
      "==================================================\n",
      "ANALYSIS COMPLETE\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Compile all results\n",
    "comprehensive_results = {\n",
    "    'metadata': {\n",
    "        'chm_path': str(CHM_PATH),\n",
    "        'dem_path': str(DEM_PATH) if DEM_PATH else None,\n",
    "        'analysis_date': datetime.now().isoformat(),\n",
    "        'gap_threshold_m': GAP_THRESHOLD,\n",
    "        'tree_height_threshold_m': DOMINANT_TREE_THRESHOLD,\n",
    "        'pixel_resolution_m': pixel_resolution,\n",
    "        'aoi_selected': selected_aoi['bounds_native'] is not None,\n",
    "        'valid_pixels': int(valid_mask.sum()),\n",
    "        'array_shape': list(chm_array.shape)\n",
    "    },\n",
    "    'hypotheses': {\n",
    "        'H1_optimal_filling': h1_results,\n",
    "        'H2_scale_invariance': h2_results,\n",
    "        'H3_zeta_distribution': h3_results,\n",
    "        'H4_universal_repulsion': h4_results,\n",
    "        'H5_biotic_decoupling': h5_results,\n",
    "        'H6_fractal_string_gap': h6_results,\n",
    "        'H7_prime_repulsion': h7_results,\n",
    "        'H8_complex_oscillation': h8_results,\n",
    "        'H9_riemann_gas': h9_results\n",
    "    },\n",
    "    'summary': {\n",
    "        'primary_hypotheses_supported': primary_supported,\n",
    "        'all_hypotheses_supported': supported_count,\n",
    "        'forest_classification': forest_class,\n",
    "        'description': description\n",
    "    },\n",
    "    'key_metrics': {\n",
    "        'dbc_fractal_dimension': float(D_dbc) if not np.isnan(D_dbc) else None,\n",
    "        'dbc_r_squared': float(r2_dbc),\n",
    "        'lacunarity_r_squared': float(r2_lac),\n",
    "        'lacunarity_slope': float(lac_slope),\n",
    "        'gap_power_law_alpha': float(gap_results.get('power_law', {}).get('alpha', np.nan)),\n",
    "        'gap_power_law_r2': float(gap_results.get('power_law', {}).get('r2', 0)),\n",
    "        'n_gaps': gap_results.get('n_gaps', 0),\n",
    "        'n_dominant_trees': len(tree_coords),\n",
    "        'mean_tree_spacing_m': float(mean_spacing) if not np.isnan(mean_spacing) else None,\n",
    "        'topographic_mean_correlation': float(topo_results.get('mean_abs_correlation', np.nan))\n",
    "    }\n",
    "}\n",
    "\n",
    "# Convert numpy types to Python types for JSON serialization\n",
    "def convert_numpy(obj):\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, (np.int64, np.int32)):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, (np.float64, np.float32)):\n",
    "        return float(obj) if not np.isnan(obj) else None\n",
    "    elif isinstance(obj, dict):\n",
    "        return {k: convert_numpy(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_numpy(v) for v in obj]\n",
    "    return obj\n",
    "\n",
    "comprehensive_results = convert_numpy(comprehensive_results)\n",
    "\n",
    "# Save to JSON\n",
    "results_path = OUTPUT_DIR / 'comprehensive_hypothesis_results.json'\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(comprehensive_results, f, indent=2, default=str)\n",
    "\n",
    "print(f\"Results saved to: {results_path}\")\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ANALYSIS COMPLETE\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "references",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "### Fractal Dimension Methods\n",
    "- Sarkar, N., & Chaudhuri, B.B. (1994). An efficient differential box-counting approach to compute fractal dimension of image. *IEEE Transactions on Systems, Man, and Cybernetics*.\n",
    "- Allain, C., & Cloitre, M. (1991). Characterizing the lacunarity of random and deterministic fractal sets. *Physical Review A*.\n",
    "\n",
    "### Ecological Applications\n",
    "- West, G.B., Brown, J.H., & Enquist, B.J. (1997). A general model for the origin of allometric scaling laws in biology. *Science*.\n",
    "- Mandelbrot, B.B. (1983). *The Fractal Geometry of Nature*. W.H. Freeman.\n",
    "\n",
    "### Random Matrix Theory\n",
    "- Wigner, E. (1955). Characteristic vectors of bordered matrices with infinite dimensions. *Annals of Mathematics*.\n",
    "- Montgomery, H.L. (1973). The pair correlation of zeros of the zeta function. *Analytic Number Theory*.\n",
    "\n",
    "### Fractal String Theory\n",
    "- Lapidus, M.L., & van Frankenhuijsen, M. (2006). *Fractal Geometry, Complex Dimensions and Zeta Functions*. Springer.\n",
    "\n",
    "### Research Framework\n",
    "- See: https://tyson-swetnam.github.io/fractal-notebooks/hypotheses/abstract/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db2ec37-7412-42ca-bcf0-1ac39f07e3c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3DEP (Python 3.11)",
   "language": "python",
   "name": "3dep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}