{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Site Canopy Height Model (CHM) Batch Processing\n",
    "\n",
    "Generate Canopy Height Models from USGS 3DEP lidar data for multiple forest sites.\n",
    "\n",
    "**Features:**\n",
    "- Process multiple sites from `forest_sites.yaml` configuration\n",
    "- Adaptive resolution based on point density\n",
    "- Direct CHM calculation (no smoothing/IDW)\n",
    "- Automatic metadata and summary generation\n",
    "- Designed for CyVerse VICE environment\n",
    "\n",
    "## Requirements\n",
    "\n",
    "Create and activate the 3dep conda environment:\n",
    "\n",
    "```bash\n",
    "# Create the environment (first time only)\n",
    "mamba env create -f environments/3dep-environment.yml\n",
    "\n",
    "# Install the Jupyter kernel\n",
    "/opt/conda/envs/3dep/bin/python -m ipykernel install --user --name 3dep --display-name \"Python (3DEP)\"\n",
    "\n",
    "# Activate the environment\n",
    "conda activate 3dep\n",
    "```\n",
    "\n",
    "Then select the **\"Python (3DEP)\"** kernel in Jupyter before running this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment OK!\n"
     ]
    }
   ],
   "source": [
    "# Environment validation\n",
    "import sys\n",
    "\n",
    "REQUIRED = ['pdal', 'geopandas', 'rioxarray', 'pyproj', 'shapely', 'numpy', 'yaml']\n",
    "missing = []\n",
    "for pkg in REQUIRED:\n",
    "    try:\n",
    "        __import__(pkg if pkg != 'yaml' else 'yaml')\n",
    "    except ImportError:\n",
    "        missing.append(pkg)\n",
    "\n",
    "if missing:\n",
    "    raise ImportError(f\"Missing: {missing}. Run: conda activate 3dep\")\n",
    "\n",
    "print(\"Environment OK!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded at 2025-12-19 16:51:20\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import traceback\n",
    "\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pdal\n",
    "import pyproj\n",
    "import requests\n",
    "import rioxarray as rio\n",
    "import yaml\n",
    "from rasterio.enums import Resampling\n",
    "from shapely.geometry import box\n",
    "from shapely.ops import transform\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "print(f\"Loaded at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: /home/jovyan/data-store/data/output/3dep\n",
      "Config: /home/jovyan/data-store/fractal-notebooks/docs/notebooks/3dep/modified/../forest_sites.yaml\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "# Output directory for all processing outputs\n",
    "OUTPUT_BASE = Path(\"/home/jovyan/data-store/data/output/3dep\")\n",
    "SITES_CONFIG = Path(\"../forest_sites.yaml\")\n",
    "\n",
    "# Create directories\n",
    "for subdir in ['chm', 'dtm', 'dsm', 'logs']:\n",
    "    (OUTPUT_BASE / subdir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Output: {OUTPUT_BASE.absolute()}\")\n",
    "print(f\"Config: {SITES_CONFIG.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Site Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load forest sites configuration\n",
    "with open(SITES_CONFIG, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "sites = config['sites']\n",
    "processing_config = config['processing']\n",
    "groups = config['groups']\n",
    "\n",
    "print(f\"Loaded {len(sites)} forest sites\")\n",
    "print(f\"Available groups: {list(groups.keys())}\")\n",
    "print(f\"\\nProcessing settings:\")\n",
    "print(f\"  Density threshold: {processing_config['density_threshold']} pts/mÂ²\")\n",
    "print(f\"  High resolution: {processing_config['resolution_high']}m\")\n",
    "print(f\"  Standard resolution: {processing_config['resolution_standard']}m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display available sites\n",
    "site_list = []\n",
    "for site_id, site_info in sites.items():\n",
    "    site_list.append({\n",
    "        'id': site_id,\n",
    "        'name': site_info['name'],\n",
    "        'state': site_info['state'],\n",
    "        'forest_type': site_info['forest_type'],\n",
    "        'priority': site_info.get('priority', 3),\n",
    "        'expected_height': site_info.get('expected_max_height_m', 'N/A')\n",
    "    })\n",
    "\n",
    "df_sites = pd.DataFrame(site_list).sort_values(['priority', 'state'])\n",
    "print(\"\\nAvailable Forest Sites:\")\n",
    "print(df_sites.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Select Sites to Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELECT SITES TO PROCESS\n",
    "# Option 1: Process specific sites by ID\n",
    "# SELECTED_SITES = ['sequoia_giant_forest', 'redwood_humboldt', 'great_smoky_cove']\n",
    "\n",
    "# Option 2: Process a predefined group\n",
    "# SELECTED_SITES = groups['all_priority_1']['sites']\n",
    "\n",
    "# Option 3: Process all sites (be careful - this is a lot of data!)\n",
    "# SELECTED_SITES = list(sites.keys())\n",
    "\n",
    "# Default: Process priority 1 sites\n",
    "SELECTED_SITES = groups['all_priority_1']['sites']\n",
    "\n",
    "print(f\"Selected {len(SELECTED_SITES)} sites for processing:\")\n",
    "for site_id in SELECTED_SITES:\n",
    "    print(f\"  - {site_id}: {sites[site_id]['name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Core Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gcs_to_proj(poly):\n",
    "    \"\"\"Reproject from EPSG:4326 to EPSG:3857.\"\"\"\n",
    "    wgs84 = pyproj.CRS(\"EPSG:4326\")\n",
    "    web_mercator = pyproj.CRS(\"EPSG:3857\")\n",
    "    project = pyproj.Transformer.from_crs(wgs84, web_mercator, always_xy=True).transform\n",
    "    return transform(project, poly)\n",
    "\n",
    "\n",
    "def bbox_to_polygon(bbox):\n",
    "    \"\"\"Convert [west, south, east, north] to (poly_4326, poly_3857).\"\"\"\n",
    "    poly = box(*bbox)\n",
    "    return poly, gcs_to_proj(poly)\n",
    "\n",
    "\n",
    "def get_resolution(density):\n",
    "    \"\"\"Get resolution based on point density.\"\"\"\n",
    "    if density >= processing_config['density_threshold']:\n",
    "        return processing_config['resolution_high']\n",
    "    return processing_config['resolution_standard']\n",
    "\n",
    "\n",
    "def make_dem_pipeline(extent_wkt, dataset_names, pc_res, dem_res, dem_type, out_path, out_crs=3857):\n",
    "    \"\"\"Build PDAL pipeline for DEM generation (no IDW).\"\"\"\n",
    "    readers = []\n",
    "    for name in dataset_names:\n",
    "        readers.append({\n",
    "            \"type\": \"readers.ept\",\n",
    "            \"filename\": f\"https://s3-us-west-2.amazonaws.com/usgs-lidar-public/{name}/ept.json\",\n",
    "            \"polygon\": extent_wkt,\n",
    "            \"requests\": 3,\n",
    "            \"resolution\": pc_res\n",
    "        })\n",
    "    \n",
    "    pipeline = {\"pipeline\": readers}\n",
    "    \n",
    "    # Filter noise\n",
    "    pipeline['pipeline'].append({\"type\": \"filters.range\", \"limits\": \"Classification![7:7]\"})\n",
    "    pipeline['pipeline'].append({\"type\": \"filters.range\", \"limits\": \"Classification![18:18]\"})\n",
    "    \n",
    "    # Reproject\n",
    "    pipeline['pipeline'].append({\"type\": \"filters.reprojection\", \"out_srs\": f\"EPSG:{out_crs}\"})\n",
    "    \n",
    "    # Ground filter for DTM\n",
    "    if dem_type == 'dtm':\n",
    "        pipeline['pipeline'].append({\"type\": \"filters.range\", \"limits\": \"Classification[2:2]\"})\n",
    "        grid_method = \"min\"\n",
    "    else:\n",
    "        grid_method = \"max\"\n",
    "    \n",
    "    # Writer - NO IDW\n",
    "    pipeline['pipeline'].append({\n",
    "        \"type\": \"writers.gdal\",\n",
    "        \"filename\": str(out_path),\n",
    "        \"gdaldriver\": \"GTiff\",\n",
    "        \"nodata\": -9999,\n",
    "        \"output_type\": grid_method,\n",
    "        \"resolution\": float(dem_res),\n",
    "        \"gdalopts\": \"COMPRESS=LZW,TILED=YES,BLOCKXSIZE=256,BLOCKYSIZE=256\"\n",
    "    })\n",
    "    \n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_site(site_id, site_info, df_3dep, output_base):\n",
    "    \"\"\"Process a single site and generate CHM.\n",
    "    \n",
    "    Returns:\n",
    "        dict with processing results and statistics\n",
    "    \"\"\"\n",
    "    result = {\n",
    "        'site_id': site_id,\n",
    "        'name': site_info['name'],\n",
    "        'status': 'pending',\n",
    "        'error': None,\n",
    "        'outputs': {},\n",
    "        'statistics': {},\n",
    "        'timing': {}\n",
    "    }\n",
    "    \n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    try:\n",
    "        # Create site output directories\n",
    "        chm_dir = output_base / 'chm' / site_id\n",
    "        dtm_dir = output_base / 'dtm' / site_id\n",
    "        dsm_dir = output_base / 'dsm' / site_id\n",
    "        for d in [chm_dir, dtm_dir, dsm_dir]:\n",
    "            d.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Convert bbox to polygons\n",
    "        bbox = site_info['bbox']\n",
    "        aoi_gcs, aoi_3857 = bbox_to_polygon(bbox)\n",
    "        aoi_area_m2 = aoi_3857.area\n",
    "        \n",
    "        # Find intersecting 3DEP datasets\n",
    "        intersecting = []\n",
    "        for idx, row in df_3dep.iterrows():\n",
    "            if row['geometry_3857'].intersects(aoi_3857):\n",
    "                intersecting.append({\n",
    "                    'name': row['name'],\n",
    "                    'count': row['count'],\n",
    "                    'area': row['geometry_3857'].area\n",
    "                })\n",
    "        \n",
    "        if not intersecting:\n",
    "            result['status'] = 'failed'\n",
    "            result['error'] = 'No 3DEP coverage'\n",
    "            return result\n",
    "        \n",
    "        dataset_names = [ds['name'] for ds in intersecting]\n",
    "        \n",
    "        # Estimate point density\n",
    "        total_points_est = sum(\n",
    "            (aoi_area_m2 / ds['area']) * ds['count'] \n",
    "            for ds in intersecting\n",
    "        )\n",
    "        density_est = total_points_est / aoi_area_m2\n",
    "        \n",
    "        # Select resolution\n",
    "        resolution = get_resolution(density_est)\n",
    "        \n",
    "        result['statistics']['area_km2'] = aoi_area_m2 / 1e6\n",
    "        result['statistics']['estimated_points'] = int(total_points_est)\n",
    "        result['statistics']['estimated_density'] = density_est\n",
    "        result['statistics']['resolution'] = resolution\n",
    "        result['statistics']['datasets'] = dataset_names\n",
    "        \n",
    "        # Define output paths\n",
    "        dsm_path = dsm_dir / f\"{site_id}_dsm.tif\"\n",
    "        dtm_path = dtm_dir / f\"{site_id}_dtm.tif\"\n",
    "        chm_path = chm_dir / f\"{site_id}_chm.tif\"\n",
    "        \n",
    "        # Generate DSM\n",
    "        dsm_start = datetime.now()\n",
    "        dsm_pipeline = make_dem_pipeline(\n",
    "            aoi_3857.wkt, dataset_names, 1.0, resolution, 'dsm', dsm_path\n",
    "        )\n",
    "        pdal.Pipeline(json.dumps(dsm_pipeline)).execute_streaming(chunk_size=1000000)\n",
    "        result['timing']['dsm_seconds'] = (datetime.now() - dsm_start).total_seconds()\n",
    "        \n",
    "        # Generate DTM\n",
    "        dtm_start = datetime.now()\n",
    "        dtm_pipeline = make_dem_pipeline(\n",
    "            aoi_3857.wkt, dataset_names, 1.0, resolution, 'dtm', dtm_path\n",
    "        )\n",
    "        pdal.Pipeline(json.dumps(dtm_pipeline)).execute_streaming(chunk_size=1000000)\n",
    "        result['timing']['dtm_seconds'] = (datetime.now() - dtm_start).total_seconds()\n",
    "        \n",
    "        # Calculate CHM\n",
    "        chm_start = datetime.now()\n",
    "        dsm = rio.open_rasterio(dsm_path, masked=True)\n",
    "        dtm = rio.open_rasterio(dtm_path, masked=True)\n",
    "        \n",
    "        # Align if needed\n",
    "        if dsm.shape != dtm.shape:\n",
    "            if dsm.shape > dtm.shape:\n",
    "                dsm = dsm.rio.reproject_match(dtm)\n",
    "            else:\n",
    "                dtm = dtm.rio.reproject_match(dsm)\n",
    "        \n",
    "        dsm = dsm.assign_coords({\"x\": dtm.x, \"y\": dtm.y})\n",
    "        \n",
    "        # Direct subtraction - NO smoothing\n",
    "        chm = dsm - dtm\n",
    "        chm = chm.compute()\n",
    "        chm.rio.set_nodata(dtm.rio.nodata, inplace=True)\n",
    "        chm.rio.to_raster(chm_path)\n",
    "        \n",
    "        result['timing']['chm_seconds'] = (datetime.now() - chm_start).total_seconds()\n",
    "        \n",
    "        # Calculate CHM statistics\n",
    "        chm_data = chm.values.flatten()\n",
    "        chm_valid = chm_data[~np.isnan(chm_data)]\n",
    "        chm_valid = chm_valid[chm_valid != chm.rio.nodata]\n",
    "        chm_heights = chm_valid[(chm_valid >= 0) & (chm_valid <= 150)]\n",
    "        \n",
    "        if len(chm_heights) > 0:\n",
    "            result['statistics']['chm_min_m'] = float(np.min(chm_heights))\n",
    "            result['statistics']['chm_max_m'] = float(np.max(chm_heights))\n",
    "            result['statistics']['chm_mean_m'] = float(np.mean(chm_heights))\n",
    "            result['statistics']['chm_median_m'] = float(np.median(chm_heights))\n",
    "            result['statistics']['chm_std_m'] = float(np.std(chm_heights))\n",
    "            result['statistics']['valid_pixels'] = int(len(chm_heights))\n",
    "        \n",
    "        # Generate preview\n",
    "        preview_path = chm_dir / f\"{site_id}_preview.png\"\n",
    "        fig, ax = plt.subplots(figsize=(8, 8))\n",
    "        chm.squeeze().plot(ax=ax, cmap='Greens', robust=True)\n",
    "        ax.set_title(f\"CHM: {site_info['name']}\")\n",
    "        ax.set_aspect('equal')\n",
    "        plt.savefig(preview_path, dpi=100, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # Cleanup\n",
    "        dsm.close()\n",
    "        dtm.close()\n",
    "        chm.close()\n",
    "        \n",
    "        # Record outputs\n",
    "        result['outputs'] = {\n",
    "            'chm': str(chm_path),\n",
    "            'dsm': str(dsm_path),\n",
    "            'dtm': str(dtm_path),\n",
    "            'preview': str(preview_path)\n",
    "        }\n",
    "        \n",
    "        result['status'] = 'completed'\n",
    "        \n",
    "    except Exception as e:\n",
    "        result['status'] = 'failed'\n",
    "        result['error'] = str(e)\n",
    "        result['traceback'] = traceback.format_exc()\n",
    "    \n",
    "    result['timing']['total_seconds'] = (datetime.now() - start_time).total_seconds()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load 3DEP Boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading 3DEP dataset boundaries...\")\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/hobuinc/usgs-lidar/master/boundaries/resources.geojson'\n",
    "r = requests.get(url)\n",
    "\n",
    "boundaries_file = OUTPUT_BASE / 'resources.geojson'\n",
    "with open(boundaries_file, 'w') as f:\n",
    "    f.write(r.content.decode(\"utf-8\"))\n",
    "\n",
    "df_3dep = gpd.read_file(boundaries_file)\n",
    "df_3dep['geometry_3857'] = df_3dep['geometry'].apply(gcs_to_proj)\n",
    "\n",
    "print(f\"Loaded {len(df_3dep)} 3DEP datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Process Selected Sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all selected sites\n",
    "results = []\n",
    "batch_start = datetime.now()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"BATCH PROCESSING: {len(SELECTED_SITES)} sites\")\n",
    "print(f\"Started: {batch_start.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "for i, site_id in enumerate(tqdm(SELECTED_SITES, desc=\"Processing sites\")):\n",
    "    if site_id not in sites:\n",
    "        print(f\"  [SKIP] {site_id}: Not found in config\")\n",
    "        continue\n",
    "    \n",
    "    site_info = sites[site_id]\n",
    "    print(f\"\\n[{i+1}/{len(SELECTED_SITES)}] Processing: {site_info['name']}\")\n",
    "    \n",
    "    result = process_site(site_id, site_info, df_3dep, OUTPUT_BASE)\n",
    "    results.append(result)\n",
    "    \n",
    "    if result['status'] == 'completed':\n",
    "        print(f\"  [OK] Completed in {result['timing']['total_seconds']:.1f}s\")\n",
    "        print(f\"       Max height: {result['statistics'].get('chm_max_m', 'N/A'):.1f}m\")\n",
    "    else:\n",
    "        print(f\"  [FAILED] {result['error']}\")\n",
    "\n",
    "batch_end = datetime.now()\n",
    "batch_elapsed = (batch_end - batch_start).total_seconds()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"BATCH COMPLETE\")\n",
    "print(f\"Total time: {batch_elapsed:.1f}s ({batch_elapsed/60:.1f} min)\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Generate Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "completed = [r for r in results if r['status'] == 'completed']\n",
    "failed = [r for r in results if r['status'] == 'failed']\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"PROCESSING SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total sites: {len(results)}\")\n",
    "print(f\"Completed:   {len(completed)}\")\n",
    "print(f\"Failed:      {len(failed)}\")\n",
    "\n",
    "if completed:\n",
    "    max_heights = [r['statistics'].get('chm_max_m', 0) for r in completed]\n",
    "    mean_heights = [r['statistics'].get('chm_mean_m', 0) for r in completed]\n",
    "    \n",
    "    print(f\"\\nCanopy Height Statistics Across Sites:\")\n",
    "    print(f\"  Tallest canopy: {max(max_heights):.1f}m\")\n",
    "    print(f\"  Average max height: {np.mean(max_heights):.1f}m\")\n",
    "    print(f\"  Average mean height: {np.mean(mean_heights):.1f}m\")\n",
    "\n",
    "if failed:\n",
    "    print(f\"\\nFailed Sites:\")\n",
    "    for r in failed:\n",
    "        print(f\"  - {r['site_id']}: {r['error']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results DataFrame\n",
    "summary_data = []\n",
    "for r in results:\n",
    "    summary_data.append({\n",
    "        'site_id': r['site_id'],\n",
    "        'name': r['name'],\n",
    "        'status': r['status'],\n",
    "        'area_km2': r['statistics'].get('area_km2'),\n",
    "        'resolution_m': r['statistics'].get('resolution'),\n",
    "        'max_height_m': r['statistics'].get('chm_max_m'),\n",
    "        'mean_height_m': r['statistics'].get('chm_mean_m'),\n",
    "        'time_seconds': r['timing'].get('total_seconds'),\n",
    "        'error': r.get('error')\n",
    "    })\n",
    "\n",
    "df_summary = pd.DataFrame(summary_data)\n",
    "print(\"\\nResults Summary:\")\n",
    "print(df_summary.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save detailed results\n",
    "results_file = OUTPUT_BASE / 'logs' / f\"batch_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "\n",
    "batch_report = {\n",
    "    'batch_info': {\n",
    "        'started': batch_start.isoformat(),\n",
    "        'completed': batch_end.isoformat(),\n",
    "        'total_seconds': batch_elapsed,\n",
    "        'sites_processed': len(results),\n",
    "        'sites_completed': len(completed),\n",
    "        'sites_failed': len(failed)\n",
    "    },\n",
    "    'processing_config': processing_config,\n",
    "    'results': results\n",
    "}\n",
    "\n",
    "with open(results_file, 'w') as f:\n",
    "    json.dump(batch_report, f, indent=2, default=str)\n",
    "\n",
    "print(f\"\\nFull results saved to: {results_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save CSV summary\n",
    "csv_file = OUTPUT_BASE / 'logs' / f\"batch_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "df_summary.to_csv(csv_file, index=False)\n",
    "print(f\"CSV summary saved to: {csv_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualize Results Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare canopy heights across sites\n",
    "if len(completed) > 1:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Bar chart of max heights\n",
    "    ax1 = axes[0]\n",
    "    df_completed = df_summary[df_summary['status'] == 'completed'].sort_values('max_height_m', ascending=True)\n",
    "    ax1.barh(df_completed['name'], df_completed['max_height_m'], color='forestgreen')\n",
    "    ax1.set_xlabel('Maximum Canopy Height (m)')\n",
    "    ax1.set_title('Maximum Canopy Heights by Site')\n",
    "    \n",
    "    # Scatter: mean vs max height\n",
    "    ax2 = axes[1]\n",
    "    ax2.scatter(df_completed['mean_height_m'], df_completed['max_height_m'], \n",
    "                s=100, c='forestgreen', alpha=0.7)\n",
    "    for _, row in df_completed.iterrows():\n",
    "        ax2.annotate(row['site_id'], (row['mean_height_m'], row['max_height_m']),\n",
    "                    fontsize=8, ha='left')\n",
    "    ax2.set_xlabel('Mean Canopy Height (m)')\n",
    "    ax2.set_ylabel('Maximum Canopy Height (m)')\n",
    "    ax2.set_title('Mean vs Maximum Canopy Height')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    comparison_path = OUTPUT_BASE / 'logs' / 'site_comparison.png'\n",
    "    plt.savefig(comparison_path, dpi=150, bbox_inches='tight')\n",
    "    print(f\"Comparison plot saved to: {comparison_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Need at least 2 completed sites for comparison visualization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Output Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"OUTPUT FILES\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nBase directory: {OUTPUT_BASE.absolute()}\")\n",
    "\n",
    "for r in completed:\n",
    "    print(f\"\\n{r['site_id']}:\")\n",
    "    for output_type, path in r['outputs'].items():\n",
    "        print(f\"  {output_type}: {Path(path).name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "- Original workflows: [OpenTopography OT_3DEP_Workflows](https://github.com/OpenTopography/OT_3DEP_Workflows)\n",
    "- 3DEP Program: https://www.usgs.gov/3d-elevation-program\n",
    "- PDAL: https://pdal.io/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
