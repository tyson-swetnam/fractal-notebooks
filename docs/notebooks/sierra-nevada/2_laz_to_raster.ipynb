{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. LAZ to Raster Processing - Sierra Nevada Giant Forest\n",
    "\n",
    "This notebook processes LAZ point cloud files into raster products (DEM, DSM, CHM) using PDAL and GDAL.\n",
    "\n",
    "**Input:** LAZ files downloaded in `1_download_data.ipynb`\n",
    "\n",
    "**Output:**\n",
    "- DEM (Digital Elevation Model) - Ground surface\n",
    "- DSM (Digital Surface Model) - Top of canopy\n",
    "- CHM (Canopy Height Model) = DSM - DEM\n",
    "- Cloud-Optimized GeoTIFFs (COG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "# Check for required libraries\n",
    "try:\n",
    "    import pdal\n",
    "    print(f\"PDAL version: {pdal.info.version}\")\n",
    "except ImportError:\n",
    "    print(\"PDAL not installed. Install with: conda install -c conda-forge pdal python-pdal\")\n",
    "\n",
    "try:\n",
    "    import rioxarray as rxr\n",
    "    print(\"rioxarray available\")\n",
    "except ImportError:\n",
    "    print(\"rioxarray not installed. Install with: conda install -c conda-forge rioxarray\")\n",
    "\n",
    "try:\n",
    "    from osgeo import gdal\n",
    "    print(f\"GDAL version: {gdal.VersionInfo()}\")\n",
    "except ImportError:\n",
    "    print(\"GDAL not installed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data directories\n",
    "DATA_ROOT = Path.home() / \"data-store\" / \"data\" / \"output\" / \"sierra-nevada\"\n",
    "RAW_DIR = DATA_ROOT / \"raw\"\n",
    "LAZ_DIR = RAW_DIR / \"laz\"\n",
    "PROCESSED_DIR = DATA_ROOT / \"processed\"\n",
    "DEM_DIR = PROCESSED_DIR / \"dem\"\n",
    "DSM_DIR = PROCESSED_DIR / \"dsm\"\n",
    "CHM_DIR = PROCESSED_DIR / \"chm\"\n",
    "COG_DIR = PROCESSED_DIR / \"cog\"\n",
    "\n",
    "# Ensure output directories exist\n",
    "for d in [DEM_DIR, DSM_DIR, CHM_DIR, COG_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Processing parameters\n",
    "RESOLUTION = 0.5  # meters (matches BCI for comparison)\n",
    "CRS = \"EPSG:32611\"  # UTM Zone 11N\n",
    "\n",
    "print(f\"LAZ input: {LAZ_DIR}\")\n",
    "print(f\"DEM output: {DEM_DIR}\")\n",
    "print(f\"DSM output: {DSM_DIR}\")\n",
    "print(f\"CHM output: {CHM_DIR}\")\n",
    "print(f\"Resolution: {RESOLUTION}m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inventory LAZ Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all LAZ files\n",
    "laz_files = sorted(LAZ_DIR.glob('*.laz'))\n",
    "print(f\"Found {len(laz_files)} LAZ files\")\n",
    "\n",
    "# Calculate total size\n",
    "total_size = sum(f.stat().st_size for f in laz_files)\n",
    "print(f\"Total size: {total_size / (1024**3):.2f} GB\")\n",
    "\n",
    "# List files\n",
    "if laz_files:\n",
    "    print(\"\\nLAZ files:\")\n",
    "    for f in laz_files[:10]:\n",
    "        size_mb = f.stat().st_size / (1024**2)\n",
    "        print(f\"  {f.name}: {size_mb:.1f} MB\")\n",
    "    if len(laz_files) > 10:\n",
    "        print(f\"  ... and {len(laz_files) - 10} more\")\n",
    "else:\n",
    "    print(\"\\n⚠ No LAZ files found. Run 1_download_data.ipynb first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect LAZ Metadata\n",
    "\n",
    "Examine one LAZ file to understand the point cloud structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_laz_info(laz_path):\n",
    "    \"\"\"\n",
    "    Get metadata from a LAZ file using PDAL.\n",
    "    \"\"\"\n",
    "    pipeline = pdal.Pipeline(json.dumps({\n",
    "        \"pipeline\": [\n",
    "            str(laz_path)\n",
    "        ]\n",
    "    }))\n",
    "    \n",
    "    # Get metadata without reading all points\n",
    "    pipeline.execute()\n",
    "    metadata = pipeline.metadata\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "if laz_files:\n",
    "    sample_file = laz_files[0]\n",
    "    print(f\"Inspecting: {sample_file.name}\\n\")\n",
    "    \n",
    "    try:\n",
    "        # Quick stats using pdal info\n",
    "        result = subprocess.run(\n",
    "            ['pdal', 'info', '--summary', str(sample_file)],\n",
    "            capture_output=True,\n",
    "            text=True\n",
    "        )\n",
    "        if result.returncode == 0:\n",
    "            info = json.loads(result.stdout)\n",
    "            summary = info.get('summary', {})\n",
    "            \n",
    "            print(f\"Point count: {summary.get('num_points', 'N/A'):,}\")\n",
    "            \n",
    "            bounds = summary.get('bounds', {})\n",
    "            if bounds:\n",
    "                print(f\"\\nBounds:\")\n",
    "                print(f\"  X: {bounds.get('minx', 0):.2f} to {bounds.get('maxx', 0):.2f}\")\n",
    "                print(f\"  Y: {bounds.get('miny', 0):.2f} to {bounds.get('maxy', 0):.2f}\")\n",
    "                print(f\"  Z: {bounds.get('minz', 0):.2f} to {bounds.get('maxz', 0):.2f}\")\n",
    "            \n",
    "            srs = summary.get('srs', {})\n",
    "            if srs:\n",
    "                print(f\"\\nCRS: {srs.get('wkt', 'Unknown')[:100]}...\")\n",
    "        else:\n",
    "            print(f\"Error: {result.stderr}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error inspecting file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get classification info\n",
    "if laz_files:\n",
    "    sample_file = laz_files[0]\n",
    "    \n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            ['pdal', 'info', '--stats', str(sample_file)],\n",
    "            capture_output=True,\n",
    "            text=True\n",
    "        )\n",
    "        if result.returncode == 0:\n",
    "            info = json.loads(result.stdout)\n",
    "            stats = info.get('stats', {}).get('statistic', [])\n",
    "            \n",
    "            # Find Classification stats\n",
    "            for stat in stats:\n",
    "                if stat.get('name') == 'Classification':\n",
    "                    print(\"Classification statistics:\")\n",
    "                    counts = stat.get('counts', [])\n",
    "                    if counts:\n",
    "                        for count_str in counts:\n",
    "                            # Format: \"class_value/count\"\n",
    "                            parts = count_str.split('/')\n",
    "                            if len(parts) == 2:\n",
    "                                class_val = int(float(parts[0]))\n",
    "                                count = int(parts[1])\n",
    "                                class_name = {\n",
    "                                    0: \"Never classified\",\n",
    "                                    1: \"Unassigned\",\n",
    "                                    2: \"Ground\",\n",
    "                                    3: \"Low Vegetation\",\n",
    "                                    4: \"Medium Vegetation\",\n",
    "                                    5: \"High Vegetation\",\n",
    "                                    6: \"Building\",\n",
    "                                    7: \"Low Point (noise)\",\n",
    "                                    9: \"Water\",\n",
    "                                    17: \"Bridge Deck\",\n",
    "                                    18: \"High Noise\"\n",
    "                                }.get(class_val, f\"Class {class_val}\")\n",
    "                                print(f\"  {class_val}: {class_name}: {count:,}\")\n",
    "                    break\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process LAZ to DEM (Ground Surface)\n",
    "\n",
    "Extract ground-classified points and create a Digital Elevation Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_laz_to_dem(laz_path, output_path, resolution=0.5):\n",
    "    \"\"\"\n",
    "    Process a LAZ file to DEM using ground-classified points.\n",
    "    \n",
    "    Uses PDAL pipeline:\n",
    "    1. Read LAZ file\n",
    "    2. Filter to ground points (classification 2)\n",
    "    3. Write to GeoTIFF using IDW interpolation\n",
    "    \"\"\"\n",
    "    pipeline_json = {\n",
    "        \"pipeline\": [\n",
    "            {\n",
    "                \"type\": \"readers.las\",\n",
    "                \"filename\": str(laz_path)\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"filters.range\",\n",
    "                \"limits\": \"Classification[2:2]\"  # Ground only\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"filters.outlier\",\n",
    "                \"method\": \"statistical\",\n",
    "                \"mean_k\": 12,\n",
    "                \"multiplier\": 2.2\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"writers.gdal\",\n",
    "                \"filename\": str(output_path),\n",
    "                \"gdaldriver\": \"GTiff\",\n",
    "                \"output_type\": \"idw\",  # Inverse distance weighting\n",
    "                \"resolution\": resolution,\n",
    "                \"data_type\": \"float32\",\n",
    "                \"gdalopts\": \"COMPRESS=DEFLATE,TILED=YES\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    pipeline = pdal.Pipeline(json.dumps(pipeline_json))\n",
    "    count = pipeline.execute()\n",
    "    \n",
    "    return count\n",
    "\n",
    "# Test with first file\n",
    "if laz_files:\n",
    "    test_laz = laz_files[0]\n",
    "    test_dem = DEM_DIR / f\"{test_laz.stem}_dem.tif\"\n",
    "    \n",
    "    print(f\"Processing: {test_laz.name}\")\n",
    "    print(f\"Output: {test_dem}\")\n",
    "    \n",
    "    try:\n",
    "        count = process_laz_to_dem(test_laz, test_dem, RESOLUTION)\n",
    "        print(f\"\\n✓ Processed {count:,} ground points\")\n",
    "        \n",
    "        if test_dem.exists():\n",
    "            size_mb = test_dem.stat().st_size / (1024**2)\n",
    "            print(f\"Output size: {size_mb:.1f} MB\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process LAZ to DSM (Top of Canopy)\n",
    "\n",
    "Extract first returns to create a Digital Surface Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_laz_to_dsm(laz_path, output_path, resolution=0.5):\n",
    "    \"\"\"\n",
    "    Process a LAZ file to DSM using first returns.\n",
    "    \n",
    "    Uses PDAL pipeline:\n",
    "    1. Read LAZ file\n",
    "    2. Filter to first returns\n",
    "    3. Write to GeoTIFF using max value (highest point per cell)\n",
    "    \"\"\"\n",
    "    pipeline_json = {\n",
    "        \"pipeline\": [\n",
    "            {\n",
    "                \"type\": \"readers.las\",\n",
    "                \"filename\": str(laz_path)\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"filters.returns\",\n",
    "                \"groups\": \"first,only\"  # First and only returns\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"filters.outlier\",\n",
    "                \"method\": \"statistical\",\n",
    "                \"mean_k\": 12,\n",
    "                \"multiplier\": 2.2\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"writers.gdal\",\n",
    "                \"filename\": str(output_path),\n",
    "                \"gdaldriver\": \"GTiff\",\n",
    "                \"output_type\": \"max\",  # Maximum height per cell\n",
    "                \"resolution\": resolution,\n",
    "                \"data_type\": \"float32\",\n",
    "                \"gdalopts\": \"COMPRESS=DEFLATE,TILED=YES\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    pipeline = pdal.Pipeline(json.dumps(pipeline_json))\n",
    "    count = pipeline.execute()\n",
    "    \n",
    "    return count\n",
    "\n",
    "# Test with first file\n",
    "if laz_files:\n",
    "    test_laz = laz_files[0]\n",
    "    test_dsm = DSM_DIR / f\"{test_laz.stem}_dsm.tif\"\n",
    "    \n",
    "    print(f\"Processing: {test_laz.name}\")\n",
    "    print(f\"Output: {test_dsm}\")\n",
    "    \n",
    "    try:\n",
    "        count = process_laz_to_dsm(test_laz, test_dsm, RESOLUTION)\n",
    "        print(f\"\\n✓ Processed {count:,} first return points\")\n",
    "        \n",
    "        if test_dsm.exists():\n",
    "            size_mb = test_dsm.stat().st_size / (1024**2)\n",
    "            print(f\"Output size: {size_mb:.1f} MB\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute CHM (Canopy Height Model)\n",
    "\n",
    "CHM = DSM - DEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_chm(dem_path, dsm_path, output_path):\n",
    "    \"\"\"\n",
    "    Compute CHM = DSM - DEM using rioxarray.\n",
    "    \"\"\"\n",
    "    # Load rasters\n",
    "    dem = rxr.open_rasterio(dem_path, masked=True).squeeze()\n",
    "    dsm = rxr.open_rasterio(dsm_path, masked=True).squeeze()\n",
    "    \n",
    "    # Compute CHM\n",
    "    chm = dsm - dem\n",
    "    \n",
    "    # Clip negative values (artifacts)\n",
    "    chm = chm.where(chm >= 0, 0)\n",
    "    \n",
    "    # Save\n",
    "    chm.rio.to_raster(\n",
    "        output_path,\n",
    "        driver=\"GTiff\",\n",
    "        dtype=\"float32\",\n",
    "        compress=\"deflate\",\n",
    "        tiled=True\n",
    "    )\n",
    "    \n",
    "    return chm\n",
    "\n",
    "# Test CHM computation\n",
    "if laz_files:\n",
    "    test_laz = laz_files[0]\n",
    "    test_dem = DEM_DIR / f\"{test_laz.stem}_dem.tif\"\n",
    "    test_dsm = DSM_DIR / f\"{test_laz.stem}_dsm.tif\"\n",
    "    test_chm = CHM_DIR / f\"{test_laz.stem}_chm.tif\"\n",
    "    \n",
    "    if test_dem.exists() and test_dsm.exists():\n",
    "        print(f\"Computing CHM: {test_chm.name}\")\n",
    "        \n",
    "        try:\n",
    "            chm = compute_chm(test_dem, test_dsm, test_chm)\n",
    "            \n",
    "            print(f\"\\n✓ CHM computed\")\n",
    "            print(f\"  Min height: {float(chm.min()):.2f} m\")\n",
    "            print(f\"  Max height: {float(chm.max()):.2f} m\")\n",
    "            print(f\"  Mean height: {float(chm.mean()):.2f} m\")\n",
    "            \n",
    "            if test_chm.exists():\n",
    "                size_mb = test_chm.stat().st_size / (1024**2)\n",
    "                print(f\"  Output size: {size_mb:.1f} MB\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "    else:\n",
    "        print(\"DEM and/or DSM not yet generated. Run cells above first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Process All LAZ Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_laz_files(laz_files, dem_dir, dsm_dir, chm_dir, resolution=0.5):\n",
    "    \"\"\"\n",
    "    Process all LAZ files to DEM, DSM, and CHM.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for i, laz_path in enumerate(laz_files):\n",
    "        print(f\"\\n[{i+1}/{len(laz_files)}] Processing {laz_path.name}\")\n",
    "        \n",
    "        stem = laz_path.stem\n",
    "        dem_path = dem_dir / f\"{stem}_dem.tif\"\n",
    "        dsm_path = dsm_dir / f\"{stem}_dsm.tif\"\n",
    "        chm_path = chm_dir / f\"{stem}_chm.tif\"\n",
    "        \n",
    "        result = {\n",
    "            'laz': laz_path.name,\n",
    "            'dem_status': 'skipped',\n",
    "            'dsm_status': 'skipped',\n",
    "            'chm_status': 'skipped'\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Process DEM\n",
    "            if not dem_path.exists():\n",
    "                print(f\"  Creating DEM...\")\n",
    "                process_laz_to_dem(laz_path, dem_path, resolution)\n",
    "                result['dem_status'] = 'created'\n",
    "            else:\n",
    "                result['dem_status'] = 'exists'\n",
    "            \n",
    "            # Process DSM\n",
    "            if not dsm_path.exists():\n",
    "                print(f\"  Creating DSM...\")\n",
    "                process_laz_to_dsm(laz_path, dsm_path, resolution)\n",
    "                result['dsm_status'] = 'created'\n",
    "            else:\n",
    "                result['dsm_status'] = 'exists'\n",
    "            \n",
    "            # Compute CHM\n",
    "            if not chm_path.exists() and dem_path.exists() and dsm_path.exists():\n",
    "                print(f\"  Computing CHM...\")\n",
    "                compute_chm(dem_path, dsm_path, chm_path)\n",
    "                result['chm_status'] = 'created'\n",
    "            elif chm_path.exists():\n",
    "                result['chm_status'] = 'exists'\n",
    "            \n",
    "            print(f\"  ✓ Complete\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error: {e}\")\n",
    "            result['error'] = str(e)\n",
    "        \n",
    "        results.append(result)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all files (uncomment to run)\n",
    "# WARNING: This may take a long time depending on the number and size of files\n",
    "\n",
    "if laz_files:\n",
    "    print(f\"Processing {len(laz_files)} LAZ files...\")\n",
    "    print(f\"Resolution: {RESOLUTION}m\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Uncomment the line below to process all files\n",
    "    # results = process_all_laz_files(laz_files, DEM_DIR, DSM_DIR, CHM_DIR, RESOLUTION)\n",
    "    \n",
    "    print(\"\\nTo process all files, uncomment the line above.\")\n",
    "else:\n",
    "    print(\"No LAZ files found. Run 1_download_data.ipynb first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Tiles into Single Rasters\n",
    "\n",
    "Merge individual tile rasters into whole-area DEM, DSM, and CHM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_rasters(input_dir, output_path, pattern=\"*.tif\"):\n",
    "    \"\"\"\n",
    "    Merge multiple raster tiles into a single file using GDAL.\n",
    "    \"\"\"\n",
    "    input_files = list(input_dir.glob(pattern))\n",
    "    \n",
    "    if not input_files:\n",
    "        print(f\"No files matching {pattern} in {input_dir}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Merging {len(input_files)} files...\")\n",
    "    \n",
    "    # Create file list for GDAL\n",
    "    file_list_path = input_dir / \"_merge_list.txt\"\n",
    "    with open(file_list_path, 'w') as f:\n",
    "        for fp in input_files:\n",
    "            f.write(str(fp) + '\\n')\n",
    "    \n",
    "    # Run gdal_merge.py or gdalbuildvrt + gdal_translate\n",
    "    vrt_path = output_path.with_suffix('.vrt')\n",
    "    \n",
    "    # Build VRT (virtual raster)\n",
    "    cmd_vrt = [\n",
    "        'gdalbuildvrt',\n",
    "        '-input_file_list', str(file_list_path),\n",
    "        str(vrt_path)\n",
    "    ]\n",
    "    \n",
    "    result = subprocess.run(cmd_vrt, capture_output=True, text=True)\n",
    "    if result.returncode != 0:\n",
    "        print(f\"VRT build error: {result.stderr}\")\n",
    "        return None\n",
    "    \n",
    "    # Convert VRT to GeoTIFF\n",
    "    cmd_translate = [\n",
    "        'gdal_translate',\n",
    "        '-of', 'GTiff',\n",
    "        '-co', 'COMPRESS=DEFLATE',\n",
    "        '-co', 'TILED=YES',\n",
    "        '-co', 'BIGTIFF=YES',\n",
    "        str(vrt_path),\n",
    "        str(output_path)\n",
    "    ]\n",
    "    \n",
    "    result = subprocess.run(cmd_translate, capture_output=True, text=True)\n",
    "    if result.returncode != 0:\n",
    "        print(f\"Translation error: {result.stderr}\")\n",
    "        return None\n",
    "    \n",
    "    # Cleanup\n",
    "    file_list_path.unlink()\n",
    "    vrt_path.unlink()\n",
    "    \n",
    "    print(f\"✓ Merged to {output_path}\")\n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge DEM tiles\n",
    "dem_files = list(DEM_DIR.glob('*_dem.tif'))\n",
    "if dem_files:\n",
    "    merged_dem = PROCESSED_DIR / \"giant_forest_dem.tif\"\n",
    "    merge_rasters(DEM_DIR, merged_dem, '*_dem.tif')\n",
    "else:\n",
    "    print(\"No DEM files to merge yet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge DSM tiles\n",
    "dsm_files = list(DSM_DIR.glob('*_dsm.tif'))\n",
    "if dsm_files:\n",
    "    merged_dsm = PROCESSED_DIR / \"giant_forest_dsm.tif\"\n",
    "    merge_rasters(DSM_DIR, merged_dsm, '*_dsm.tif')\n",
    "else:\n",
    "    print(\"No DSM files to merge yet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge CHM tiles\n",
    "chm_files = list(CHM_DIR.glob('*_chm.tif'))\n",
    "if chm_files:\n",
    "    merged_chm = PROCESSED_DIR / \"giant_forest_chm.tif\"\n",
    "    merge_rasters(CHM_DIR, merged_chm, '*_chm.tif')\n",
    "else:\n",
    "    print(\"No CHM files to merge yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to Cloud-Optimized GeoTIFF (COG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_cog(input_path, output_path):\n",
    "    \"\"\"\n",
    "    Convert a GeoTIFF to Cloud-Optimized GeoTIFF.\n",
    "    \"\"\"\n",
    "    cmd = [\n",
    "        'gdal_translate',\n",
    "        '-of', 'COG',\n",
    "        '-co', 'COMPRESS=DEFLATE',\n",
    "        '-co', 'OVERVIEWS=AUTO',\n",
    "        '-co', 'BIGTIFF=YES',\n",
    "        str(input_path),\n",
    "        str(output_path)\n",
    "    ]\n",
    "    \n",
    "    result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "    if result.returncode != 0:\n",
    "        # Try rio-cogeo as fallback\n",
    "        try:\n",
    "            from rio_cogeo.cogeo import cog_translate\n",
    "            from rio_cogeo.profiles import cog_profiles\n",
    "            \n",
    "            cog_translate(\n",
    "                str(input_path),\n",
    "                str(output_path),\n",
    "                cog_profiles.get(\"deflate\"),\n",
    "                overview_level=5\n",
    "            )\n",
    "        except ImportError:\n",
    "            print(f\"COG conversion failed: {result.stderr}\")\n",
    "            return None\n",
    "    \n",
    "    print(f\"✓ COG created: {output_path}\")\n",
    "    return output_path\n",
    "\n",
    "# Convert merged files to COG\n",
    "merged_dem = PROCESSED_DIR / \"giant_forest_dem.tif\"\n",
    "merged_dsm = PROCESSED_DIR / \"giant_forest_dsm.tif\"\n",
    "merged_chm = PROCESSED_DIR / \"giant_forest_chm.tif\"\n",
    "\n",
    "if merged_dem.exists():\n",
    "    convert_to_cog(merged_dem, COG_DIR / \"giant_forest_dem_cog.tif\")\n",
    "if merged_dsm.exists():\n",
    "    convert_to_cog(merged_dsm, COG_DIR / \"giant_forest_dsm_cog.tif\")\n",
    "if merged_chm.exists():\n",
    "    convert_to_cog(merged_chm, COG_DIR / \"giant_forest_chm_cog.tif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Processing Summary\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Count files\n",
    "dem_count = len(list(DEM_DIR.glob('*.tif')))\n",
    "dsm_count = len(list(DSM_DIR.glob('*.tif')))\n",
    "chm_count = len(list(CHM_DIR.glob('*.tif')))\n",
    "cog_count = len(list(COG_DIR.glob('*.tif')))\n",
    "\n",
    "print(f\"\\nTile Products:\")\n",
    "print(f\"  DEM tiles: {dem_count}\")\n",
    "print(f\"  DSM tiles: {dsm_count}\")\n",
    "print(f\"  CHM tiles: {chm_count}\")\n",
    "\n",
    "print(f\"\\nMerged Products:\")\n",
    "for name, path in [\n",
    "    ('DEM', PROCESSED_DIR / \"giant_forest_dem.tif\"),\n",
    "    ('DSM', PROCESSED_DIR / \"giant_forest_dsm.tif\"),\n",
    "    ('CHM', PROCESSED_DIR / \"giant_forest_chm.tif\")\n",
    "]:\n",
    "    if path.exists():\n",
    "        size_gb = path.stat().st_size / (1024**3)\n",
    "        print(f\"  {name}: {path.name} ({size_gb:.2f} GB)\")\n",
    "    else:\n",
    "        print(f\"  {name}: Not created yet\")\n",
    "\n",
    "print(f\"\\nCOG Products:\")\n",
    "for f in COG_DIR.glob('*.tif'):\n",
    "    size_gb = f.stat().st_size / (1024**3)\n",
    "    print(f\"  {f.name} ({size_gb:.2f} GB)\")\n",
    "\n",
    "if cog_count == 0:\n",
    "    print(\"  No COGs created yet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Processing Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processing metadata\n",
    "processing_metadata = {\n",
    "    'project': 'Sierra Nevada Giant Forest LiDAR',\n",
    "    'processing_date': datetime.now().isoformat(),\n",
    "    'resolution_m': RESOLUTION,\n",
    "    'crs': CRS,\n",
    "    'input': {\n",
    "        'laz_files': len(laz_files),\n",
    "        'laz_dir': str(LAZ_DIR)\n",
    "    },\n",
    "    'output': {\n",
    "        'dem_tiles': dem_count,\n",
    "        'dsm_tiles': dsm_count,\n",
    "        'chm_tiles': chm_count,\n",
    "        'cog_files': cog_count\n",
    "    },\n",
    "    'products': {\n",
    "        'merged_dem': str(PROCESSED_DIR / \"giant_forest_dem.tif\") if (PROCESSED_DIR / \"giant_forest_dem.tif\").exists() else None,\n",
    "        'merged_dsm': str(PROCESSED_DIR / \"giant_forest_dsm.tif\") if (PROCESSED_DIR / \"giant_forest_dsm.tif\").exists() else None,\n",
    "        'merged_chm': str(PROCESSED_DIR / \"giant_forest_chm.tif\") if (PROCESSED_DIR / \"giant_forest_chm.tif\").exists() else None\n",
    "    }\n",
    "}\n",
    "\n",
    "metadata_path = PROCESSED_DIR / \"processing_metadata.json\"\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(processing_metadata, f, indent=2)\n",
    "\n",
    "print(f\"Saved processing metadata to {metadata_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "After processing is complete:\n",
    "\n",
    "1. **Run `3_chm_exploration.ipynb`** to analyze the generated CHM\n",
    "2. **Run `4_fractal_analysis.ipynb`** to compute fractal dimensions\n",
    "\n",
    "See `PLAN.md` for the complete project workflow."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
