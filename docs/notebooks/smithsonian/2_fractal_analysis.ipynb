{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# CHM Fractal Analysis: Barro Colorado Island\n",
    "\n",
    "**Site:** Barro Colorado Island, Panama  \n",
    "**Forest Type:** Tropical Moist Forest (Neotropical Lowland Rainforest)  \n",
    "**Data Source:** [Smithsonian ALS Panama 2023](https://smithsonian.dataone.org/datasets/ALS_Panama_2023/)  \n",
    "**Resolution:** 0.5m\n",
    "\n",
    "## Research Hypotheses\n",
    "\n",
    "This notebook tests fractal properties of the BCI tropical forest canopy:\n",
    "\n",
    "1. **Optimal Filling (H1)** - Old-growth tropical forests maximize light interception, producing higher fractal dimensions than temperate forests\n",
    "2. **Scale Invariance (H2)** - Mature tropical forests show scale-invariant gap distributions (power-law decay)\n",
    "3. **Zeta Distribution (H3)** - Canopy gaps follow power-law with exponent related to optimal packing\n",
    "4. **Universal Repulsion (H4)** - Emergent tree spacing follows competition-driven distributions\n",
    "5. **Vertical Complexity (H5)** - Multi-layered tropical canopy shows higher complexity than single-layer temperate forests\n",
    "\n",
    "## Methods\n",
    "\n",
    "- **Differential Box Counting (DBC)** - Self-affine fractal dimension of height surfaces\n",
    "- **Standard Box Counting** - Self-similar fractal dimension comparison\n",
    "- **Lacunarity Analysis** - Scale invariance and gap texture\n",
    "- **Gap Size Distribution** - Power-law vs exponential fitting\n",
    "- **Nearest Neighbor Spacing** - Tree spacing statistics\n",
    "\n",
    "## Expected Results for Tropical Forest\n",
    "\n",
    "Based on BCI's multi-layered structure and high species diversity:\n",
    "- **Fractal Dimension:** D > 2.5 (higher than temperate due to layering)\n",
    "- **Scale Invariance:** High R² in lacunarity (mature forest)\n",
    "- **Gap Distribution:** Power-law (continuous gap dynamics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import rioxarray as rxr\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import json\n",
    "from scipy import ndimage\n",
    "from scipy.stats import ks_2samp, pearsonr, spearmanr\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.spatial import cKDTree\n",
    "from scipy.special import zeta\n",
    "from rasterio.enums import Resampling\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"Environment ready at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-header",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Site configuration\n",
    "SITE_NAME = \"bci_panama\"\n",
    "SITE_DESCRIPTION = \"Barro Colorado Island - Tropical Moist Forest\"\n",
    "\n",
    "# Data paths\n",
    "DATA_BASE = Path.home() / \"data-store/data/output/smithsonian\"\n",
    "RAW_DATA = DATA_BASE / \"raw\"\n",
    "\n",
    "# Input files\n",
    "CHM_PATH = RAW_DATA / \"chm\" / \"BCI_whole_2023_05_26_chm.tif\"\n",
    "DTM_PATH = RAW_DATA / \"dtm\" / \"BCI_whole_2023_05_26_dtm.tif\"\n",
    "\n",
    "# Output directory\n",
    "OUTPUT_DIR = DATA_BASE / \"analysis\" / \"fractal\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Analysis parameters\n",
    "GAP_THRESHOLD = 2.0  # meters - pixels below this are gaps\n",
    "EMERGENT_THRESHOLD = 35.0  # meters - emergent tree detection\n",
    "LOCAL_MAX_WINDOW = 21  # pixels - for tree top detection (10.5m at 0.5m resolution)\n",
    "\n",
    "# Verify files exist\n",
    "print(\"Input files:\")\n",
    "for name, path in [(\"CHM\", CHM_PATH), (\"DTM\", DTM_PATH)]:\n",
    "    exists = path.exists()\n",
    "    size = f\"{path.stat().st_size / 1e9:.2f} GB\" if exists else \"NOT FOUND\"\n",
    "    print(f\"  [{('OK' if exists else 'MISSING')}] {name}: {path.name} ({size})\")\n",
    "\n",
    "print(f\"\\nOutput: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-header",
   "metadata": {},
   "source": [
    "## 3. Load and Subset CHM\n",
    "\n",
    "The full BCI CHM is 1.1 GB. For efficient analysis, we'll work with a representative subset.\n",
    "You can analyze the full island by setting `USE_SUBSET = False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-chm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CHM\n",
    "print(f\"Loading CHM from: {CHM_PATH.name}\")\n",
    "print(\"This may take a moment for the 1.1 GB file...\")\n",
    "\n",
    "chm_full = rxr.open_rasterio(CHM_PATH, masked=True)\n",
    "\n",
    "print(f\"\\nFull CHM loaded:\")\n",
    "print(f\"  Shape: {chm_full.shape}\")\n",
    "print(f\"  CRS: {chm_full.rio.crs}\")\n",
    "print(f\"  Resolution: {abs(chm_full.rio.resolution()[0]):.2f}m\")\n",
    "print(f\"  Bounds: {chm_full.rio.bounds()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subset-chm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option to use subset for faster analysis\n",
    "USE_SUBSET = True\n",
    "SUBSET_SIZE = 2000  # pixels (1km x 1km at 0.5m resolution)\n",
    "\n",
    "if USE_SUBSET:\n",
    "    # Get center region of the island (avoiding water edges)\n",
    "    full_shape = chm_full.shape\n",
    "    center_y = full_shape[1] // 2\n",
    "    center_x = full_shape[2] // 2\n",
    "    half_size = SUBSET_SIZE // 2\n",
    "    \n",
    "    # Extract subset\n",
    "    chm = chm_full[:, \n",
    "                   center_y - half_size : center_y + half_size,\n",
    "                   center_x - half_size : center_x + half_size]\n",
    "    \n",
    "    print(f\"Using center subset: {SUBSET_SIZE}x{SUBSET_SIZE} pixels\")\n",
    "    print(f\"  = {SUBSET_SIZE * 0.5 / 1000:.1f} km x {SUBSET_SIZE * 0.5 / 1000:.1f} km\")\n",
    "else:\n",
    "    chm = chm_full\n",
    "    print(\"Using full CHM (this may be slow)\")\n",
    "\n",
    "# Get the data array\n",
    "chm_array = chm.values.squeeze().astype(np.float64)\n",
    "\n",
    "# Get NoData value\n",
    "NODATA_VALUE = chm.rio.nodata if chm.rio.nodata is not None else -9999\n",
    "\n",
    "# Create masks\n",
    "nodata_mask = np.isnan(chm_array) | (chm_array == NODATA_VALUE)\n",
    "valid_mask = ~nodata_mask\n",
    "\n",
    "# Get pixel resolution\n",
    "pixel_resolution = abs(chm.rio.resolution()[0])\n",
    "\n",
    "print(f\"\\nAnalysis array:\")\n",
    "print(f\"  Shape: {chm_array.shape}\")\n",
    "print(f\"  Valid pixels: {valid_mask.sum():,} ({100*valid_mask.sum()/chm_array.size:.1f}%)\")\n",
    "print(f\"  Resolution: {pixel_resolution:.3f}m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "basic-stats",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic height statistics for subset\n",
    "valid_heights = chm_array[valid_mask]\n",
    "# Filter to reasonable values\n",
    "valid_heights = valid_heights[(valid_heights >= 0) & (valid_heights <= 80)]\n",
    "\n",
    "print(\"Height Statistics (subset):\")\n",
    "print(f\"  Min:    {np.min(valid_heights):.2f} m\")\n",
    "print(f\"  Max:    {np.max(valid_heights):.2f} m\")\n",
    "print(f\"  Mean:   {np.mean(valid_heights):.2f} m\")\n",
    "print(f\"  Median: {np.median(valid_heights):.2f} m\")\n",
    "print(f\"  Std:    {np.std(valid_heights):.2f} m\")\n",
    "print(f\"  P95:    {np.percentile(valid_heights, 95):.2f} m\")\n",
    "print(f\"  P99:    {np.percentile(valid_heights, 99):.2f} m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h1-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# HYPOTHESIS 1: Optimal Filling\n",
    "\n",
    "**Prediction:** Tropical old-growth forests maximize light interception through multi-layered canopy structure, producing higher fractal dimensions than temperate forests.\n",
    "\n",
    "**Method:** Differential Box Counting (DBC) for self-affine surfaces\n",
    "\n",
    "**Expected Results:**\n",
    "- D ∈ [2.0, 2.3]: Simple structure (plantation, young forest)\n",
    "- D ∈ [2.3, 2.5]: Moderate complexity (disturbed or temperate)\n",
    "- D ∈ [2.5, 2.8]: High complexity (old-growth, tropical multi-layer)\n",
    "- D > 2.8: Very high complexity (exceptional tropical canopy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def differential_box_counting(img, valid_mask, scales=None, min_valid_frac=0.8):\n",
    "    \"\"\"\n",
    "    Compute fractal dimension using Differential Box Counting (Sarkar & Chaudhuri 1994).\n",
    "    \n",
    "    For self-affine surfaces (like canopy height models):\n",
    "    - D = 2: Perfectly smooth planar surface\n",
    "    - D = 3: Maximally rough space-filling surface\n",
    "    \n",
    "    Args:\n",
    "        img: 2D height array\n",
    "        valid_mask: Boolean mask of valid pixels\n",
    "        scales: List of box sizes to use\n",
    "        min_valid_frac: Minimum fraction of valid pixels in box\n",
    "        \n",
    "    Returns:\n",
    "        D: Fractal dimension\n",
    "        r2: R-squared of fit\n",
    "        scales: Box sizes used\n",
    "        Ns: Box counts at each scale\n",
    "    \"\"\"\n",
    "    rows, cols = img.shape\n",
    "    M = min(rows, cols)\n",
    "\n",
    "    # Get height range from valid data\n",
    "    valid_values = img[valid_mask]\n",
    "    z_min_global, z_max_global = valid_values.min(), valid_values.max()\n",
    "    z_range_global = z_max_global - z_min_global\n",
    "\n",
    "    if z_range_global <= 0:\n",
    "        return np.nan, 0.0, np.array([]), np.array([])\n",
    "\n",
    "    # Default scales: powers of 2\n",
    "    if scales is None:\n",
    "        max_scale = M // 4\n",
    "        scales = [2**i for i in range(1, 12) if 2**i <= max_scale]\n",
    "\n",
    "    Ns_list = []\n",
    "    valid_scales = []\n",
    "\n",
    "    for s in scales:\n",
    "        nx = cols // s\n",
    "        ny = rows // s\n",
    "\n",
    "        if nx < 1 or ny < 1:\n",
    "            continue\n",
    "\n",
    "        N_total = 0\n",
    "        valid_boxes = 0\n",
    "\n",
    "        for i in range(ny):\n",
    "            for j in range(nx):\n",
    "                y_start, y_end = i * s, (i + 1) * s\n",
    "                x_start, x_end = j * s, (j + 1) * s\n",
    "\n",
    "                box_data = img[y_start:y_end, x_start:x_end]\n",
    "                box_valid = valid_mask[y_start:y_end, x_start:x_end]\n",
    "\n",
    "                valid_frac = box_valid.sum() / (s * s)\n",
    "                if valid_frac < min_valid_frac:\n",
    "                    continue\n",
    "\n",
    "                valid_boxes += 1\n",
    "                valid_heights = box_data[box_valid]\n",
    "                z_min_box = valid_heights.min()\n",
    "                z_max_box = valid_heights.max()\n",
    "\n",
    "                # Scale heights to grid\n",
    "                z_min_scaled = (z_min_box - z_min_global) / z_range_global * M\n",
    "                z_max_scaled = (z_max_box - z_min_global) / z_range_global * M\n",
    "\n",
    "                k = int(np.floor(z_min_scaled / s))\n",
    "                l = int(np.floor(z_max_scaled / s))\n",
    "                n = l - k + 1\n",
    "\n",
    "                N_total += n\n",
    "\n",
    "        if valid_boxes > 0:\n",
    "            Ns_list.append(N_total)\n",
    "            valid_scales.append(s)\n",
    "\n",
    "    if len(valid_scales) < 3:\n",
    "        return np.nan, 0.0, np.array([]), np.array([])\n",
    "\n",
    "    # Linear regression in log-log space\n",
    "    log_inv_s = np.log(1.0 / np.array(valid_scales))\n",
    "    log_Ns = np.log(np.array(Ns_list))\n",
    "\n",
    "    coeffs = np.polyfit(log_inv_s, log_Ns, 1)\n",
    "    D = coeffs[0]\n",
    "\n",
    "    # R-squared\n",
    "    predicted = np.polyval(coeffs, log_inv_s)\n",
    "    ss_res = np.sum((log_Ns - predicted) ** 2)\n",
    "    ss_tot = np.sum((log_Ns - np.mean(log_Ns)) ** 2)\n",
    "    r2 = 1 - (ss_res / ss_tot) if ss_tot > 0 else 0\n",
    "\n",
    "    return D, r2, np.array(valid_scales), np.array(Ns_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-h1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"HYPOTHESIS 1: OPTIMAL FILLING\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nRunning Differential Box Counting (DBC)...\")\n",
    "\n",
    "D_dbc, r2_dbc, scales_dbc, Ns_dbc = differential_box_counting(\n",
    "    chm_array, valid_mask, min_valid_frac=0.8\n",
    ")\n",
    "\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"  DBC Fractal Dimension: D = {D_dbc:.4f}\")\n",
    "print(f\"  R-squared: {r2_dbc:.4f}\")\n",
    "print(f\"  Scales used: {len(scales_dbc)} ({scales_dbc.min()}-{scales_dbc.max()} pixels)\")\n",
    "\n",
    "# Classification\n",
    "if np.isnan(D_dbc):\n",
    "    h1_class = \"INSUFFICIENT_DATA\"\n",
    "    h1_interp = \"Insufficient data for analysis\"\n",
    "elif D_dbc >= 2.7:\n",
    "    h1_class = \"EXCEPTIONAL_TROPICAL\"\n",
    "    h1_interp = f\"D = {D_dbc:.2f} indicates EXCEPTIONAL complexity (multi-layered tropical)\"\n",
    "elif D_dbc >= 2.5:\n",
    "    h1_class = \"HIGH_COMPLEXITY\"\n",
    "    h1_interp = f\"D = {D_dbc:.2f} indicates HIGH complexity (old-growth tropical)\"\n",
    "elif D_dbc >= 2.3:\n",
    "    h1_class = \"MODERATE_COMPLEXITY\"\n",
    "    h1_interp = f\"D = {D_dbc:.2f} indicates MODERATE complexity (typical forest)\"\n",
    "else:\n",
    "    h1_class = \"LOW_COMPLEXITY\"\n",
    "    h1_interp = f\"D = {D_dbc:.2f} indicates LOW complexity (simple structure)\"\n",
    "\n",
    "print(f\"\\nInterpretation: {h1_interp}\")\n",
    "\n",
    "# Store results\n",
    "h1_results = {\n",
    "    'hypothesis': 'H1: Optimal Filling',\n",
    "    'method': 'Differential Box Counting (DBC)',\n",
    "    'D_dbc': float(D_dbc) if not np.isnan(D_dbc) else None,\n",
    "    'r_squared': float(r2_dbc),\n",
    "    'n_scales': len(scales_dbc),\n",
    "    'classification': h1_class,\n",
    "    'interpretation': h1_interp\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-h1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot DBC results\n",
    "if len(scales_dbc) > 0:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Log-log plot\n",
    "    ax1 = axes[0]\n",
    "    log_inv_s = np.log(1.0 / scales_dbc)\n",
    "    log_Ns = np.log(Ns_dbc)\n",
    "    \n",
    "    ax1.scatter(log_inv_s, log_Ns, s=80, c='forestgreen', edgecolors='darkgreen', \n",
    "                linewidths=1.5, zorder=5, label='Data')\n",
    "    \n",
    "    # Fit line\n",
    "    coeffs = np.polyfit(log_inv_s, log_Ns, 1)\n",
    "    fit_line = np.polyval(coeffs, log_inv_s)\n",
    "    ax1.plot(log_inv_s, fit_line, 'r--', linewidth=2, \n",
    "             label=f'Fit: D = {D_dbc:.3f} (R² = {r2_dbc:.3f})')\n",
    "    \n",
    "    ax1.set_xlabel('log(1/s)', fontsize=12)\n",
    "    ax1.set_ylabel('log(N)', fontsize=12)\n",
    "    ax1.set_title('Differential Box Counting - BCI Tropical Forest', fontsize=14)\n",
    "    ax1.legend(loc='lower right')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # CHM visualization\n",
    "    ax2 = axes[1]\n",
    "    plot_data = np.ma.masked_where(~valid_mask, chm_array)\n",
    "    im = ax2.imshow(plot_data, cmap='Greens', vmin=0, vmax=np.percentile(valid_heights, 99))\n",
    "    ax2.set_title(f'CHM Subset - D = {D_dbc:.3f}', fontsize=14)\n",
    "    ax2.axis('off')\n",
    "    plt.colorbar(im, ax=ax2, label='Height (m)', shrink=0.8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save\n",
    "    fig_path = OUTPUT_DIR / f\"{SITE_NAME}_h1_dbc.png\"\n",
    "    plt.savefig(fig_path, dpi=150, bbox_inches='tight', facecolor='white')\n",
    "    print(f\"Saved: {fig_path.name}\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h2-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# HYPOTHESIS 2: Scale Invariance\n",
    "\n",
    "**Prediction:** Mature tropical forests show scale-invariant structure - gaps of all sizes following power-law decay.\n",
    "\n",
    "**Method:** Lacunarity analysis using Gliding Box Algorithm\n",
    "\n",
    "**Expected Results:**\n",
    "- R² > 0.95: Strong scale invariance (mature old-growth)\n",
    "- R² 0.7-0.95: Partial scale invariance (recovering forest)\n",
    "- R² < 0.7: Characteristic scales present (disturbed or managed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lacunarity-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gliding_box_lacunarity(img, valid_mask, box_sizes=None, min_valid_frac=0.5):\n",
    "    \"\"\"\n",
    "    Calculate lacunarity using the Gliding Box Algorithm.\n",
    "    \n",
    "    Lacunarity measures \"gappiness\" at different scales:\n",
    "    L(r) = (sigma^2 / mu^2) + 1\n",
    "    \n",
    "    Scale invariance: L(r) follows power law in log-log space\n",
    "    \n",
    "    Args:\n",
    "        img: 2D height array\n",
    "        valid_mask: Boolean mask of valid pixels\n",
    "        box_sizes: List of box sizes\n",
    "        min_valid_frac: Minimum fraction of valid pixels\n",
    "        \n",
    "    Returns:\n",
    "        lacunarity: Array of lacunarity values\n",
    "        sizes: Box sizes used\n",
    "        r2: R-squared of log-log fit\n",
    "        slope: Slope of log-log fit\n",
    "    \"\"\"\n",
    "    rows, cols = img.shape\n",
    "    \n",
    "    img_clean = img.copy()\n",
    "    img_clean[~valid_mask] = np.nan\n",
    "    \n",
    "    if box_sizes is None:\n",
    "        max_size = min(rows, cols) // 4\n",
    "        box_sizes = [2**i for i in range(2, 10) if 2**i <= max_size]\n",
    "    \n",
    "    lacunarity_values = []\n",
    "    valid_sizes = []\n",
    "    \n",
    "    for r in box_sizes:\n",
    "        box_sums = []\n",
    "        \n",
    "        # Gliding window with step size r/2\n",
    "        step = max(1, r // 2)\n",
    "        for i in range(0, rows - r + 1, step):\n",
    "            for j in range(0, cols - r + 1, step):\n",
    "                box = img_clean[i:i+r, j:j+r]\n",
    "                box_valid = valid_mask[i:i+r, j:j+r]\n",
    "                \n",
    "                valid_frac = box_valid.sum() / (r * r)\n",
    "                if valid_frac < min_valid_frac:\n",
    "                    continue\n",
    "                \n",
    "                box_sum = np.nansum(box)\n",
    "                box_sums.append(box_sum)\n",
    "        \n",
    "        if len(box_sums) < 10:\n",
    "            continue\n",
    "        \n",
    "        box_sums = np.array(box_sums)\n",
    "        mu = np.mean(box_sums)\n",
    "        sigma2 = np.var(box_sums)\n",
    "        \n",
    "        L = (sigma2 / (mu ** 2)) + 1 if mu > 0 else 1.0\n",
    "        \n",
    "        lacunarity_values.append(L)\n",
    "        valid_sizes.append(r)\n",
    "    \n",
    "    if len(valid_sizes) < 3:\n",
    "        return np.array([]), np.array([]), 0.0, 0.0\n",
    "    \n",
    "    # Fit power law in log-log space\n",
    "    log_r = np.log(np.array(valid_sizes))\n",
    "    log_L = np.log(np.array(lacunarity_values))\n",
    "    \n",
    "    coeffs = np.polyfit(log_r, log_L, 1)\n",
    "    slope = coeffs[0]\n",
    "    \n",
    "    predicted = np.polyval(coeffs, log_r)\n",
    "    ss_res = np.sum((log_L - predicted) ** 2)\n",
    "    ss_tot = np.sum((log_L - np.mean(log_L)) ** 2)\n",
    "    r2 = 1 - (ss_res / ss_tot) if ss_tot > 0 else 0\n",
    "    \n",
    "    return np.array(lacunarity_values), np.array(valid_sizes), r2, slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-h2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"HYPOTHESIS 2: SCALE INVARIANCE\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nRunning Lacunarity Analysis (Gliding Box)...\")\n",
    "\n",
    "lacunarity, lac_sizes, r2_lac, lac_slope = gliding_box_lacunarity(chm_array, valid_mask)\n",
    "\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"  Scales analyzed: {len(lac_sizes)}\")\n",
    "print(f\"  R² (log-log linearity): {r2_lac:.4f}\")\n",
    "print(f\"  Slope: {lac_slope:.4f}\")\n",
    "\n",
    "# Classification\n",
    "if r2_lac > 0.95:\n",
    "    h2_class = \"SCALE_INVARIANT\"\n",
    "    h2_interp = f\"R² = {r2_lac:.3f} indicates STRONG scale invariance (mature old-growth)\"\n",
    "elif r2_lac > 0.80:\n",
    "    h2_class = \"MODERATE_INVARIANCE\"\n",
    "    h2_interp = f\"R² = {r2_lac:.3f} indicates MODERATE scale invariance\"\n",
    "elif r2_lac > 0.60:\n",
    "    h2_class = \"PARTIAL_INVARIANCE\"\n",
    "    h2_interp = f\"R² = {r2_lac:.3f} indicates PARTIAL scale invariance (some characteristic scales)\"\n",
    "else:\n",
    "    h2_class = \"CHARACTERISTIC_SCALES\"\n",
    "    h2_interp = f\"R² = {r2_lac:.3f} indicates CHARACTERISTIC SCALES present\"\n",
    "\n",
    "print(f\"\\nInterpretation: {h2_interp}\")\n",
    "\n",
    "h2_results = {\n",
    "    'hypothesis': 'H2: Scale Invariance',\n",
    "    'method': 'Lacunarity (Gliding Box)',\n",
    "    'r_squared': float(r2_lac),\n",
    "    'slope': float(lac_slope),\n",
    "    'n_scales': len(lac_sizes),\n",
    "    'classification': h2_class,\n",
    "    'interpretation': h2_interp\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-h2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot lacunarity results\n",
    "if len(lac_sizes) > 0:\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    log_r = np.log(lac_sizes)\n",
    "    log_L = np.log(lacunarity)\n",
    "    \n",
    "    ax.scatter(log_r, log_L, s=100, c='forestgreen', edgecolors='darkgreen',\n",
    "               linewidths=1.5, zorder=5, label='Data')\n",
    "    \n",
    "    # Fit line\n",
    "    coeffs = np.polyfit(log_r, log_L, 1)\n",
    "    fit_line = np.polyval(coeffs, log_r)\n",
    "    ax.plot(log_r, fit_line, 'r--', linewidth=2,\n",
    "            label=f'Fit: slope = {lac_slope:.3f}, R² = {r2_lac:.3f}')\n",
    "    \n",
    "    ax.set_xlabel('log(Box Size) [pixels]', fontsize=12)\n",
    "    ax.set_ylabel('log(Lacunarity)', fontsize=12)\n",
    "    ax.set_title('Lacunarity Analysis - BCI Tropical Forest', fontsize=14)\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add scale bar (convert pixels to meters)\n",
    "    ax2 = ax.twiny()\n",
    "    ax2.set_xlim(ax.get_xlim())\n",
    "    meter_ticks = [4, 8, 16, 32, 64, 128, 256]\n",
    "    ax2.set_xticks([np.log(m / pixel_resolution) for m in meter_ticks if m / pixel_resolution <= lac_sizes.max()])\n",
    "    ax2.set_xticklabels([f'{m}m' for m in meter_ticks if m / pixel_resolution <= lac_sizes.max()])\n",
    "    ax2.set_xlabel('Box Size (meters)', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    fig_path = OUTPUT_DIR / f\"{SITE_NAME}_h2_lacunarity.png\"\n",
    "    plt.savefig(fig_path, dpi=150, bbox_inches='tight', facecolor='white')\n",
    "    print(f\"Saved: {fig_path.name}\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h3-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# HYPOTHESIS 3: Gap Size Distribution\n",
    "\n",
    "**Prediction:** Canopy gaps in mature tropical forests follow a power-law distribution, reflecting self-organized gap dynamics.\n",
    "\n",
    "**Method:** Gap size distribution fitting (power-law vs exponential)\n",
    "\n",
    "**Expected Results:**\n",
    "- Power-law (α ≈ 1.5-2.5): Natural gap dynamics (old-growth)\n",
    "- Exponential: Random/managed gap creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gap-functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_gaps(chm, valid_mask, gap_threshold=2.0, min_gap_pixels=4):\n",
    "    \"\"\"\n",
    "    Identify contiguous gap regions in the CHM.\n",
    "    \n",
    "    Args:\n",
    "        chm: 2D height array\n",
    "        valid_mask: Boolean mask of valid pixels\n",
    "        gap_threshold: Height below which pixels are considered gaps\n",
    "        min_gap_pixels: Minimum number of pixels to count as a gap\n",
    "        \n",
    "    Returns:\n",
    "        gap_areas: Array of gap sizes in pixels\n",
    "        labeled: Labeled gap array\n",
    "    \"\"\"\n",
    "    gap_mask = (chm < gap_threshold) & valid_mask\n",
    "    labeled, num_features = ndimage.label(gap_mask)\n",
    "    \n",
    "    if num_features == 0:\n",
    "        return np.array([]), labeled\n",
    "    \n",
    "    gap_areas = ndimage.sum(gap_mask, labeled, range(1, num_features + 1))\n",
    "    gap_areas = gap_areas[gap_areas >= min_gap_pixels]\n",
    "    \n",
    "    return gap_areas, labeled\n",
    "\n",
    "\n",
    "def fit_gap_distribution(gap_areas, resolution=1.0):\n",
    "    \"\"\"\n",
    "    Fit power law and exponential distributions to gap sizes.\n",
    "    \n",
    "    Args:\n",
    "        gap_areas: Gap sizes in pixels\n",
    "        resolution: Pixel resolution in meters\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with fit results\n",
    "    \"\"\"\n",
    "    # Convert to m²\n",
    "    areas = gap_areas * (resolution ** 2)\n",
    "    areas = areas[areas > 0]\n",
    "    \n",
    "    if len(areas) < 20:\n",
    "        return {'error': f'Insufficient gaps ({len(areas)}) for distribution fitting'}\n",
    "    \n",
    "    # Sort and compute CCDF\n",
    "    sorted_areas = np.sort(areas)\n",
    "    ecdf = np.arange(1, len(sorted_areas) + 1) / len(sorted_areas)\n",
    "    ccdf = 1 - ecdf\n",
    "    \n",
    "    # Power law fit (in log-log space)\n",
    "    try:\n",
    "        mask = ccdf > 0.01  # Avoid log(0)\n",
    "        log_x = np.log(sorted_areas[mask])\n",
    "        log_ccdf = np.log(ccdf[mask])\n",
    "        \n",
    "        pl_coeffs = np.polyfit(log_x, log_ccdf, 1)\n",
    "        alpha_pl = -pl_coeffs[0] + 1  # Power law exponent\n",
    "        \n",
    "        predicted_pl = np.polyval(pl_coeffs, log_x)\n",
    "        ss_res_pl = np.sum((log_ccdf - predicted_pl) ** 2)\n",
    "        ss_tot = np.sum((log_ccdf - np.mean(log_ccdf)) ** 2)\n",
    "        r2_pl = 1 - (ss_res_pl / ss_tot) if ss_tot > 0 else 0\n",
    "    except:\n",
    "        alpha_pl = np.nan\n",
    "        r2_pl = 0\n",
    "    \n",
    "    # Exponential fit\n",
    "    try:\n",
    "        x_vals = sorted_areas[mask]\n",
    "        log_ccdf_exp = np.log(ccdf[mask])\n",
    "        \n",
    "        exp_coeffs = np.polyfit(x_vals, log_ccdf_exp, 1)\n",
    "        beta_exp = -exp_coeffs[0]\n",
    "        \n",
    "        predicted_exp = np.polyval(exp_coeffs, x_vals)\n",
    "        ss_res_exp = np.sum((log_ccdf_exp - predicted_exp) ** 2)\n",
    "        r2_exp = 1 - (ss_res_exp / ss_tot) if ss_tot > 0 else 0\n",
    "    except:\n",
    "        beta_exp = np.nan\n",
    "        r2_exp = 0\n",
    "    \n",
    "    return {\n",
    "        'n_gaps': len(areas),\n",
    "        'min_area_m2': float(areas.min()),\n",
    "        'max_area_m2': float(areas.max()),\n",
    "        'median_area_m2': float(np.median(areas)),\n",
    "        'mean_area_m2': float(np.mean(areas)),\n",
    "        'power_law': {'alpha': float(alpha_pl), 'r2': float(r2_pl)},\n",
    "        'exponential': {'beta': float(beta_exp), 'r2': float(r2_exp)},\n",
    "        'sorted_areas': sorted_areas,\n",
    "        'ccdf': ccdf\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-h3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"HYPOTHESIS 3: GAP SIZE DISTRIBUTION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nIdentifying gaps (height < {GAP_THRESHOLD}m)...\")\n",
    "\n",
    "gap_areas, labeled_gaps = identify_gaps(chm_array, valid_mask, GAP_THRESHOLD)\n",
    "gap_results = fit_gap_distribution(gap_areas, pixel_resolution)\n",
    "\n",
    "if 'error' in gap_results:\n",
    "    print(f\"Error: {gap_results['error']}\")\n",
    "    h3_class = \"INSUFFICIENT_DATA\"\n",
    "    h3_interp = gap_results['error']\n",
    "else:\n",
    "    print(f\"\\nGap Statistics:\")\n",
    "    print(f\"  Number of gaps: {gap_results['n_gaps']}\")\n",
    "    print(f\"  Area range: {gap_results['min_area_m2']:.1f} - {gap_results['max_area_m2']:.1f} m²\")\n",
    "    print(f\"  Median area: {gap_results['median_area_m2']:.1f} m²\")\n",
    "    \n",
    "    print(f\"\\nPower Law Fit:\")\n",
    "    print(f\"  α (exponent): {gap_results['power_law']['alpha']:.3f}\")\n",
    "    print(f\"  R²: {gap_results['power_law']['r2']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nExponential Fit:\")\n",
    "    print(f\"  R²: {gap_results['exponential']['r2']:.4f}\")\n",
    "    \n",
    "    # Classification\n",
    "    pl_r2 = gap_results['power_law']['r2']\n",
    "    exp_r2 = gap_results['exponential']['r2']\n",
    "    alpha = gap_results['power_law']['alpha']\n",
    "    \n",
    "    if pl_r2 > exp_r2 and pl_r2 > 0.7:\n",
    "        h3_class = \"POWER_LAW\"\n",
    "        h3_interp = f\"Power law (α = {alpha:.2f}, R² = {pl_r2:.2f}) - natural gap dynamics\"\n",
    "    elif exp_r2 > pl_r2 and exp_r2 > 0.7:\n",
    "        h3_class = \"EXPONENTIAL\"\n",
    "        h3_interp = f\"Exponential (R² = {exp_r2:.2f}) - random/managed gap creation\"\n",
    "    else:\n",
    "        h3_class = \"MIXED\"\n",
    "        h3_interp = f\"Mixed distribution (PL R²={pl_r2:.2f}, Exp R²={exp_r2:.2f})\"\n",
    "    \n",
    "    print(f\"\\nInterpretation: {h3_interp}\")\n",
    "\n",
    "h3_results = {\n",
    "    'hypothesis': 'H3: Gap Size Distribution',\n",
    "    'method': 'Power Law vs Exponential Fit',\n",
    "    'n_gaps': gap_results.get('n_gaps', 0),\n",
    "    'power_law_alpha': gap_results.get('power_law', {}).get('alpha'),\n",
    "    'power_law_r2': gap_results.get('power_law', {}).get('r2'),\n",
    "    'exponential_r2': gap_results.get('exponential', {}).get('r2'),\n",
    "    'classification': h3_class,\n",
    "    'interpretation': h3_interp\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-h3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot gap distribution\n",
    "if 'sorted_areas' in gap_results:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    sorted_areas = gap_results['sorted_areas']\n",
    "    ccdf = gap_results['ccdf']\n",
    "    \n",
    "    # Log-log plot (power law test)\n",
    "    ax1 = axes[0]\n",
    "    mask = ccdf > 0.01\n",
    "    ax1.scatter(sorted_areas[mask], ccdf[mask], s=30, alpha=0.6, c='forestgreen', label='Data')\n",
    "    ax1.set_xscale('log')\n",
    "    ax1.set_yscale('log')\n",
    "    ax1.set_xlabel('Gap Area (m²)', fontsize=12)\n",
    "    ax1.set_ylabel('P(X > x)', fontsize=12)\n",
    "    ax1.set_title(f'Gap Size CCDF (Power Law α = {gap_results[\"power_law\"][\"alpha\"]:.2f})', fontsize=14)\n",
    "    ax1.grid(True, alpha=0.3, which='both')\n",
    "    \n",
    "    # Histogram\n",
    "    ax2 = axes[1]\n",
    "    ax2.hist(sorted_areas, bins=50, color='forestgreen', alpha=0.7, edgecolor='darkgreen')\n",
    "    ax2.set_xlabel('Gap Area (m²)', fontsize=12)\n",
    "    ax2.set_ylabel('Count', fontsize=12)\n",
    "    ax2.set_title(f'Gap Size Distribution (n = {gap_results[\"n_gaps\"]})', fontsize=14)\n",
    "    ax2.axvline(gap_results['median_area_m2'], color='red', linestyle='--', \n",
    "                label=f'Median: {gap_results[\"median_area_m2\"]:.1f} m²')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    fig_path = OUTPUT_DIR / f\"{SITE_NAME}_h3_gap_distribution.png\"\n",
    "    plt.savefig(fig_path, dpi=150, bbox_inches='tight', facecolor='white')\n",
    "    print(f\"Saved: {fig_path.name}\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h4-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# HYPOTHESIS 4: Emergent Tree Spacing\n",
    "\n",
    "**Prediction:** Emergent trees in tropical forests show competition-driven spacing patterns.\n",
    "\n",
    "**Method:** Nearest neighbor distance analysis for emergent tree tops\n",
    "\n",
    "**Expected Results:**\n",
    "- Regular spacing: Competition-structured (old-growth)\n",
    "- Random (Poisson): Young or disturbed forest\n",
    "- Clustered: Regeneration patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tree-detection",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_tree_tops(chm, valid_mask, min_height=15.0, window_size=21):\n",
    "    \"\"\"\n",
    "    Detect tree tops as local maxima in the CHM.\n",
    "    \n",
    "    Args:\n",
    "        chm: 2D height array\n",
    "        valid_mask: Boolean mask of valid pixels\n",
    "        min_height: Minimum height for tree detection (meters)\n",
    "        window_size: Window size for local maximum detection (pixels)\n",
    "        \n",
    "    Returns:\n",
    "        positions: Array of (row, col) positions\n",
    "        heights: Array of heights at each position\n",
    "    \"\"\"\n",
    "    # Apply minimum height threshold\n",
    "    chm_filtered = np.where(valid_mask & (chm >= min_height), chm, -np.inf)\n",
    "    \n",
    "    # Find local maxima\n",
    "    local_max = ndimage.maximum_filter(chm_filtered, size=window_size)\n",
    "    is_peak = (chm_filtered == local_max) & (chm_filtered > -np.inf)\n",
    "    \n",
    "    # Get positions\n",
    "    rows, cols = np.where(is_peak)\n",
    "    heights = chm[rows, cols]\n",
    "    \n",
    "    return np.column_stack([rows, cols]), heights\n",
    "\n",
    "\n",
    "def nearest_neighbor_analysis(positions, pixel_resolution=1.0):\n",
    "    \"\"\"\n",
    "    Analyze nearest neighbor distances for tree positions.\n",
    "    \n",
    "    Args:\n",
    "        positions: Array of (row, col) positions\n",
    "        pixel_resolution: Pixel size in meters\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with NN statistics\n",
    "    \"\"\"\n",
    "    if len(positions) < 10:\n",
    "        return {'error': f'Insufficient trees ({len(positions)}) for NN analysis'}\n",
    "    \n",
    "    # Build KD-tree\n",
    "    tree = cKDTree(positions * pixel_resolution)\n",
    "    \n",
    "    # Query nearest neighbor for each point\n",
    "    distances, _ = tree.query(positions * pixel_resolution, k=2)  # k=2 because first is self\n",
    "    nn_distances = distances[:, 1]  # Second column is nearest neighbor\n",
    "    \n",
    "    # Calculate statistics\n",
    "    mean_nn = np.mean(nn_distances)\n",
    "    std_nn = np.std(nn_distances)\n",
    "    \n",
    "    # Clark-Evans R ratio (R = observed mean / expected random mean)\n",
    "    # For random Poisson: E[r] = 1 / (2 * sqrt(density))\n",
    "    area = (positions[:, 0].max() - positions[:, 0].min()) * \\\n",
    "           (positions[:, 1].max() - positions[:, 1].min()) * (pixel_resolution ** 2)\n",
    "    density = len(positions) / area if area > 0 else 0\n",
    "    expected_random = 0.5 / np.sqrt(density) if density > 0 else np.inf\n",
    "    \n",
    "    R = mean_nn / expected_random if expected_random > 0 else np.nan\n",
    "    \n",
    "    return {\n",
    "        'n_trees': len(positions),\n",
    "        'density_per_ha': density * 10000,\n",
    "        'mean_nn_m': float(mean_nn),\n",
    "        'std_nn_m': float(std_nn),\n",
    "        'cv_nn': float(std_nn / mean_nn) if mean_nn > 0 else np.nan,\n",
    "        'clark_evans_R': float(R),\n",
    "        'nn_distances': nn_distances\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-h4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"HYPOTHESIS 4: EMERGENT TREE SPACING\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nDetecting emergent trees (height >= {EMERGENT_THRESHOLD}m)...\")\n",
    "\n",
    "tree_positions, tree_heights = detect_tree_tops(\n",
    "    chm_array, valid_mask, \n",
    "    min_height=EMERGENT_THRESHOLD, \n",
    "    window_size=LOCAL_MAX_WINDOW\n",
    ")\n",
    "\n",
    "print(f\"Detected {len(tree_positions)} emergent tree tops\")\n",
    "\n",
    "nn_results = nearest_neighbor_analysis(tree_positions, pixel_resolution)\n",
    "\n",
    "if 'error' in nn_results:\n",
    "    print(f\"Error: {nn_results['error']}\")\n",
    "    h4_class = \"INSUFFICIENT_DATA\"\n",
    "    h4_interp = nn_results['error']\n",
    "else:\n",
    "    print(f\"\\nEmergent Tree Statistics:\")\n",
    "    print(f\"  Count: {nn_results['n_trees']}\")\n",
    "    print(f\"  Density: {nn_results['density_per_ha']:.1f} trees/ha\")\n",
    "    print(f\"  Mean NN distance: {nn_results['mean_nn_m']:.1f} m\")\n",
    "    print(f\"  Std NN distance: {nn_results['std_nn_m']:.1f} m\")\n",
    "    print(f\"  CV: {nn_results['cv_nn']:.3f}\")\n",
    "    print(f\"\\nClark-Evans R: {nn_results['clark_evans_R']:.3f}\")\n",
    "    \n",
    "    # Interpret R\n",
    "    # R < 1: Clustered\n",
    "    # R = 1: Random (Poisson)\n",
    "    # R > 1: Regular/dispersed\n",
    "    R = nn_results['clark_evans_R']\n",
    "    \n",
    "    if R > 1.2:\n",
    "        h4_class = \"REGULAR\"\n",
    "        h4_interp = f\"R = {R:.2f} indicates REGULAR spacing (competition-structured)\"\n",
    "    elif R > 0.8:\n",
    "        h4_class = \"RANDOM\"\n",
    "        h4_interp = f\"R = {R:.2f} indicates RANDOM spacing (Poisson-like)\"\n",
    "    else:\n",
    "        h4_class = \"CLUSTERED\"\n",
    "        h4_interp = f\"R = {R:.2f} indicates CLUSTERED spacing (aggregated)\"\n",
    "    \n",
    "    print(f\"\\nInterpretation: {h4_interp}\")\n",
    "\n",
    "h4_results = {\n",
    "    'hypothesis': 'H4: Emergent Tree Spacing',\n",
    "    'method': 'Nearest Neighbor Analysis',\n",
    "    'n_trees': nn_results.get('n_trees', 0),\n",
    "    'density_per_ha': nn_results.get('density_per_ha'),\n",
    "    'mean_nn_m': nn_results.get('mean_nn_m'),\n",
    "    'clark_evans_R': nn_results.get('clark_evans_R'),\n",
    "    'classification': h4_class,\n",
    "    'interpretation': h4_interp\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-h4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot tree detection and NN distribution\n",
    "if len(tree_positions) > 10:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # Tree positions overlaid on CHM\n",
    "    ax1 = axes[0]\n",
    "    plot_data = np.ma.masked_where(~valid_mask, chm_array)\n",
    "    im = ax1.imshow(plot_data, cmap='Greens', vmin=0, vmax=np.percentile(valid_heights, 99))\n",
    "    ax1.scatter(tree_positions[:, 1], tree_positions[:, 0], \n",
    "                c='red', s=20, marker='^', label=f'Emergent trees (n={len(tree_positions)})')\n",
    "    ax1.set_title(f'Emergent Tree Detection (>= {EMERGENT_THRESHOLD}m)', fontsize=14)\n",
    "    ax1.legend(loc='upper right')\n",
    "    ax1.axis('off')\n",
    "    plt.colorbar(im, ax=ax1, label='Height (m)', shrink=0.8)\n",
    "    \n",
    "    # NN distance distribution\n",
    "    ax2 = axes[1]\n",
    "    nn_dist = nn_results['nn_distances']\n",
    "    ax2.hist(nn_dist, bins=30, color='forestgreen', alpha=0.7, edgecolor='darkgreen', density=True)\n",
    "    ax2.axvline(nn_results['mean_nn_m'], color='red', linestyle='--', linewidth=2,\n",
    "                label=f'Mean: {nn_results[\"mean_nn_m\"]:.1f} m')\n",
    "    \n",
    "    # Expected Poisson distribution\n",
    "    x_range = np.linspace(0, nn_dist.max(), 100)\n",
    "    lambda_param = 2 * np.sqrt(nn_results['density_per_ha'] / 10000 * np.pi)\n",
    "    poisson_pdf = lambda_param * np.exp(-lambda_param * x_range)\n",
    "    ax2.plot(x_range, poisson_pdf, 'b-', linewidth=2, label='Expected Poisson')\n",
    "    \n",
    "    ax2.set_xlabel('Nearest Neighbor Distance (m)', fontsize=12)\n",
    "    ax2.set_ylabel('Density', fontsize=12)\n",
    "    ax2.set_title(f'NN Distance Distribution (R = {nn_results[\"clark_evans_R\"]:.2f})', fontsize=14)\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    fig_path = OUTPUT_DIR / f\"{SITE_NAME}_h4_tree_spacing.png\"\n",
    "    plt.savefig(fig_path, dpi=150, bbox_inches='tight', facecolor='white')\n",
    "    print(f\"Saved: {fig_path.name}\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Summary and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile all results\n",
    "all_results = {\n",
    "    'site': {\n",
    "        'name': SITE_NAME,\n",
    "        'description': SITE_DESCRIPTION,\n",
    "        'subset_used': USE_SUBSET,\n",
    "        'subset_size_pixels': SUBSET_SIZE if USE_SUBSET else None,\n",
    "        'resolution_m': float(pixel_resolution),\n",
    "        'analysis_date': datetime.now().isoformat()\n",
    "    },\n",
    "    'height_stats': {\n",
    "        'mean_m': float(np.mean(valid_heights)),\n",
    "        'median_m': float(np.median(valid_heights)),\n",
    "        'max_m': float(np.max(valid_heights)),\n",
    "        'std_m': float(np.std(valid_heights)),\n",
    "        'p95_m': float(np.percentile(valid_heights, 95)),\n",
    "        'p99_m': float(np.percentile(valid_heights, 99))\n",
    "    },\n",
    "    'hypotheses': {\n",
    "        'H1_optimal_filling': h1_results,\n",
    "        'H2_scale_invariance': h2_results,\n",
    "        'H3_gap_distribution': h3_results,\n",
    "        'H4_tree_spacing': h4_results\n",
    "    }\n",
    "}\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FRACTAL ANALYSIS SUMMARY - BARRO COLORADO ISLAND\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nSite: {SITE_DESCRIPTION}\")\n",
    "print(f\"Resolution: {pixel_resolution}m\")\n",
    "print(f\"Mean canopy height: {np.mean(valid_heights):.1f} m\")\n",
    "print(f\"Max canopy height: {np.max(valid_heights):.1f} m\")\n",
    "\n",
    "print(f\"\\n\" + \"-\"*70)\n",
    "print(\"HYPOTHESIS RESULTS\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "hypotheses = [\n",
    "    ('H1', 'Optimal Filling (DBC)', h1_results),\n",
    "    ('H2', 'Scale Invariance (Lacunarity)', h2_results),\n",
    "    ('H3', 'Gap Distribution', h3_results),\n",
    "    ('H4', 'Tree Spacing', h4_results)\n",
    "]\n",
    "\n",
    "for code, name, results in hypotheses:\n",
    "    print(f\"\\n{code}: {name}\")\n",
    "    print(f\"   Classification: {results.get('classification', 'N/A')}\")\n",
    "    print(f\"   {results.get('interpretation', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with expected temperate forest values\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TROPICAL VS TEMPERATE COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Reference values from 3DEP temperate forest analyses\n",
    "temperate_reference = {\n",
    "    'D_dbc': 2.3,  # Typical temperate forest\n",
    "    'lacunarity_r2': 0.85,\n",
    "    'gap_alpha': 1.8,\n",
    "    'description': 'Typical US temperate forest (3DEP sites)'\n",
    "}\n",
    "\n",
    "print(f\"\\n{'Metric':<30} {'BCI Tropical':<20} {'Temperate Ref':<20}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "if h1_results.get('D_dbc'):\n",
    "    d_diff = h1_results['D_dbc'] - temperate_reference['D_dbc']\n",
    "    print(f\"{'Fractal Dimension (D)':<30} {h1_results['D_dbc']:<20.3f} {temperate_reference['D_dbc']:<20.3f}\")\n",
    "    if d_diff > 0.1:\n",
    "        print(f\"   -> BCI shows HIGHER complexity (D +{d_diff:.2f})\")\n",
    "    elif d_diff < -0.1:\n",
    "        print(f\"   -> BCI shows LOWER complexity (D {d_diff:.2f})\")\n",
    "    else:\n",
    "        print(f\"   -> Similar complexity to temperate\")\n",
    "\n",
    "print(f\"{'Lacunarity R²':<30} {h2_results.get('r_squared', 'N/A'):<20.3f} {temperate_reference['lacunarity_r2']:<20.3f}\")\n",
    "\n",
    "if h3_results.get('power_law_alpha'):\n",
    "    print(f\"{'Gap Power Law α':<30} {h3_results['power_law_alpha']:<20.3f} {temperate_reference['gap_alpha']:<20.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to JSON\n",
    "results_path = OUTPUT_DIR / f\"{SITE_NAME}_fractal_results.json\"\n",
    "\n",
    "# Remove numpy arrays (not JSON serializable)\n",
    "def clean_for_json(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: clean_for_json(v) for k, v in obj.items() \n",
    "                if not isinstance(v, np.ndarray)}\n",
    "    elif isinstance(obj, (np.floating, np.integer)):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return None\n",
    "    return obj\n",
    "\n",
    "clean_results = clean_for_json(all_results)\n",
    "\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(clean_results, f, indent=2)\n",
    "\n",
    "print(f\"\\nResults saved to: {results_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ANALYSIS COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nOutput files in: {OUTPUT_DIR}\")\n",
    "for f in sorted(OUTPUT_DIR.glob(f\"{SITE_NAME}*\")):\n",
    "    print(f\"  - {f.name}\")\n",
    "\n",
    "print(f\"\\nKey Findings for BCI Tropical Forest:\")\n",
    "if h1_results.get('D_dbc'):\n",
    "    print(f\"  - Fractal Dimension: D = {h1_results['D_dbc']:.3f} ({h1_results['classification']})\")\n",
    "print(f\"  - Scale Invariance: R² = {h2_results.get('r_squared', 0):.3f} ({h2_results['classification']})\")\n",
    "print(f\"  - Gap Distribution: {h3_results['classification']}\")\n",
    "print(f\"  - Tree Spacing: {h4_results['classification']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleanup",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleanup-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free memory\n",
    "chm_full.close()\n",
    "if USE_SUBSET:\n",
    "    chm.close()\n",
    "\n",
    "del chm_array, valid_mask, valid_heights\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "print(\"Memory cleaned up.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "references",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "1. **Sarkar & Chaudhuri (1994)** - Differential Box Counting method for fractal dimension\n",
    "2. **Plotnick et al. (1996)** - Lacunarity analysis of spatial patterns\n",
    "3. **Clark & Evans (1954)** - Nearest neighbor analysis for spatial patterns\n",
    "4. **Smithsonian ALS Panama 2023** - https://smithsonian.dataone.org/datasets/ALS_Panama_2023/\n",
    "5. **BCI Forest Dynamics Plot** - https://forestgeo.si.edu/sites/neotropics/barro-colorado-island\n",
    "6. **3DEP Fractal Analysis Framework** - Adapted from fractal-notebooks/docs/notebooks/3dep/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (3DEP)",
   "language": "python",
   "name": "3dep"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
