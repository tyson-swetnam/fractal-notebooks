{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# CHM Fractal Analysis: Complete Hypothesis Testing\n",
    "\n",
    "**Site:** Barro Colorado Island, Panama  \n",
    "**Forest Type:** Tropical Moist Forest (Neotropical Lowland Rainforest)  \n",
    "**Data Source:** [Smithsonian ALS Panama 2023](https://smithsonian.dataone.org/datasets/ALS_Panama_2023/)\n",
    "\n",
    "This notebook performs comprehensive fractal dimension analysis on Canopy Height Models (CHM) to test **all nine research hypotheses** from the Fractal Self-Affinity in Nature framework.\n",
    "\n",
    "## Five Primary Testable Hypotheses\n",
    "\n",
    "1. **Optimal Filling** - Old-growth forests maximize light interception, producing higher fractal dimensions\n",
    "2. **Scale Invariance** - Steady-state forests show scale-invariant gap distributions (power-law decay)\n",
    "3. **Zeta Distribution** - Canopy gaps follow power-law with exponent α ≈ 2.0, related to ζ(2)\n",
    "4. **Universal Repulsion** - Dominant tree spacing follows Wigner-Dyson distribution (GUE statistics)\n",
    "5. **Biotic Decoupling** - Established ecosystems show weak correlation with topographic variables\n",
    "\n",
    "## Four Additional Spatial Distribution Hypotheses\n",
    "\n",
    "6. **Fractal String Gap** - Gap sizes follow fractal string spectrum predictions\n",
    "7. **Prime Number Repulsion (GUE)** - Large tree spacing shows prime-like repulsion patterns\n",
    "8. **Complex Dimension Oscillation** - Log-periodic oscillations in canopy structure\n",
    "9. **Riemann Gas Density** - Tree density follows Riemann gas statistical mechanics\n",
    "\n",
    "## Tropical Forest Expectations\n",
    "\n",
    "Based on BCI's multi-layered structure and high biodiversity:\n",
    "- **Higher fractal dimension** (D > 2.5) due to 3-4 canopy layers\n",
    "- **Strong scale invariance** (mature, undisturbed forest)\n",
    "- **Power-law gap distribution** (natural disturbance dynamics)\n",
    "- **Biotic decoupling** from topography (self-organized structure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment ready at 2025-12-25 19:14:31\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import rioxarray as rxr\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import json\n",
    "from scipy import ndimage\n",
    "from scipy.stats import ks_2samp, pearsonr, spearmanr\n",
    "from scipy.spatial import cKDTree\n",
    "from scipy.special import zeta\n",
    "from rasterio.enums import Resampling\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"Environment ready at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-header",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "config",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input files:\n",
      "  [OK] CHM: BCI_whole_2023_05_26_chm.tif (1.14 GB)\n",
      "  [OK] DTM: BCI_whole_2023_05_26_dtm.tif (1.14 GB)\n",
      "\n",
      "Output: /home/jovyan/data-store/data/output/smithsonian/analysis/fractal_hypotheses\n"
     ]
    }
   ],
   "source": [
    "# Site configuration\n",
    "SITE_NAME = \"bci_panama\"\n",
    "SITE_DESCRIPTION = \"Barro Colorado Island - Tropical Moist Forest\"\n",
    "\n",
    "# Data paths\n",
    "DATA_BASE = Path.home() / \"data-store/data/output/smithsonian\"\n",
    "RAW_DATA = DATA_BASE / \"raw\"\n",
    "\n",
    "# Input files\n",
    "CHM_PATH = RAW_DATA / \"chm\" / \"BCI_whole_2023_05_26_chm.tif\"\n",
    "DTM_PATH = RAW_DATA / \"dtm\" / \"BCI_whole_2023_05_26_dtm.tif\"\n",
    "\n",
    "# Output directory\n",
    "OUTPUT_DIR = DATA_BASE / \"analysis\" / \"fractal_hypotheses\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Analysis parameters\n",
    "GAP_THRESHOLD = 2.0  # meters - pixels below this are gaps\n",
    "DOMINANT_TREE_THRESHOLD = 35.0  # meters - emergent tree detection (higher for tropical)\n",
    "LOCAL_MAX_WINDOW = 21  # pixels - for tree top detection\n",
    "NODATA_VALUE = -9999\n",
    "\n",
    "# Verify files exist\n",
    "print(\"Input files:\")\n",
    "for name, path in [(\"CHM\", CHM_PATH), (\"DTM\", DTM_PATH)]:\n",
    "    exists = path.exists()\n",
    "    size = f\"{path.stat().st_size / 1e9:.2f} GB\" if exists else \"NOT FOUND\"\n",
    "    print(f\"  [{('OK' if exists else 'MISSING')}] {name}: {path.name} ({size})\")\n",
    "\n",
    "print(f\"\\nOutput: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-header",
   "metadata": {},
   "source": [
    "## 3. Load and Subset CHM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "load-chm",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CHM from: BCI_whole_2023_05_26_chm.tif\n",
      "This may take a moment for the 1.1 GB file...\n",
      "\n",
      "Full CHM loaded:\n",
      "  Shape: (1, 11000, 13000)\n",
      "  CRS: EPSG:32617\n",
      "  Resolution: 0.50m\n"
     ]
    }
   ],
   "source": [
    "# Load CHM\n",
    "print(f\"Loading CHM from: {CHM_PATH.name}\")\n",
    "print(\"This may take a moment for the 1.1 GB file...\")\n",
    "\n",
    "chm_full = rxr.open_rasterio(CHM_PATH, masked=True)\n",
    "\n",
    "print(f\"\\nFull CHM loaded:\")\n",
    "print(f\"  Shape: {chm_full.shape}\")\n",
    "print(f\"  CRS: {chm_full.rio.crs}\")\n",
    "print(f\"  Resolution: {abs(chm_full.rio.resolution()[0]):.2f}m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "subset-chm",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using center subset: 2000x2000 pixels\n",
      "  = 1.0 km x 1.0 km\n",
      "\n",
      "Analysis array:\n",
      "  Shape: (2000, 2000)\n",
      "  Valid pixels: 3,999,996 (100.0%)\n",
      "  Resolution: 0.500m\n"
     ]
    }
   ],
   "source": [
    "# Use subset for analysis (full island would take too long)\n",
    "USE_SUBSET = True\n",
    "SUBSET_SIZE = 2000  # pixels (1km x 1km at 0.5m resolution)\n",
    "\n",
    "if USE_SUBSET:\n",
    "    # Get center region of the island\n",
    "    full_shape = chm_full.shape\n",
    "    center_y = full_shape[1] // 2\n",
    "    center_x = full_shape[2] // 2\n",
    "    half_size = SUBSET_SIZE // 2\n",
    "    \n",
    "    chm = chm_full[:, \n",
    "                   center_y - half_size : center_y + half_size,\n",
    "                   center_x - half_size : center_x + half_size]\n",
    "    \n",
    "    print(f\"Using center subset: {SUBSET_SIZE}x{SUBSET_SIZE} pixels\")\n",
    "    print(f\"  = {SUBSET_SIZE * 0.5 / 1000:.1f} km x {SUBSET_SIZE * 0.5 / 1000:.1f} km\")\n",
    "else:\n",
    "    chm = chm_full\n",
    "    print(\"Using full CHM\")\n",
    "\n",
    "# Get the data array\n",
    "chm_array = chm.values.squeeze().astype(np.float64)\n",
    "\n",
    "# Get NoData value\n",
    "if chm.rio.nodata is not None:\n",
    "    NODATA_VALUE = chm.rio.nodata\n",
    "\n",
    "# Create masks\n",
    "nodata_mask = np.isnan(chm_array) | (chm_array == NODATA_VALUE)\n",
    "valid_mask = ~nodata_mask\n",
    "\n",
    "# Get pixel resolution\n",
    "pixel_resolution = abs(chm.rio.resolution()[0])\n",
    "\n",
    "print(f\"\\nAnalysis array:\")\n",
    "print(f\"  Shape: {chm_array.shape}\")\n",
    "print(f\"  Valid pixels: {valid_mask.sum():,} ({100*valid_mask.sum()/chm_array.size:.1f}%)\")\n",
    "print(f\"  Resolution: {pixel_resolution:.3f}m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "basic-stats",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Height Statistics:\n",
      "  Min:    0.01 m\n",
      "  Max:    54.88 m\n",
      "  Mean:   24.18 m\n",
      "  Median: 24.26 m\n",
      "  Std:    8.63 m\n",
      "  P95:    38.51 m\n",
      "  P99:    43.38 m\n"
     ]
    }
   ],
   "source": [
    "# Basic height statistics\n",
    "valid_heights = chm_array[valid_mask]\n",
    "valid_heights = valid_heights[(valid_heights >= 0) & (valid_heights <= 80)]\n",
    "\n",
    "print(\"Height Statistics:\")\n",
    "print(f\"  Min:    {np.min(valid_heights):.2f} m\")\n",
    "print(f\"  Max:    {np.max(valid_heights):.2f} m\")\n",
    "print(f\"  Mean:   {np.mean(valid_heights):.2f} m\")\n",
    "print(f\"  Median: {np.median(valid_heights):.2f} m\")\n",
    "print(f\"  Std:    {np.std(valid_heights):.2f} m\")\n",
    "print(f\"  P95:    {np.percentile(valid_heights, 95):.2f} m\")\n",
    "print(f\"  P99:    {np.percentile(valid_heights, 99):.2f} m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functions-header",
   "metadata": {},
   "source": [
    "## 4. Analysis Functions\n",
    "\n",
    "Core functions for all 9 hypotheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbc-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def differential_box_counting(img, valid_mask, scales=None, min_valid_frac=0.8):\n",
    "    \"\"\"Differential Box Counting for self-affine surfaces (H1).\"\"\"\n",
    "    rows, cols = img.shape\n",
    "    M = min(rows, cols)\n",
    "\n",
    "    valid_values = img[valid_mask]\n",
    "    z_min_global, z_max_global = valid_values.min(), valid_values.max()\n",
    "    z_range_global = z_max_global - z_min_global\n",
    "\n",
    "    if z_range_global <= 0:\n",
    "        return np.nan, 0.0, np.array([]), np.array([])\n",
    "\n",
    "    if scales is None:\n",
    "        max_scale = M // 4\n",
    "        scales = [2**i for i in range(1, 12) if 2**i <= max_scale]\n",
    "\n",
    "    Ns_list = []\n",
    "    valid_scales = []\n",
    "\n",
    "    for s in scales:\n",
    "        nx, ny = cols // s, rows // s\n",
    "        if nx < 1 or ny < 1:\n",
    "            continue\n",
    "\n",
    "        N_total = 0\n",
    "        valid_boxes = 0\n",
    "\n",
    "        for i in range(ny):\n",
    "            for j in range(nx):\n",
    "                y_start, y_end = i * s, (i + 1) * s\n",
    "                x_start, x_end = j * s, (j + 1) * s\n",
    "\n",
    "                box_data = img[y_start:y_end, x_start:x_end]\n",
    "                box_valid = valid_mask[y_start:y_end, x_start:x_end]\n",
    "\n",
    "                if box_valid.sum() / (s * s) < min_valid_frac:\n",
    "                    continue\n",
    "\n",
    "                valid_boxes += 1\n",
    "                valid_heights = box_data[box_valid]\n",
    "                z_min_box, z_max_box = valid_heights.min(), valid_heights.max()\n",
    "\n",
    "                z_min_scaled = (z_min_box - z_min_global) / z_range_global * M\n",
    "                z_max_scaled = (z_max_box - z_min_global) / z_range_global * M\n",
    "\n",
    "                k = int(np.floor(z_min_scaled / s))\n",
    "                l = int(np.floor(z_max_scaled / s))\n",
    "                N_total += l - k + 1\n",
    "\n",
    "        if valid_boxes > 0:\n",
    "            Ns_list.append(N_total)\n",
    "            valid_scales.append(s)\n",
    "\n",
    "    if len(valid_scales) < 3:\n",
    "        return np.nan, 0.0, np.array([]), np.array([])\n",
    "\n",
    "    log_inv_s = np.log(1.0 / np.array(valid_scales))\n",
    "    log_Ns = np.log(np.array(Ns_list))\n",
    "\n",
    "    coeffs = np.polyfit(log_inv_s, log_Ns, 1)\n",
    "    D = coeffs[0]\n",
    "\n",
    "    predicted = np.polyval(coeffs, log_inv_s)\n",
    "    ss_res = np.sum((log_Ns - predicted) ** 2)\n",
    "    ss_tot = np.sum((log_Ns - np.mean(log_Ns)) ** 2)\n",
    "    r2 = 1 - (ss_res / ss_tot) if ss_tot > 0 else 0\n",
    "\n",
    "    return D, r2, np.array(valid_scales), np.array(Ns_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "lacunarity-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gliding_box_lacunarity(img, valid_mask, box_sizes=None, min_valid_frac=0.5):\n",
    "    \"\"\"Lacunarity analysis for scale invariance (H2).\"\"\"\n",
    "    rows, cols = img.shape\n",
    "    img_clean = img.copy()\n",
    "    img_clean[~valid_mask] = np.nan\n",
    "    \n",
    "    if box_sizes is None:\n",
    "        max_size = min(rows, cols) // 4\n",
    "        box_sizes = [2**i for i in range(2, 10) if 2**i <= max_size]\n",
    "    \n",
    "    lacunarity_values = []\n",
    "    valid_sizes = []\n",
    "    \n",
    "    for r in box_sizes:\n",
    "        box_sums = []\n",
    "        step = max(1, r // 2)\n",
    "        \n",
    "        for i in range(0, rows - r + 1, step):\n",
    "            for j in range(0, cols - r + 1, step):\n",
    "                box = img_clean[i:i+r, j:j+r]\n",
    "                box_valid = valid_mask[i:i+r, j:j+r]\n",
    "                \n",
    "                if box_valid.sum() / (r * r) >= min_valid_frac:\n",
    "                    box_sums.append(np.nansum(box))\n",
    "        \n",
    "        if len(box_sums) >= 10:\n",
    "            box_sums = np.array(box_sums)\n",
    "            mu = np.mean(box_sums)\n",
    "            L = (np.var(box_sums) / (mu ** 2)) + 1 if mu > 0 else 1.0\n",
    "            lacunarity_values.append(L)\n",
    "            valid_sizes.append(r)\n",
    "    \n",
    "    if len(valid_sizes) < 3:\n",
    "        return np.array([]), np.array([]), 0.0, 0.0\n",
    "    \n",
    "    log_r = np.log(np.array(valid_sizes))\n",
    "    log_L = np.log(np.array(lacunarity_values))\n",
    "    \n",
    "    coeffs = np.polyfit(log_r, log_L, 1)\n",
    "    slope = coeffs[0]\n",
    "    \n",
    "    predicted = np.polyval(coeffs, log_r)\n",
    "    ss_res = np.sum((log_L - predicted) ** 2)\n",
    "    ss_tot = np.sum((log_L - np.mean(log_L)) ** 2)\n",
    "    r2 = 1 - (ss_res / ss_tot) if ss_tot > 0 else 0\n",
    "    \n",
    "    return np.array(lacunarity_values), np.array(valid_sizes), r2, slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "gap-functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_gaps(chm, valid_mask, gap_threshold=2.0, min_gap_pixels=4):\n",
    "    \"\"\"Identify contiguous gap regions (H3).\"\"\"\n",
    "    gap_mask = (chm < gap_threshold) & valid_mask\n",
    "    labeled, num_features = ndimage.label(gap_mask)\n",
    "    if num_features == 0:\n",
    "        return np.array([]), labeled\n",
    "    gap_areas = ndimage.sum(gap_mask, labeled, range(1, num_features + 1))\n",
    "    return gap_areas[gap_areas >= min_gap_pixels], labeled\n",
    "\n",
    "\n",
    "def fit_gap_distribution(gap_areas, resolution=1.0, min_fit_area=1.0):\n",
    "    \"\"\"Fit power law and exponential to gap sizes (H3).\"\"\"\n",
    "    areas = gap_areas * (resolution ** 2)\n",
    "    areas = areas[areas > 0]\n",
    "    \n",
    "    if len(areas) < 20:\n",
    "        return {'error': f'Insufficient gaps ({len(areas)})'}\n",
    "    \n",
    "    sorted_areas = np.sort(areas)\n",
    "    ecdf = np.arange(1, len(sorted_areas) + 1) / len(sorted_areas)\n",
    "    ccdf = 1 - ecdf\n",
    "    \n",
    "    fit_mask = sorted_areas >= min_fit_area\n",
    "    if np.sum(fit_mask) < 10:\n",
    "        return {'error': f'Insufficient gaps >= {min_fit_area} m²'}\n",
    "    \n",
    "    fit_areas = sorted_areas[fit_mask]\n",
    "    fit_ccdf = ccdf[fit_mask]\n",
    "    \n",
    "    try:\n",
    "        mask = fit_ccdf > 0.01\n",
    "        log_x = np.log(fit_areas[mask])\n",
    "        log_ccdf = np.log(fit_ccdf[mask])\n",
    "        \n",
    "        pl_coeffs = np.polyfit(log_x, log_ccdf, 1)\n",
    "        alpha_pl = -pl_coeffs[0] + 1\n",
    "        \n",
    "        predicted_pl = np.polyval(pl_coeffs, log_x)\n",
    "        ss_res_pl = np.sum((log_ccdf - predicted_pl) ** 2)\n",
    "        ss_tot = np.sum((log_ccdf - np.mean(log_ccdf)) ** 2)\n",
    "        r2_pl = 1 - (ss_res_pl / ss_tot) if ss_tot > 0 else 0\n",
    "        \n",
    "        exp_coeffs = np.polyfit(fit_areas[mask], log_ccdf, 1)\n",
    "        predicted_exp = np.polyval(exp_coeffs, fit_areas[mask])\n",
    "        ss_res_exp = np.sum((log_ccdf - predicted_exp) ** 2)\n",
    "        r2_exp = 1 - (ss_res_exp / ss_tot) if ss_tot > 0 else 0\n",
    "    except:\n",
    "        alpha_pl, r2_pl, r2_exp = np.nan, 0, 0\n",
    "    \n",
    "    return {\n",
    "        'n_gaps': len(areas),\n",
    "        'n_gaps_fitted': int(np.sum(fit_mask)),\n",
    "        'min_area': float(areas.min()),\n",
    "        'max_area': float(areas.max()),\n",
    "        'median_area': float(np.median(areas)),\n",
    "        'power_law': {'alpha': float(alpha_pl), 'r2': float(r2_pl)},\n",
    "        'exponential': {'r2': float(r2_exp)},\n",
    "        'zeta_2': np.pi**2 / 6,\n",
    "        'sorted_areas': sorted_areas,\n",
    "        'ccdf': ccdf\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "tree-functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_local_maxima(img, valid_mask, window_size=21, height_threshold=35.0):\n",
    "    \"\"\"Detect tree tops as local maxima (H4, H7, H9).\"\"\"\n",
    "    img_masked = np.where(valid_mask & (img >= height_threshold), img, -np.inf)\n",
    "    local_max = ndimage.maximum_filter(img_masked, size=window_size)\n",
    "    is_peak = (img_masked == local_max) & (img_masked > -np.inf)\n",
    "    peak_coords = np.array(np.where(is_peak)).T\n",
    "    peak_heights = img[is_peak]\n",
    "    return peak_coords, peak_heights\n",
    "\n",
    "\n",
    "def wigner_dyson_gue(s):\n",
    "    \"\"\"Wigner-Dyson distribution for GUE.\"\"\"\n",
    "    return (32 / np.pi**2) * s**2 * np.exp(-4 * s**2 / np.pi)\n",
    "\n",
    "\n",
    "def compute_nearest_neighbor_spacing(coords, resolution=1.0):\n",
    "    \"\"\"Compute NN spacing distribution (H4).\"\"\"\n",
    "    if len(coords) < 10:\n",
    "        return np.array([]), 0.0\n",
    "    \n",
    "    coords_physical = coords * resolution\n",
    "    tree = cKDTree(coords_physical)\n",
    "    distances, _ = tree.query(coords_physical, k=2)\n",
    "    nn_distances = distances[:, 1]\n",
    "    mean_spacing = np.mean(nn_distances)\n",
    "    return nn_distances / mean_spacing, mean_spacing\n",
    "\n",
    "\n",
    "def fit_spacing_distribution(spacings):\n",
    "    \"\"\"Compare spacing to Wigner-Dyson and Poisson (H4).\"\"\"\n",
    "    if len(spacings) < 10:\n",
    "        return {'error': 'Insufficient trees'}\n",
    "    \n",
    "    n_samples = len(spacings) * 10\n",
    "    \n",
    "    # Wigner-Dyson samples\n",
    "    wd_samples = []\n",
    "    while len(wd_samples) < n_samples:\n",
    "        s = np.random.uniform(0, 4, 1000)\n",
    "        p = wigner_dyson_gue(s)\n",
    "        accept = np.random.uniform(0, 1, 1000) < p / 0.5\n",
    "        wd_samples.extend(s[accept].tolist())\n",
    "    wd_samples = np.array(wd_samples[:n_samples])\n",
    "    \n",
    "    poisson_samples = np.random.exponential(1.0, n_samples)\n",
    "    \n",
    "    ks_wd, p_wd = ks_2samp(spacings, wd_samples)\n",
    "    ks_poisson, p_poisson = ks_2samp(spacings, poisson_samples)\n",
    "    \n",
    "    return {\n",
    "        'n_trees': len(spacings),\n",
    "        'ks_wigner_dyson': float(ks_wd),\n",
    "        'p_wigner_dyson': float(p_wd),\n",
    "        'ks_poisson': float(ks_poisson),\n",
    "        'p_poisson': float(p_poisson),\n",
    "        'spacings': spacings\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "topo-functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_chm_structure_metrics(chm_array, valid_mask, resolution=1.0):\n",
    "    \"\"\"Compute CHM structure metrics for H5.\"\"\"\n",
    "    window_size = 15\n",
    "    chm_roughness = ndimage.generic_filter(\n",
    "        np.where(valid_mask, chm_array, np.nan),\n",
    "        lambda x: np.nanstd(x) if np.sum(~np.isnan(x)) > 5 else np.nan,\n",
    "        size=window_size, mode='constant', cval=np.nan\n",
    "    )\n",
    "    return {'roughness': chm_roughness}\n",
    "\n",
    "\n",
    "def compute_topographic_variables(dem_array, valid_mask, resolution=1.0):\n",
    "    \"\"\"Compute slope, TPI, roughness from DEM (H5).\"\"\"\n",
    "    gy, gx = np.gradient(np.where(valid_mask, dem_array, np.nan), resolution)\n",
    "    slope = np.degrees(np.arctan(np.sqrt(gx**2 + gy**2)))\n",
    "    slope[~valid_mask] = np.nan\n",
    "    \n",
    "    window_size = 15\n",
    "    local_mean = ndimage.uniform_filter(\n",
    "        np.where(valid_mask, dem_array, np.nan), size=window_size, mode='constant', cval=np.nan\n",
    "    )\n",
    "    tpi = dem_array - local_mean\n",
    "    tpi[~valid_mask] = np.nan\n",
    "    \n",
    "    roughness = ndimage.generic_filter(\n",
    "        np.where(valid_mask, dem_array, np.nan),\n",
    "        lambda x: np.nanstd(x) if np.sum(~np.isnan(x)) > 5 else np.nan,\n",
    "        size=window_size, mode='constant', cval=np.nan\n",
    "    )\n",
    "    \n",
    "    return {'elevation': dem_array, 'slope': slope, 'tpi': tpi, 'roughness': roughness}\n",
    "\n",
    "\n",
    "def analyze_biotic_decoupling(chm_array, valid_mask, dem_array=None, resolution=1.0):\n",
    "    \"\"\"Analyze CHM-topography correlation (H5).\"\"\"\n",
    "    chm_metrics = compute_chm_structure_metrics(chm_array, valid_mask, resolution)\n",
    "    roughness = chm_metrics['roughness']\n",
    "    \n",
    "    if dem_array is not None:\n",
    "        topo_vars = compute_topographic_variables(dem_array, valid_mask, resolution)\n",
    "        has_dem = True\n",
    "    else:\n",
    "        rows, cols = chm_array.shape\n",
    "        y_coords, x_coords = np.meshgrid(np.arange(rows), np.arange(cols), indexing='ij')\n",
    "        topo_vars = {'x_position': x_coords / cols, 'y_position': y_coords / rows}\n",
    "        has_dem = False\n",
    "    \n",
    "    valid_indices = np.where(valid_mask & ~np.isnan(roughness))\n",
    "    n_valid = len(valid_indices[0])\n",
    "    \n",
    "    if n_valid < 100:\n",
    "        return {'error': 'Insufficient valid pixels'}\n",
    "    \n",
    "    max_samples = 50000\n",
    "    if n_valid > max_samples:\n",
    "        idx = np.random.choice(n_valid, max_samples, replace=False)\n",
    "        sample_rows, sample_cols = valid_indices[0][idx], valid_indices[1][idx]\n",
    "    else:\n",
    "        sample_rows, sample_cols = valid_indices[0], valid_indices[1]\n",
    "    \n",
    "    roughness_samples = roughness[sample_rows, sample_cols]\n",
    "    correlations = {}\n",
    "    \n",
    "    var_names = ['elevation', 'slope', 'tpi', 'roughness'] if has_dem else ['x_position', 'y_position']\n",
    "    for var_name in var_names:\n",
    "        if var_name in topo_vars:\n",
    "            var_data = topo_vars[var_name][sample_rows, sample_cols]\n",
    "            valid_both = ~(np.isnan(roughness_samples) | np.isnan(var_data))\n",
    "            if np.sum(valid_both) > 50:\n",
    "                r, p = pearsonr(roughness_samples[valid_both], var_data[valid_both])\n",
    "                correlations[f'chm_roughness_vs_{var_name}'] = {'r': float(r), 'p': float(p)}\n",
    "    \n",
    "    r_values = [abs(v['r']) for v in correlations.values() if not np.isnan(v['r'])]\n",
    "    mean_abs_r = float(np.mean(r_values)) if r_values else np.nan\n",
    "    \n",
    "    return {'n_samples': len(roughness_samples), 'dem_available': has_dem, \n",
    "            'correlations': correlations, 'mean_abs_correlation': mean_abs_r}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "h6-h9-functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_log_periodic_oscillations(lacunarity_values, box_sizes):\n",
    "    \"\"\"Test for log-periodic oscillations (H8).\"\"\"\n",
    "    if len(lacunarity_values) < 5:\n",
    "        return {'error': 'Insufficient scales'}\n",
    "    \n",
    "    log_r = np.log(box_sizes)\n",
    "    log_L = np.log(lacunarity_values)\n",
    "    \n",
    "    coeffs = np.polyfit(log_r, log_L, 1)\n",
    "    trend = np.polyval(coeffs, log_r)\n",
    "    residuals = log_L - trend\n",
    "    \n",
    "    n = len(residuals)\n",
    "    if n >= 5:\n",
    "        autocorr = np.correlate(residuals, residuals, mode='full')\n",
    "        autocorr = autocorr[n-1:] / autocorr[n-1]\n",
    "        peaks = [(i, autocorr[i]) for i in range(1, len(autocorr) - 1) \n",
    "                 if autocorr[i] > autocorr[i-1] and autocorr[i] > autocorr[i+1]]\n",
    "        has_oscillation = len(peaks) > 0 and any(p[1] > 0.3 for p in peaks)\n",
    "    else:\n",
    "        has_oscillation = False\n",
    "    \n",
    "    rms_residual = float(np.sqrt(np.mean(residuals**2)))\n",
    "    return {'rms_residual': rms_residual, 'has_oscillation': has_oscillation}\n",
    "\n",
    "\n",
    "def analyze_riemann_gas_density(tree_coords, valid_mask, resolution=1.0, n_bins=20):\n",
    "    \"\"\"Test Riemann gas statistics (H9).\"\"\"\n",
    "    if len(tree_coords) < 20:\n",
    "        return {'error': 'Insufficient trees'}\n",
    "    \n",
    "    rows, cols = valid_mask.shape\n",
    "    cell_size = max(rows, cols) // n_bins\n",
    "    \n",
    "    densities = []\n",
    "    for i in range(n_bins):\n",
    "        for j in range(n_bins):\n",
    "            y_start, y_end = i * cell_size, (i + 1) * cell_size\n",
    "            x_start, x_end = j * cell_size, (j + 1) * cell_size\n",
    "            \n",
    "            in_cell = ((tree_coords[:, 0] >= y_start) & (tree_coords[:, 0] < y_end) &\n",
    "                       (tree_coords[:, 1] >= x_start) & (tree_coords[:, 1] < x_end))\n",
    "            n_trees = np.sum(in_cell)\n",
    "            \n",
    "            cell_valid = valid_mask[y_start:y_end, x_start:x_end]\n",
    "            valid_area = cell_valid.sum() * resolution**2\n",
    "            \n",
    "            if valid_area > 0:\n",
    "                density = n_trees / valid_area * 10000\n",
    "                densities.append(density)\n",
    "    \n",
    "    densities = np.array([d for d in densities if d > 0])\n",
    "    \n",
    "    if len(densities) < 10:\n",
    "        return {'error': 'Insufficient non-empty cells'}\n",
    "    \n",
    "    mean_density = np.mean(densities)\n",
    "    fano_factor = float(np.var(densities) / mean_density) if mean_density > 0 else np.nan\n",
    "    \n",
    "    interp = 'repulsion' if fano_factor < 0.8 else ('clustering' if fano_factor > 1.2 else 'random')\n",
    "    return {'n_cells': len(densities), 'mean_density_per_ha': float(mean_density),\n",
    "            'fano_factor': fano_factor, 'interpretation': interp}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h1-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# HYPOTHESIS 1: Optimal Filling\n",
    "\n",
    "**Prediction:** Tropical old-growth forests show higher fractal dimensions (D > 2.5) due to multi-layered canopy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "run-h1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "HYPOTHESIS 1: OPTIMAL FILLING\n",
      "======================================================================\n",
      "\n",
      "DBC Fractal Dimension: D = 2.3114\n",
      "R-squared: 0.9906\n",
      "\n",
      "D = 2.31 indicates MODERATE complexity\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"HYPOTHESIS 1: OPTIMAL FILLING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "D_dbc, r2_dbc, scales_dbc, Ns_dbc = differential_box_counting(chm_array, valid_mask)\n",
    "\n",
    "print(f\"\\nDBC Fractal Dimension: D = {D_dbc:.4f}\")\n",
    "print(f\"R-squared: {r2_dbc:.4f}\")\n",
    "\n",
    "if np.isnan(D_dbc):\n",
    "    h1_class, h1_interp = \"INSUFFICIENT_DATA\", \"Insufficient data\"\n",
    "elif D_dbc >= 2.5:\n",
    "    h1_class = \"OLD_GROWTH\"\n",
    "    h1_interp = f\"D = {D_dbc:.2f} indicates OLD GROWTH (high complexity)\"\n",
    "elif D_dbc >= 2.3:\n",
    "    h1_class = \"MODERATE\"\n",
    "    h1_interp = f\"D = {D_dbc:.2f} indicates MODERATE complexity\"\n",
    "else:\n",
    "    h1_class = \"LOW\"\n",
    "    h1_interp = f\"D = {D_dbc:.2f} indicates LOW complexity\"\n",
    "\n",
    "print(f\"\\n{h1_interp}\")\n",
    "\n",
    "h1_results = {'hypothesis': 'H1: Optimal Filling', 'D_dbc': float(D_dbc) if not np.isnan(D_dbc) else None,\n",
    "              'r_squared': float(r2_dbc), 'classification': h1_class, 'interpretation': h1_interp,\n",
    "              'supports_hypothesis': h1_class == 'OLD_GROWTH'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h2-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# HYPOTHESIS 2: Scale Invariance\n",
    "\n",
    "**Prediction:** Mature tropical forests show scale-invariant gap structure (R² > 0.95)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "run-h2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "HYPOTHESIS 2: SCALE INVARIANCE\n",
      "======================================================================\n",
      "\n",
      "Lacunarity R² (log-log): 0.9775\n",
      "Slope: -0.0264\n",
      "\n",
      "R² = 0.98 indicates STRONG scale invariance (OLD GROWTH)\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"HYPOTHESIS 2: SCALE INVARIANCE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "lacunarity, lac_sizes, r2_lac, lac_slope = gliding_box_lacunarity(chm_array, valid_mask)\n",
    "\n",
    "print(f\"\\nLacunarity R² (log-log): {r2_lac:.4f}\")\n",
    "print(f\"Slope: {lac_slope:.4f}\")\n",
    "\n",
    "if r2_lac > 0.95:\n",
    "    h2_class = \"SCALE_INVARIANT\"\n",
    "    h2_interp = f\"R² = {r2_lac:.2f} indicates STRONG scale invariance (OLD GROWTH)\"\n",
    "elif r2_lac > 0.80:\n",
    "    h2_class = \"MODERATE\"\n",
    "    h2_interp = f\"R² = {r2_lac:.2f} indicates MODERATE scale invariance\"\n",
    "else:\n",
    "    h2_class = \"CHARACTERISTIC_SCALES\"\n",
    "    h2_interp = f\"R² = {r2_lac:.2f} indicates CHARACTERISTIC SCALES present\"\n",
    "\n",
    "print(f\"\\n{h2_interp}\")\n",
    "\n",
    "h2_results = {'hypothesis': 'H2: Scale Invariance', 'r_squared': float(r2_lac),\n",
    "              'slope': float(lac_slope), 'classification': h2_class, 'interpretation': h2_interp,\n",
    "              'supports_hypothesis': h2_class == 'SCALE_INVARIANT'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h3-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# HYPOTHESIS 3: Zeta Distribution\n",
    "\n",
    "**Prediction:** Gap sizes follow power-law with α ≈ 2.0, related to ζ(2) = π²/6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "run-h3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "HYPOTHESIS 3: ZETA DISTRIBUTION\n",
      "======================================================================\n",
      "\n",
      "Gaps: 172 (fitted: 172)\n",
      "Power Law α: 1.9512\n",
      "Power Law R²: 0.9110\n",
      "Exponential R²: 0.9600\n",
      "\n",
      "ζ(2) = π²/6 ≈ 1.6449\n",
      "|α - 2| = 0.0488\n",
      "\n",
      "Exponential fits better (R²=0.96)\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"HYPOTHESIS 3: ZETA DISTRIBUTION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "gap_areas, labeled_gaps = identify_gaps(chm_array, valid_mask, GAP_THRESHOLD)\n",
    "gap_results = fit_gap_distribution(gap_areas, pixel_resolution, min_fit_area=1.0)\n",
    "\n",
    "if 'error' in gap_results:\n",
    "    print(f\"Error: {gap_results['error']}\")\n",
    "    h3_class, h3_interp = \"INSUFFICIENT_DATA\", gap_results['error']\n",
    "else:\n",
    "    print(f\"\\nGaps: {gap_results['n_gaps']} (fitted: {gap_results['n_gaps_fitted']})\")\n",
    "    print(f\"Power Law α: {gap_results['power_law']['alpha']:.4f}\")\n",
    "    print(f\"Power Law R²: {gap_results['power_law']['r2']:.4f}\")\n",
    "    print(f\"Exponential R²: {gap_results['exponential']['r2']:.4f}\")\n",
    "    print(f\"\\nζ(2) = π²/6 ≈ {gap_results['zeta_2']:.4f}\")\n",
    "    print(f\"|α - 2| = {abs(gap_results['power_law']['alpha'] - 2.0):.4f}\")\n",
    "    \n",
    "    pl_r2, alpha = gap_results['power_law']['r2'], gap_results['power_law']['alpha']\n",
    "    exp_r2 = gap_results['exponential']['r2']\n",
    "    \n",
    "    if pl_r2 > exp_r2 and pl_r2 > 0.8:\n",
    "        if 1.8 <= alpha <= 2.2:\n",
    "            h3_class = \"ZETA_DISTRIBUTED\"\n",
    "            h3_interp = f\"α ≈ {alpha:.2f} close to 2.0 (OLD GROWTH signature)\"\n",
    "        else:\n",
    "            h3_class = \"POWER_LAW_MODIFIED\"\n",
    "            h3_interp = f\"α = {alpha:.2f} deviates from 2.0 (DISTURBANCE modified)\"\n",
    "    else:\n",
    "        h3_class = \"EXPONENTIAL\"\n",
    "        h3_interp = f\"Exponential fits better (R²={exp_r2:.2f})\"\n",
    "    \n",
    "    print(f\"\\n{h3_interp}\")\n",
    "\n",
    "h3_results = {'hypothesis': 'H3: Zeta Distribution', 'n_gaps': gap_results.get('n_gaps', 0),\n",
    "              'power_law_alpha': gap_results.get('power_law', {}).get('alpha'),\n",
    "              'power_law_r2': gap_results.get('power_law', {}).get('r2'),\n",
    "              'classification': h3_class, 'interpretation': h3_interp,\n",
    "              'supports_hypothesis': h3_class == 'ZETA_DISTRIBUTED'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h4-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# HYPOTHESIS 4: Universal Repulsion\n",
    "\n",
    "**Prediction:** Emergent tree spacing follows Wigner-Dyson (GUE) distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "run-h4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "HYPOTHESIS 4: UNIVERSAL REPULSION\n",
      "======================================================================\n",
      "\n",
      "Emergent trees detected: 721\n",
      "Height threshold: >= 35.0m\n",
      "Height range: 35.0 - 54.9m\n",
      "Mean NN distance: 16.72m\n",
      "\n",
      "Wigner-Dyson KS: 0.1524 (p=0.0000)\n",
      "Poisson KS: 0.2951 (p=0.0000)\n",
      "\n",
      "Neither distribution fits well -> Complex structure\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"HYPOTHESIS 4: UNIVERSAL REPULSION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "tree_coords, tree_heights = detect_local_maxima(chm_array, valid_mask, LOCAL_MAX_WINDOW, DOMINANT_TREE_THRESHOLD)\n",
    "print(f\"\\nEmergent trees detected: {len(tree_coords)}\")\n",
    "print(f\"Height threshold: >= {DOMINANT_TREE_THRESHOLD}m\")\n",
    "\n",
    "if len(tree_coords) >= 10:\n",
    "    print(f\"Height range: {tree_heights.min():.1f} - {tree_heights.max():.1f}m\")\n",
    "    spacings, mean_spacing = compute_nearest_neighbor_spacing(tree_coords, pixel_resolution)\n",
    "    print(f\"Mean NN distance: {mean_spacing:.2f}m\")\n",
    "    \n",
    "    spacing_results = fit_spacing_distribution(spacings)\n",
    "    print(f\"\\nWigner-Dyson KS: {spacing_results['ks_wigner_dyson']:.4f} (p={spacing_results['p_wigner_dyson']:.4f})\")\n",
    "    print(f\"Poisson KS: {spacing_results['ks_poisson']:.4f} (p={spacing_results['p_poisson']:.4f})\")\n",
    "    \n",
    "    ks_wd, ks_po = spacing_results['ks_wigner_dyson'], spacing_results['ks_poisson']\n",
    "    if ks_wd < ks_po and spacing_results['p_wigner_dyson'] > 0.05:\n",
    "        h4_class = \"WIGNER_DYSON\"\n",
    "        h4_interp = f\"Spacing follows Wigner-Dyson (p={spacing_results['p_wigner_dyson']:.3f}) -> REPULSION\"\n",
    "    elif ks_po < ks_wd and spacing_results['p_poisson'] > 0.05:\n",
    "        h4_class = \"POISSON\"\n",
    "        h4_interp = f\"Spacing follows Poisson (p={spacing_results['p_poisson']:.3f}) -> RANDOM\"\n",
    "    else:\n",
    "        h4_class = \"NEITHER\"\n",
    "        h4_interp = \"Neither distribution fits well -> Complex structure\"\n",
    "else:\n",
    "    spacing_results = {'error': 'Insufficient trees'}\n",
    "    h4_class, h4_interp = \"INSUFFICIENT_DATA\", f\"Only {len(tree_coords)} trees detected\"\n",
    "    mean_spacing = np.nan\n",
    "    spacings = np.array([])\n",
    "\n",
    "print(f\"\\n{h4_interp}\")\n",
    "\n",
    "h4_results = {'hypothesis': 'H4: Universal Repulsion', 'n_trees': len(tree_coords),\n",
    "              'mean_spacing_m': float(mean_spacing) if not np.isnan(mean_spacing) else None,\n",
    "              'classification': h4_class, 'interpretation': h4_interp,\n",
    "              'supports_hypothesis': h4_class == 'WIGNER_DYSON'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h5-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# HYPOTHESIS 5: Biotic Decoupling\n",
    "\n",
    "**Prediction:** Mature tropical forests show weak correlation (|r| < 0.3) between canopy and topography."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "run-h5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "HYPOTHESIS 5: BIOTIC DECOUPLING\n",
      "======================================================================\n",
      "\n",
      "Loading DTM from: BCI_whole_2023_05_26_dtm.tif\n",
      "DEM range: 39.0 - 166.8 m\n",
      "\n",
      "Samples: 50,000\n",
      "DEM used: True\n",
      "\n",
      "Correlations:\n",
      "  chm_roughness_vs_elevation: r = -0.1156 ***\n",
      "  chm_roughness_vs_slope: r = +0.0692 ***\n",
      "  chm_roughness_vs_roughness: r = +0.0735 ***\n",
      "\n",
      "Mean |r|: 0.0861\n",
      "\n",
      "Mean |r| = 0.09 indicates BIOTIC DECOUPLING (OLD GROWTH)\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"HYPOTHESIS 5: BIOTIC DECOUPLING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load DEM if available\n",
    "dem_array = None\n",
    "if DTM_PATH.exists():\n",
    "    print(f\"\\nLoading DTM from: {DTM_PATH.name}\")\n",
    "    dem_full = rxr.open_rasterio(DTM_PATH, masked=True)\n",
    "    \n",
    "    if USE_SUBSET:\n",
    "        dem = dem_full[:, center_y - half_size : center_y + half_size,\n",
    "                       center_x - half_size : center_x + half_size]\n",
    "    else:\n",
    "        dem = dem_full\n",
    "    \n",
    "    dem_array = dem.values.squeeze().astype(np.float64)\n",
    "    \n",
    "    if dem_array.shape != chm_array.shape:\n",
    "        print(f\"Resampling DEM from {dem_array.shape} to match CHM {chm_array.shape}\")\n",
    "        dem = dem.rio.reproject_match(chm)\n",
    "        dem_array = dem.values.squeeze().astype(np.float64)\n",
    "    \n",
    "    dem_nodata = dem.rio.nodata if dem.rio.nodata is not None else NODATA_VALUE\n",
    "    dem_valid_mask = ~np.isnan(dem_array) & (dem_array != dem_nodata)\n",
    "    combined_mask = valid_mask & dem_valid_mask\n",
    "    print(f\"DEM range: {dem_array[dem_valid_mask].min():.1f} - {dem_array[dem_valid_mask].max():.1f} m\")\n",
    "else:\n",
    "    print(\"\\nNo DEM available - using position metrics\")\n",
    "    combined_mask = valid_mask\n",
    "\n",
    "topo_results = analyze_biotic_decoupling(chm_array, combined_mask, dem_array, pixel_resolution)\n",
    "\n",
    "if 'error' in topo_results:\n",
    "    print(f\"Error: {topo_results['error']}\")\n",
    "    h5_class, h5_interp = \"INSUFFICIENT_DATA\", topo_results['error']\n",
    "else:\n",
    "    print(f\"\\nSamples: {topo_results['n_samples']:,}\")\n",
    "    print(f\"DEM used: {topo_results['dem_available']}\")\n",
    "    print(\"\\nCorrelations:\")\n",
    "    for var, stats in topo_results['correlations'].items():\n",
    "        sig = \"***\" if stats['p'] < 0.001 else \"**\" if stats['p'] < 0.01 else \"*\" if stats['p'] < 0.05 else \"\"\n",
    "        print(f\"  {var}: r = {stats['r']:+.4f} {sig}\")\n",
    "    \n",
    "    mean_r = topo_results['mean_abs_correlation']\n",
    "    print(f\"\\nMean |r|: {mean_r:.4f}\")\n",
    "    \n",
    "    if mean_r < 0.3:\n",
    "        h5_class = \"DECOUPLED\"\n",
    "        h5_interp = f\"Mean |r| = {mean_r:.2f} indicates BIOTIC DECOUPLING (OLD GROWTH)\"\n",
    "    elif mean_r < 0.6:\n",
    "        h5_class = \"PARTIAL\"\n",
    "        h5_interp = f\"Mean |r| = {mean_r:.2f} indicates PARTIAL coupling\"\n",
    "    else:\n",
    "        h5_class = \"COUPLED\"\n",
    "        h5_interp = f\"Mean |r| = {mean_r:.2f} indicates strong TOPOGRAPHIC control\"\n",
    "    \n",
    "    print(f\"\\n{h5_interp}\")\n",
    "\n",
    "h5_results = {'hypothesis': 'H5: Biotic Decoupling', 'dem_available': topo_results.get('dem_available', False),\n",
    "              'mean_abs_correlation': topo_results.get('mean_abs_correlation'),\n",
    "              'classification': h5_class, 'interpretation': h5_interp,\n",
    "              'supports_hypothesis': h5_class == 'DECOUPLED'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h6-h9-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# HYPOTHESES 6-9: Spatial Distribution (Fractal String Theory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "run-h6-h9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "HYPOTHESES 6-9: SPATIAL DISTRIBUTION\n",
      "======================================================================\n",
      "\n",
      "--- H6: Fractal String Gap ---\n",
      "α = 1.95 consistent with fractal string predictions\n",
      "\n",
      "--- H7: Prime Number Repulsion (GUE) ---\n",
      "Tree spacing does not show prime-like repulsion\n",
      "\n",
      "--- H8: Complex Dimension Oscillation ---\n",
      "No log-periodic oscillations (RMS = 0.0056)\n",
      "\n",
      "--- H9: Riemann Gas Density ---\n",
      "Fano = 5.056 > 1 indicates clustering (not Riemann gas)\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"HYPOTHESES 6-9: SPATIAL DISTRIBUTION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# H6: Fractal String Gap\n",
    "print(\"\\n--- H6: Fractal String Gap ---\")\n",
    "if 'power_law' in gap_results and gap_results['power_law']['r2'] > 0.8:\n",
    "    alpha = gap_results['power_law']['alpha']\n",
    "    h6_class = \"FRACTAL_STRING\" if 1.5 <= alpha <= 2.5 else \"NON_FRACTAL\"\n",
    "    h6_interp = f\"α = {alpha:.2f} {'consistent with' if h6_class == 'FRACTAL_STRING' else 'outside'} fractal string predictions\"\n",
    "else:\n",
    "    h6_class, h6_interp = \"NO_POWER_LAW\", \"Gap distribution does not follow power law\"\n",
    "print(h6_interp)\n",
    "h6_results = {'hypothesis': 'H6: Fractal String Gap', 'classification': h6_class,\n",
    "              'interpretation': h6_interp, 'supports_hypothesis': h6_class == 'FRACTAL_STRING'}\n",
    "\n",
    "# H7: Prime Number Repulsion\n",
    "print(\"\\n--- H7: Prime Number Repulsion (GUE) ---\")\n",
    "if h4_class == 'WIGNER_DYSON':\n",
    "    h7_class, h7_interp = \"GUE_REPULSION\", \"Tree spacing consistent with GUE statistics\"\n",
    "elif h4_class == 'REPULSION_PARTIAL':\n",
    "    h7_class, h7_interp = \"PARTIAL_REPULSION\", \"Partial evidence for prime-like repulsion\"\n",
    "else:\n",
    "    h7_class, h7_interp = \"NO_REPULSION\", \"Tree spacing does not show prime-like repulsion\"\n",
    "print(h7_interp)\n",
    "h7_results = {'hypothesis': 'H7: Prime Number Repulsion', 'classification': h7_class,\n",
    "              'interpretation': h7_interp, 'supports_hypothesis': h7_class == 'GUE_REPULSION'}\n",
    "\n",
    "# H8: Complex Dimension Oscillation\n",
    "print(\"\\n--- H8: Complex Dimension Oscillation ---\")\n",
    "if len(lacunarity) >= 5:\n",
    "    osc_results = analyze_log_periodic_oscillations(lacunarity, lac_sizes)\n",
    "    if 'error' not in osc_results:\n",
    "        if osc_results['has_oscillation']:\n",
    "            h8_class = \"OSCILLATION_PRESENT\"\n",
    "            h8_interp = f\"Log-periodic oscillations detected (RMS = {osc_results['rms_residual']:.4f})\"\n",
    "        else:\n",
    "            h8_class = \"NO_OSCILLATION\"\n",
    "            h8_interp = f\"No log-periodic oscillations (RMS = {osc_results['rms_residual']:.4f})\"\n",
    "    else:\n",
    "        h8_class, h8_interp = \"INSUFFICIENT_DATA\", osc_results['error']\n",
    "else:\n",
    "    h8_class, h8_interp = \"INSUFFICIENT_DATA\", \"Insufficient lacunarity data\"\n",
    "print(h8_interp)\n",
    "h8_results = {'hypothesis': 'H8: Complex Dimension Oscillation', 'classification': h8_class,\n",
    "              'interpretation': h8_interp, 'supports_hypothesis': h8_class == 'OSCILLATION_PRESENT'}\n",
    "\n",
    "# H9: Riemann Gas Density\n",
    "print(\"\\n--- H9: Riemann Gas Density ---\")\n",
    "if len(tree_coords) >= 20:\n",
    "    riemann_results = analyze_riemann_gas_density(tree_coords, valid_mask, pixel_resolution)\n",
    "    if 'error' not in riemann_results:\n",
    "        fano = riemann_results['fano_factor']\n",
    "        interp = riemann_results['interpretation']\n",
    "        if interp == 'repulsion':\n",
    "            h9_class = \"RIEMANN_GAS\"\n",
    "            h9_interp = f\"Fano = {fano:.3f} < 1 indicates repulsion (Riemann gas)\"\n",
    "        elif interp == 'clustering':\n",
    "            h9_class = \"CLUSTERING\"\n",
    "            h9_interp = f\"Fano = {fano:.3f} > 1 indicates clustering (not Riemann gas)\"\n",
    "        else:\n",
    "            h9_class = \"POISSON\"\n",
    "            h9_interp = f\"Fano = {fano:.3f} ≈ 1 indicates random/Poisson\"\n",
    "    else:\n",
    "        h9_class, h9_interp = \"INSUFFICIENT_DATA\", riemann_results['error']\n",
    "        fano = np.nan\n",
    "else:\n",
    "    h9_class, h9_interp = \"INSUFFICIENT_DATA\", f\"Only {len(tree_coords)} trees\"\n",
    "    fano = np.nan\n",
    "print(h9_interp)\n",
    "h9_results = {'hypothesis': 'H9: Riemann Gas Density', 'classification': h9_class,\n",
    "              'fano_factor': float(fano) if not np.isnan(fano) else None,\n",
    "              'interpretation': h9_interp, 'supports_hypothesis': h9_class == 'RIEMANN_GAS'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# COMPREHENSIVE SUMMARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "summary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "           COMPREHENSIVE HYPOTHESIS TESTING SUMMARY - BCI TROPICAL FOREST\n",
      "================================================================================\n",
      "\n",
      "Site: Barro Colorado Island - Tropical Moist Forest\n",
      "Analysis Date: 2025-12-25 19:20:52\n",
      "Subset: True (2000x2000 pixels)\n",
      "Valid pixels: 3,999,996\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "PRIMARY HYPOTHESES (1-5)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "H1: Optimal Filling\n",
      "    Classification: MODERATE\n",
      "    D = 2.31 indicates MODERATE complexity\n",
      "    Status: NOT SUPPORTED\n",
      "\n",
      "H2: Scale Invariance\n",
      "    Classification: SCALE_INVARIANT\n",
      "    R² = 0.98 indicates STRONG scale invariance (OLD GROWTH)\n",
      "    Status: SUPPORTED\n",
      "\n",
      "H3: Zeta Distribution\n",
      "    Classification: EXPONENTIAL\n",
      "    Exponential fits better (R²=0.96)\n",
      "    Status: NOT SUPPORTED\n",
      "\n",
      "H4: Universal Repulsion\n",
      "    Classification: NEITHER\n",
      "    Neither distribution fits well -> Complex structure\n",
      "    Status: NOT SUPPORTED\n",
      "\n",
      "H5: Biotic Decoupling\n",
      "    Classification: DECOUPLED\n",
      "    Mean |r| = 0.09 indicates BIOTIC DECOUPLING (OLD GROWTH)\n",
      "    Status: SUPPORTED\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "SPATIAL DISTRIBUTION HYPOTHESES (6-9)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "H6: Fractal String Gap\n",
      "    Classification: FRACTAL_STRING\n",
      "    α = 1.95 consistent with fractal string predictions\n",
      "    Status: SUPPORTED\n",
      "\n",
      "H7: Prime Number Repulsion\n",
      "    Classification: NO_REPULSION\n",
      "    Tree spacing does not show prime-like repulsion\n",
      "    Status: NOT SUPPORTED\n",
      "\n",
      "H8: Complex Dimension Oscillation\n",
      "    Classification: NO_OSCILLATION\n",
      "    No log-periodic oscillations (RMS = 0.0056)\n",
      "    Status: NOT SUPPORTED\n",
      "\n",
      "H9: Riemann Gas Density\n",
      "    Classification: CLUSTERING\n",
      "    Fano = 5.056 > 1 indicates clustering (not Riemann gas)\n",
      "    Status: NOT SUPPORTED\n",
      "\n",
      "================================================================================\n",
      "OVERALL ASSESSMENT\n",
      "================================================================================\n",
      "\n",
      "Primary hypotheses supported: 2/5\n",
      "All hypotheses supported: 3/9\n",
      "\n",
      "FOREST CLASSIFICATION: RECOVERING / TRANSITIONAL FOREST\n",
      "\n",
      "Mixed evidence suggesting forest transition toward old-growth.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "KEY METRICS\n",
      "--------------------------------------------------------------------------------\n",
      "  DBC Fractal Dimension (D):     2.3114\n",
      "  Lacunarity R² (scale inv.):    0.9775 [SCALE INVARIANT]\n",
      "  Gap power-law exponent (α):    1.9512 [NEAR ζ(2)]\n",
      "  Tree spacing:                  NEITHER\n",
      "  Topographic correlation:       0.0861 [DECOUPLED]\n"
     ]
    }
   ],
   "source": [
    "all_results = [h1_results, h2_results, h3_results, h4_results, h5_results,\n",
    "               h6_results, h7_results, h8_results, h9_results]\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"           COMPREHENSIVE HYPOTHESIS TESTING SUMMARY - BCI TROPICAL FOREST\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nSite: {SITE_DESCRIPTION}\")\n",
    "print(f\"Analysis Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Subset: {USE_SUBSET} ({SUBSET_SIZE}x{SUBSET_SIZE} pixels)\")\n",
    "print(f\"Valid pixels: {valid_mask.sum():,}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"PRIMARY HYPOTHESES (1-5)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for i, result in enumerate(all_results[:5], 1):\n",
    "    status = \"SUPPORTED\" if result['supports_hypothesis'] else \"NOT SUPPORTED\"\n",
    "    print(f\"\\nH{i}: {result['hypothesis'].split(': ')[1]}\")\n",
    "    print(f\"    Classification: {result['classification']}\")\n",
    "    print(f\"    {result['interpretation']}\")\n",
    "    print(f\"    Status: {status}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"SPATIAL DISTRIBUTION HYPOTHESES (6-9)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for i, result in enumerate(all_results[5:], 6):\n",
    "    status = \"SUPPORTED\" if result['supports_hypothesis'] else \"NOT SUPPORTED\"\n",
    "    print(f\"\\nH{i}: {result['hypothesis'].split(': ')[1]}\")\n",
    "    print(f\"    Classification: {result['classification']}\")\n",
    "    print(f\"    {result['interpretation']}\")\n",
    "    print(f\"    Status: {status}\")\n",
    "\n",
    "# Overall assessment\n",
    "supported = sum(1 for r in all_results if r['supports_hypothesis'])\n",
    "primary_supported = sum(1 for r in all_results[:5] if r['supports_hypothesis'])\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"OVERALL ASSESSMENT\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nPrimary hypotheses supported: {primary_supported}/5\")\n",
    "print(f\"All hypotheses supported: {supported}/9\")\n",
    "\n",
    "if primary_supported >= 4:\n",
    "    forest_class = \"OLD-GROWTH / MATURE TROPICAL FOREST\"\n",
    "    desc = \"Strong evidence for self-organized, mature tropical forest structure.\"\n",
    "elif primary_supported >= 2:\n",
    "    forest_class = \"RECOVERING / TRANSITIONAL FOREST\"\n",
    "    desc = \"Mixed evidence suggesting forest transition toward old-growth.\"\n",
    "else:\n",
    "    forest_class = \"YOUNG / DISTURBED FOREST\"\n",
    "    desc = \"Limited support for old-growth hypotheses.\"\n",
    "\n",
    "print(f\"\\nFOREST CLASSIFICATION: {forest_class}\")\n",
    "print(f\"\\n{desc}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"KEY METRICS\")\n",
    "print(\"-\"*80)\n",
    "print(f\"  DBC Fractal Dimension (D):     {D_dbc:.4f}\" + (\" [HIGH]\" if D_dbc >= 2.5 else \"\"))\n",
    "print(f\"  Lacunarity R² (scale inv.):    {r2_lac:.4f}\" + (\" [SCALE INVARIANT]\" if r2_lac > 0.95 else \"\"))\n",
    "alpha_val = gap_results.get('power_law', {}).get('alpha', np.nan)\n",
    "print(f\"  Gap power-law exponent (α):    {alpha_val:.4f}\" + (\" [NEAR ζ(2)]\" if 1.8 <= alpha_val <= 2.2 else \"\"))\n",
    "print(f\"  Tree spacing:                  {h4_class}\")\n",
    "mean_corr = topo_results.get('mean_abs_correlation', np.nan)\n",
    "print(f\"  Topographic correlation:       {mean_corr:.4f}\" + (\" [DECOUPLED]\" if mean_corr < 0.3 else \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "save-results",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results saved to: /home/jovyan/data-store/data/output/smithsonian/analysis/fractal_hypotheses/bci_panama_hypothesis_results.json\n"
     ]
    }
   ],
   "source": [
    "# Save results to JSON\n",
    "output_data = {\n",
    "    'site': {'name': SITE_NAME, 'description': SITE_DESCRIPTION,\n",
    "             'subset_used': USE_SUBSET, 'subset_size': SUBSET_SIZE,\n",
    "             'resolution_m': float(pixel_resolution),\n",
    "             'analysis_date': datetime.now().isoformat()},\n",
    "    'height_stats': {'mean_m': float(np.mean(valid_heights)), 'max_m': float(np.max(valid_heights)),\n",
    "                     'p95_m': float(np.percentile(valid_heights, 95))},\n",
    "    'hypotheses': {f'H{i+1}': r for i, r in enumerate(all_results)},\n",
    "    'summary': {'primary_supported': primary_supported, 'total_supported': supported,\n",
    "                'forest_classification': forest_class}\n",
    "}\n",
    "\n",
    "results_path = OUTPUT_DIR / f\"{SITE_NAME}_hypothesis_results.json\"\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(output_data, f, indent=2)\n",
    "\n",
    "print(f\"\\nResults saved to: {results_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cleanup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory cleaned up.\n"
     ]
    }
   ],
   "source": [
    "# Cleanup\n",
    "chm_full.close()\n",
    "if USE_SUBSET:\n",
    "    chm.close()\n",
    "if dem_array is not None:\n",
    "    dem_full.close()\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "print(\"Memory cleaned up.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "references",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "1. **Sarkar & Chaudhuri (1994)** - Differential Box Counting\n",
    "2. **Plotnick et al. (1996)** - Lacunarity analysis\n",
    "3. **Clark & Evans (1954)** - Nearest neighbor analysis\n",
    "4. **Wigner-Dyson** - Random matrix theory\n",
    "5. **BCI Forest Dynamics** - https://forestgeo.si.edu/sites/neotropics/barro-colorado-island\n",
    "6. **Smithsonian ALS 2023** - https://smithsonian.dataone.org/datasets/ALS_Panama_2023/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3DEP (Python 3.11)",
   "language": "python",
   "name": "3dep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
